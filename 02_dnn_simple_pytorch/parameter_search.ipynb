{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJU2RPpSvlQT"
      },
      "source": [
        "# ハイパーパラメータの探索と検証データ\n",
        "\n",
        "\n",
        "---\n",
        "## 目的\n",
        "CNNを用いたMNISTデータセット文字認識を通じて，ハイパーパラメータの探索・検証および検証データの役割について理解する．\n",
        "\n",
        "また，ここではGPUを用いたネットワークの計算を行う．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rQGfxWYK_4O"
      },
      "source": [
        "## 準備\n",
        "\n",
        "### Google Colaboratoryの設定確認・変更\n",
        "本チュートリアルではPyTorchを利用してニューラルネットワークの実装を確認，学習および評価を行います．\n",
        "**GPUを用いて処理を行うために，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo4jjpmwvle1"
      },
      "source": [
        "## モジュールのインポート\n",
        "はじめに必要なモジュールをインポートする．\n",
        "\n",
        "### GPUの確認\n",
        "GPUを使用した計算が可能かどうかを確認します．\n",
        "\n",
        "`GPU availability: True`と表示されれば，GPUを使用した計算をPyTorchで行うことが可能です．\n",
        "Falseとなっている場合は，上記の「Google Colaboratoryの設定確認・変更」に記載している手順にしたがって，設定を変更した後に，モジュールのインポートから始めてください．\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCeaCulfvlao"
      },
      "outputs": [],
      "source": [
        "# モジュールのインポート\n",
        "import os\n",
        "from time import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torchsummary\n",
        "\n",
        "import gzip\n",
        "from random import randint\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# GPUの確認\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('Use CUDA:', use_cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nbdiIyZId5i"
      },
      "source": [
        "## データセットのダウンロードと読み込みと学習サンプルの削減\n",
        "\n",
        "\n",
        "まずはじめに，`wget`コマンドを使用して，MNISTデータセットをダウンロードします．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1LbRsqxIfoF"
      },
      "outputs": [],
      "source": [
        "!wget -q http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz -O train-images-idx3-ubyte.gz\n",
        "!wget -q http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz -O train-labels-idx1-ubyte.gz\n",
        "!wget -q http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz -O t10k-images-idx3-ubyte.gz\n",
        "!wget -q http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz -O t10k-labels-idx1-ubyte.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5R2ghpzIqyf"
      },
      "source": [
        "次に，ダウンロードしたファイルからデータを読み込みます．詳細は前回までのプログラムを確認してください．\n",
        "\n",
        "今回は2次元の画像データとしてMNISTデータセットを扱うため，\n",
        "データを`(チャンネル, 縦，横)`の形に並べ替えます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlChw880IqDC"
      },
      "outputs": [],
      "source": [
        "# load images\n",
        "with gzip.open('train-images-idx3-ubyte.gz', 'rb') as f:\n",
        "    x_train = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "\n",
        "with gzip.open('t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
        "    x_test = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "with gzip.open('train-labels-idx1-ubyte.gz', 'rb') as f:\n",
        "    y_train = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "\n",
        "with gzip.open('t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
        "    y_test = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "\n",
        "x_train = x_train.reshape(-1, 1, 28, 28)\n",
        "x_test = x_test.reshape(-1, 1, 28, 28)\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8w7yul9UBOc"
      },
      "source": [
        "## 検証データの作成\n",
        "\n",
        "ネットワークの学習やモデルの定義には多くのハイパーパラメータが存在します．例えば，\n",
        "* ネットワークのハイパーパラメータ\n",
        "  * 中間層の層数\n",
        "  * 中間層のユニット数\n",
        "  * Dropoutを適用するかどうか（Dropoutを用いる場所やdropout ratio）\n",
        "  * Batch Normalizationを適用するかどうか\n",
        "\n",
        "* 学習のハイパーパラメータ\n",
        "  * 学習率\n",
        "  * 学習回数\n",
        "  * ミニバッチサイズ\n",
        "\n",
        "などが挙げられます．\n",
        "\n",
        "最適なハイパーパラメータを決定するために使用するデータを**検証データ（validation data）**と呼びます．\n",
        "\n",
        "MNISTデータセットには学習データ（train data）とテストデータ（test data）しか存在しません．このように，専用の検証データが存在しないデータセットや個人が作成したオリジナルのデータセットでは，学習データの一部を検証データとして使用します．\n",
        "\n",
        "以下では，MNISTデータセットの学習データを分割し，学習および検証データを作成します．\n",
        "まず，検証データに使用するデータの割合を`validation_ratio`として定義します．\n",
        "今回は学習データの20%を検証データとして使用することとし，0.2と指定します．\n",
        "この割合に基づいて，学習データの20%となるサンプル数を`n_val`として計算します．\n",
        "\n",
        "その後，学習および検証データになるよう，データを分割します．\n",
        "\n",
        "実行すると，60000枚の学習サンプルの20%である12000枚が検証データ，残りの80%の48000枚が学習データになるよう分割されていることがわかります．\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnMbRmPTUBUn"
      },
      "outputs": [],
      "source": [
        "validation_ratio = 0.2   # 検証に使用するデータの割合\n",
        "n_train_original = x_train.shape[0]\n",
        "n_val = int(n_train_original * validation_ratio)\n",
        "\n",
        "# 検証データ\n",
        "x_val = x_train[0:n_val]\n",
        "y_val = y_train[0:n_val]\n",
        "\n",
        "# 学習データ\n",
        "x_train = x_train[n_val:]\n",
        "y_train = y_train[n_val:]\n",
        "\n",
        "print(\"train      :\", x_train.shape, y_train.shape)\n",
        "print(\"validation :\", x_val.shape, y_val.shape)\n",
        "print(\"test       :\", x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgDd3iX2zmSV"
      },
      "source": [
        "## ネットワークモデルの定義\n",
        "畳み込みニューラルネットワークを定義します．\n",
        "\n",
        "ここでは，畳み込み層２層，全結合層３層から構成されるネットワークとします．\n",
        "\n",
        "１層目の畳み込み層は入力チャンネル数が１，出力する特徴マップ数が16，畳み込むフィルタサイズが3x3です．２層目の畳み込み層は入力チャネル数が16．出力する特徴マップ数が32，畳み込むフィルタサイズは同じく3x3です．１つ目の全結合層は入力ユニット数は不定とし，出力は1024としています．次の全結合層入力，出力共に1024，出力層は入力が1024，出力が10です．これらの各層の構成を`__init__`関数で定義します．\n",
        "\n",
        "次に，`forward`関数では，定義した層を接続して処理するように記述します．`forward`関数の引数xは入力データです．それを`__init__`関数で定義したconv1に与え，その出力を活性化関数であるrelu関数に与えます．そして，その出力をmax_pooling_2dに与えて，プーリング処理結果をhとして出力します．hはconv2に与えられて畳み込み処理とプーリング処理を行います．そして，出力hをl1に与えて全結合層の処理を行います．最終的にl3の全結合層の処理を行った出力hを戻り値としています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNHnp_YczmY3"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, n_channels=1, filter_size=3, num_kernel=64, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(n_channels, num_kernel, kernel_size=filter_size, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.l1 = nn.Linear(int(28/2) * int(28/2) * num_kernel, hidden_size)\n",
        "        self.l2 = nn.Linear(hidden_size, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.relu(self.conv(x))\n",
        "        h = self.pool(h)\n",
        "        h = h.view(h.size()[0], -1)\n",
        "        h = self.relu(self.l1(h))\n",
        "        h = self.l2(h)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIPMneA4UMES"
      },
      "source": [
        "## 学習およびハイパーパラメータ探索の準備\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEpMENsiUMI2"
      },
      "source": [
        "\n",
        "### 探索時に共通するパラメータの設定\n",
        "\n",
        "まずはじめに，探索時に共通するパラメータを定義します．\n",
        "具体的には，入力層のユニット数`input_size`および出力層のユニット数`output_size`は，探索時には共通であるため，事前に定義しておきます．\n",
        "また，学習，検証データのサンプル数もすでに決まっているため，変数として定義しておきます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqITHAy3UMQw"
      },
      "outputs": [],
      "source": [
        "input_size = x_train.shape[1]\n",
        "output_size = 10\n",
        "\n",
        "num_train_data = x_train.shape[0]\n",
        "num_val_data = x_val.shape[0]\n",
        "num_test_data = x_test.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A03OafHqUMXn"
      },
      "source": [
        "### 探索するパラメータの設定\n",
        "\n",
        "次に，検証データを用いて最適な値を求めたいパラメータを定義します．\n",
        "\n",
        "ここでは，中間層のユニット数の最適な値を`[32, 64, 128, 256]`の中から求めるものとし，他のパラメータは固定します．\n",
        "\n",
        "また，パラメータの探索には複数回の学習を行う必要があり計算に多くの時間を要します．\n",
        "ここでは，実習中の計算時間削減のために，学習エポック数を10と小さい値に設定します．\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gnZyXtcUMc3"
      },
      "outputs": [],
      "source": [
        "n_hidden = 64\n",
        "\n",
        "channel_list = [32, 64, 128, 256]\n",
        "\n",
        "batch_size = 100\n",
        "n_iter = num_train_data / batch_size\n",
        "\n",
        "learning_rate = 0.01\n",
        "# learning_rate_list = [0.1, 0.05, 0.01, 0.005, 0.001]\n",
        "\n",
        "epoch_num = 10\n",
        "\n",
        "learning_rate = 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmDpq4r9Ujsj"
      },
      "source": [
        "## ハイパーパラメータ探索\n",
        "\n",
        "ハイパーパラメータ探索を行います．\n",
        "\n",
        "まず，各パラメータでの結果を保存するためのリスト`param_search_list`を作成します．\n",
        "\n",
        "次に，for文を用いて探索したいハイパーパラメータを一つづつ指定し，ネットワークの学習と検証データでの精度を求めます．\n",
        "ネットワークの学習プログラムは前回までのもの同様のため詳細は割愛します．\n",
        "\n",
        "学習が終了すると，探索したパラメータの値やその時の誤差および精度の推移のデータを辞書型オブジェクト`result`に格納し，それを`param_search_list`に保存します．\n",
        "\n",
        "これを繰り返すことで，各パラメータの値を用いた場合の精度や学習推移を確認比較することが可能となり，より精度の高いネットワークを構築するためのあたりをつけることができます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkOTUB2HUjyN"
      },
      "outputs": [],
      "source": [
        "param_search_list = []\n",
        "\n",
        "# 誤差関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "if use_cuda:\n",
        "    criterion.cuda()\n",
        "\n",
        "for channel_size in channel_list:\n",
        "    model = CNN(num_kernel=channel_size, hidden_size=n_hidden)\n",
        "    if use_cuda:\n",
        "        model.cuda()\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "    epoch_list = []\n",
        "    train_loss_list = []\n",
        "    train_accuracy_list = []\n",
        "    val_accuracy_list = []\n",
        "\n",
        "    # ネットワークを学習モードへ変更\n",
        "    model.train()\n",
        "\n",
        "    iteration = 1\n",
        "    for epoch in range(1, epoch_num+1):\n",
        "        count, sum_loss = 0.0, 0.0\n",
        "        \n",
        "        perm = np.random.permutation(num_train_data)\n",
        "        for i in range(0, num_train_data, batch_size):\n",
        "            x_batch = x_train[perm[i:i+batch_size]]\n",
        "            y_batch = y_train[perm[i:i+batch_size]]\n",
        "\n",
        "            x_batch = torch.from_numpy(x_batch).type(torch.float32)\n",
        "            y_batch = torch.from_numpy(y_batch).type(torch.int64)\n",
        "\n",
        "            if use_cuda:\n",
        "                x_batch = x_batch.cuda()\n",
        "                y_batch = y_batch.cuda()\n",
        "\n",
        "            y = model(x_batch)\n",
        "\n",
        "            loss = criterion(y, y_batch)\n",
        "            \n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            sum_loss += loss.item()\n",
        "            pred = torch.argmax(y, dim=1)\n",
        "            count += torch.sum(pred == y_batch)\n",
        "\n",
        "            iteration += 1\n",
        "            \n",
        "        # 検証データでの精度の確認\n",
        "        val_correct_count = 0\n",
        "        for i in range(num_val_data):\n",
        "            input = x_val[i:i+1]\n",
        "            label = y_val[i:i+1]\n",
        "            input = torch.from_numpy(input).type(torch.float32)\n",
        "            label = torch.from_numpy(label).type(torch.int64)\n",
        "            if use_cuda:\n",
        "                input = input.cuda()\n",
        "                label = label.cuda()\n",
        "            y = model(input)\n",
        "            pred = torch.argmax(y)\n",
        "            if pred == label:\n",
        "              val_correct_count += 1\n",
        "\n",
        "        # 学習途中のlossと精度の保存\n",
        "        epoch_list.append(epoch)\n",
        "        train_loss_list.append(sum_loss / num_train_data)\n",
        "        train_accuracy_list.append(count / num_train_data)\n",
        "        val_accuracy_list.append(val_correct_count / num_val_data)\n",
        "            \n",
        "        print(\"epoch: {}, mean loss: {}, mean accuracy: {}\".format(epoch,\n",
        "                                                                  sum_loss / n_iter,\n",
        "                                                                  count.item() / num_train_data))\n",
        "\n",
        "    # 探索した結果の保存\n",
        "    result = {'lr': learning_rate, 'n_hidden': hidden_size,\n",
        "              'val_acc': val_accuracy_list,\n",
        "              'train_acc': train_accuracy_list,\n",
        "              'train_loss': train_loss_list}\n",
        "    param_search_list.append(result)\n",
        "    print(result['lr'], result['n_hidden'], result['train_acc'][-1], result['val_acc'][-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "119eIrSmzmw6"
      },
      "source": [
        "## 探索結果の確認\n",
        "\n",
        "`param_search_list`に保存しておいた各ハイパーパラメータに対する結果を確認します．\n",
        "\n",
        "まず，各結果の数値をprintします．\n",
        "その後，各パラメータでの学習推移（検証データ）をプロットし比較します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoYVMRGLzm1I"
      },
      "outputs": [],
      "source": [
        "# 学習結果の表示\n",
        "for ps in param_search_list:\n",
        "    print(\"lr:\", ps['lr'],\n",
        "          \"hidden size\", ps['n_hidden'],\n",
        "          \"train accuracy:\", ps['train_acc'][-1],\n",
        "          \"validation accuracy:\", ps['val_acc'][-1])\n",
        "\n",
        "# グラフプロット\n",
        "plt.figure()\n",
        "for ps in param_search_list:\n",
        "    plt.plot(ps['val_acc'], label='hidden=%d' % ps['n_hidden'])\n",
        "plt.xlabel(\"epoch\")     # x軸ラベル\n",
        "plt.ylabel(\"accuracy\")  # y軸ラベル\n",
        "plt.legend()            # 凡例\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH36DIdfXWR5"
      },
      "source": [
        "## 課題\n",
        "1. 他のパラメータについても探索をして最適な値を見つけよう\n",
        "2. 複数種類のパラメータの最適な組み合わせを求めよう"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjS-WptIg9Pf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "12_parameter_search.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
