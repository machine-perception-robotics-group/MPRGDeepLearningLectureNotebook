{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"05_linear_svm.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3TwUs1a8sSDM"},"source":["# 線形SVMによる教師あり学習\n","\n","---\n","## 目的\n","線形SVM(Support Vector Machine)を用いて2つのサンプルを識別する．その後，交差検定法を用いて識別テストを行う．\n","\n","\n","## プログラムの動作\n","以下のプログラムを実行すると，`data/car.txt`と`data/human.txt`の2つ読み込む．次に，線形SVMによる学習およびテストを交差検定法を用いて行う．最後に，識別率と識別されたグラフを表示する．\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5aYrBRlEsfsp"},"source":["## 準備\n","プログラムの動作に必要なデータをダウンロードし，zipファイルを解凍する．"]},{"cell_type":"code","metadata":{"id":"JouGYBdrsSNi"},"source":["!wget -q http://www.mprg.cs.chubu.ac.jp/tutorial/ML_Lecture/sklearn/data.zip\n","!unzip -q data.zip\n","!ls\n","!ls ./data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aZLojkvIsSVc"},"source":["##モジュールのインポート\n","初めに，必要なモジュールをインポートする．\n","\n","今回は，SVMを用いるために`SVM`をインポートする．また，精度評価を行うために`metrics`を，交差検定法を用いるため`model_selection`をインポートする．\n"]},{"cell_type":"code","metadata":{"id":"EuGAq8InsSbb"},"source":["from os import path\n","import numpy as np\n","from sklearn import svm\n","from sklearn import metrics\n","from sklearn import model_selection"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VzTVMeYIu12x"},"source":["##データの読み込み\n","次に，テキストファイルを読み込む．\n"]},{"cell_type":"code","metadata":{"id":"K9ZwdH46tA0_"},"source":["in_txt1 = open(path.join('data', 'car.txt'))\n","in_txt2 = open(path.join('data', 'human.txt'))\n","\n","car = np.asarray([(line.strip()).split('\\t') for line in in_txt1], dtype=float)\n","print(car.shape)\n","human = np.asarray([(line.strip()).split('\\t') for line in in_txt2], dtype=float)\n","print(human.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0GLv9PVuu61S"},"source":["## データのラベル付けと結合，交差検定法の準備\n","学習を行う前に，データがcarまたはhumanどちらのクラスに属するかラベル付けをする．"]},{"cell_type":"code","metadata":{"id":"NvCtgdVvu695"},"source":["car_y =  np.zeros(car.shape[0])\n","human_y = np.ones(human.shape[0])\n","X= np.r_[car, human]\n","y= np.r_[car_y, human_y]\n","\n","kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=0)\n","print(X.shape, y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RFkkciS0vTyd"},"source":["基本的に第3回と同じであるが，`kfold = model_selection.KFold(n_splits=5)`は今回が初出である．これは，交差検定法をとしてk-fold法を用いるためのオブジェクトである．\n","\n","ここでは，`n_splits=5`でデータを5分割するように設定している．\n","\n","`shuffle=True`は，データをランダムにわけるようにする．例えば，この値が`False`のとき，`学習用データ：[ 1, 2, 3, 4, 5, 6 ]`，`テスト用データ[ 7, 8, 9 ]`のように連番でデータを分割するが，このままだと値が偏ってしまうため，データをランダムに分ける必要がある．\n","\n","`random_state=0`は，上記のランダム値のシード値である．この値を元にしてランダム値を発生させるため，この値を変えなければ毎回同じランダム値が得られる．通常は「現在時刻」など絶対に二度発生し得ない値を選ぶのだが，今回はテスト条件を統一したいため，値は`0`で固定する．次回以降のプログラムも同じである．\n","\n","\n","### k-fold法\n","k-fold法では，データをk個に分割し，そのうち1つをテスト用データ，残りを学習用データとする交差検定法である．テスト用データは持ち回りで選ばれ，合計k回繰り返される．\n","例えば，データとして9つの数字`[ 1, 2, 3, 4, 5, 6, 7, 8, 9 ]`があると仮定する．`k=3`のとき，データは`[ 1, 2, 3 ]`，`[ 4, 5, 6 ]`，`[ 7, 8, 9 ]`の3つに分けられる．この3つのデータを`k[0]`，`k[1]`，`k[2]`と仮定したとき，1回目のテストは`k[0]`がテスト用データになり，残りは学習用データとなる．2回目は`k[1]`がテスト用データ，3回目は`k[2]`がテスト用データである．このようにして，全てのデータをうまくテストすることができる．なお，結果はk回のテストの平均となる．\n","\n","![cross_validation.png](https://qiita-image-store.s3.amazonaws.com/0/143078/aa879433-4a18-2690-6caa-16f8868f7af6.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XPkEIVd_wVOA"},"source":["## 学習と評価\n","for文で学習と評価をk回繰り返す．以下，順に説明する．\n","\n","\n","### 1. リストと変数の初期化\n","for文の前に，学習とテストに必要なリストと変数を初期化する．\n","\n","必要なリストおよび変数は次の通りである：\n","\n","- `scores`: 識別率を記憶するリスト\n","- `best_score`: k回繰り返すうちの最も良かった識別率を記憶する変数\n","\n","リストは，`[リスト名].append([新しい要素])`で要素を逐次追加することができる※．\n","\n","\n","### 2. 学習\n","学習とテスト，識別率の計算はfor文の中で行う．まずは学習について説明する．\n","\n","このfor文の書き方はC言語にはないPython独特な書き表し方である．ここでは，変数`train`と`test`にリスト`kfold`で定義されたリストを順番に代入してfor文を回していく．train`に学習用データ，`test`にテスト用データが代入されてループがスタートする．データを入れ替えながらk回繰り返したらループ終了である．\n","\n","`classifier = svm.LinearSVC(C=0.1, random_state=0)`では，第3回と同じように識別器の設定を行う．`LinearSVC`が線形SVMの識別器を表す．パラメータは，誤識別をどれだけ許容するかを表す`C=0.1`と，データをランダムにする`random_state=0`である．k-foldと同じ理由から，`random_state`は`0`のまま固定にしておく．\n","\n","用意した識別器を用いて学習を行う方法は第4回のkNNと同じである．\n","\n","\n","### 3. テスト\n","学習と同じく，テストも簡単に行える．\n","\n","`classifier.predict(X[test])`は，テストデータがどのクラスに属するかをテストである．リスト`preds`はテストの結果が代入される．次に，`metrics.accuracy_score(preds, y[test])`は先ほどのテストで得られた結果と正解ラベル`y[test]`を比べて検出率を計算する．変数`score`には検出率が代入される．\n","\n","\n","### 4. 最も良かったスコアと識別器の記録\n","もし，テスト結果が今までで一番良かった場合は識別機を記憶する．\n","\n","現時点でのベストスコアよりも高いスコアが出た場合，学習済み識別器を保管し，ベストスコアを更新する．\n","\n","\n","※「リスト」と「配列」は用意されている機能（関数）が大きく違っているが，基本的な使い方（添字でのアクセス等）は同じである．今回はリストでしか行えない処理（`append`）を行うためリストを用いる．numpyの「配列」にも`append`関数はあるが，挙動が違うためここではPython標準のリストを用いる．どの変数が配列でどの変数がリストか注意すること．\n"]},{"cell_type":"code","metadata":{"id":"ckfHk003u7Jr"},"source":["# 1. リストと変数の初期化\n","scores = []\n","best_score = 0.0\n","\n","# 2. 学習\n","for train, test in kfold.split(X):\n","    classifier = svm.LinearSVC(C=0.1, random_state=0)\n","    classifier.fit(X[train], y[train])\n","    \n","    # 3. テスト\n","    preds = classifier.predict(X[test])\n","    score = metrics.accuracy_score(preds, y[test])\n","    scores.append(score)\n","\n","    # 4. 最も良かったスコアと識別器の記録\n","    if score > best_score:\n","        best_classifier = classifier\n","        best_score = score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6dWQG_Hqw7r3"},"source":["## 識別率の計算\n","得られたk個の結果を平均して，識別率を求める．\n","\n","`sum(リスト)`でリストの合計，`len(リスト)`でリストの要素数を求め，平均値を出し，100倍して百分率の値にする．次の行では，結果を画面に表示するために文字を整形する．`'{accuracy:.2f}%'.format(accuracy=accuracy)`は`accuracy`を小数点第2位まで表示となる．．C言語の`printf`のような記述方法も可能である．興味のある人は調べてみると良い．整形したテキストは次の行でprintされる．"]},{"cell_type":"code","metadata":{"id":"oFY2w0vNu7UO"},"source":["accuracy = (sum(scores) / len(scores)) * 100\n","msg = 'recognition rate: {accuracy:.2f}%'.format(accuracy=accuracy)\n","print(msg)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JW0BPvhzu7ZU"},"source":["## グラフの描画\n","識別結果を可視化するために，先程求めた「最も識別率の良かった学習データを学習させてある識別器」を用いて，グラフを作成する．どちらのクラスがどの領域かわかりやすく表示することができる．\n","\n","第3回でも用いた`meshgrid`と，等高線を描画する`contour`を用いる，今回は等高線を塗りつぶして2つのクラスをエリアのように表示するため，塗りつぶす等高線`contourf`を用いる．他は第3回目とほとんど同じである．"]},{"cell_type":"code","metadata":{"id":"DOFHsNMTu7el"},"source":["import matplotlib.pyplot as plt\n","\n","fig = plt.figure()\n","subfig = fig.add_subplot(1,1,1)\n","plt.xlim(0, 10000)\n","plt.ylim(20, 50)\n","\n","xx, yy = np.meshgrid(np.linspace(plt.xlim()[0], plt.xlim()[1], 500),\n","                     np.linspace(plt.ylim()[0], plt.ylim()[1], 500))\n","\n","Z = best_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n","Z = Z.reshape(xx.shape)\n","cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n","\n","subfig.scatter(car[:,0], car[:,1],color='black')\n","subfig.scatter(human[:,0], human[:,1],color='red')\n","\n","subfig.set_title('05 SVM Linear')\n","subfig.set_xlabel('Area')\n","subfig.set_ylabel('complexity')\n","\n","plt.savefig(\"05_graph.png\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MXjIXxqaxb_F"},"source":["## 課題\n","1. 線形SVMのパラメータ`C`の値を変えると識別境界はどう変化するか．\n","2. より識別率が高くなる`C`の値を求めよ．\n","\n","\n","## ヒント\n","1. `C`は誤識別サンプルに対するペナルティを示す．グラフを表示して確かめてみる．\n","2. ある値より高くすると変化しなくなる．高すぎても，低すぎても良くない．"]}]}