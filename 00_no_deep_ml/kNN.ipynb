{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04_kNN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"V-QoKHIlkTQj"},"source":["# k最近傍法による教師あり学習\n","\n","\n","---\n","## 目的\n","k最近傍法(kNN : k-Nearest Neighbor)を用いて2つのサンプルを識別する．識別結果をグラフに表示して，識別境界がどうなるか確認する．\n","\n","\n","## プログラムの動作\n","`04_kNN.py`を実行すると，`data/car.txt`と`data/human.txt`の2つ読み込む．次に，2つのデータをkNNを用いて学習する．その後，識別境界のわかるグラフを表示する．\n"]},{"cell_type":"markdown","metadata":{"id":"_Kof4LqpkUYb"},"source":["## 準備\n","プログラムの動作に必要なデータをダウンロードし，zipファイルを解凍する．`！`で始まるコマンドはpythonではなく，Linux（Ubuntu）のコマンドを実行している．"]},{"cell_type":"code","metadata":{"id":"TNZ5kAjyjgTy"},"source":["!wget -q http://www.mprg.cs.chubu.ac.jp/tutorial/ML_Lecture/sklearn/data.zip\n","!unzip -q data.zip\n","!ls\n","!ls ./data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pIM_xN3AknlK"},"source":["## モジュールのインポート\n","初めに，必要なモジュールをインポートする．\n","\n","今回は，k最近傍法を用いるために`KNeighborsClassifier`をインポートする．また，精度評価を行うために`metrics`をインポートする．\n"]},{"cell_type":"code","metadata":{"id":"D3DxlvIQkonn"},"source":["from os import path\n","import numpy as np\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn import metrics"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GT-OHGdSkovh"},"source":["##データの読み込み\n","次に，テキストファイルを読み込む．\n"]},{"cell_type":"code","metadata":{"id":"eS6Mx9e-ko1U"},"source":["in_txt1 = open(path.join('data', 'car.txt'))\n","in_txt2 = open(path.join('data', 'human.txt'))\n","\n","car = np.asarray([(line.strip()).split('\\t') for line in in_txt1], dtype=float)\n","print(car.shape)\n","human = np.asarray([(line.strip()).split('\\t') for line in in_txt2], dtype=float)\n","print(human.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jR3qutjnko68"},"source":["## データのラベル付けと結合\n","学習を行う前に，データがcarまたはhumanどちらのクラスに属するかラベル付けをする必要がある．ここでは，carのラベルを0，humanのラベルを1とする．また，carとhumanの配列をマージして1つの配列にしないと，学習を行うことができないため，これを行う．\n","\n"]},{"cell_type":"code","metadata":{"id":"a1Y_-NWwkpA7"},"source":["car_y =  np.zeros(car.shape[0])\n","human_y = np.ones(human.shape[0])\n","X= np.r_[car, human]\n","y= np.r_[car_y, human_y]\n","print(X.shape, y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VCXW638akpGu"},"source":["`np.zeros`と`np.ones`はそれぞれnumpyの関数である．`zeros`は与えられた数だけ要素を持った要素を持った配列を用意する．配列の中身は全て0になる．`ones`は`zeros`と同じだが，配列の中身は全て1になる．つまり，`car_y =  np.zeros(car.shape[0])`は，配列`car`と同じ要素数の配列`car_y`が定義され，中身は全て0になる．\n","\n","`np.r_`は同じくnumpyの関数である．与えられた2つ以上の配列を1つの配列としてマージする．つまり，`X= np.r_[car, human]`は配列`car`と`human`を1つの配列`X`として作成するものになる．配列`X`のラベルが入った配列が`y`ということになる．\n","\n","最終的に，carとhumanのデータが入った配列`X`と，そのラベルが入った配列`y`が作成される．`X`のn番目の要素がどちらのクラスに属するかは配列`y`のn番目の要素を確認すればわかる，ということになる．\n","\n","![array.png](https://qiita-image-store.s3.amazonaws.com/0/143078/6cbc837a-d3a4-b953-683e-520052288ffd.png)\n","\n","\n","確認のため，最後に`print(X.shape, y.shape)`で各配列の要素数を出力する．"]},{"cell_type":"markdown","metadata":{"id":"_R7OcPdal7nK"},"source":["## 学習用データの作成\n","第2回のユークリッド距離の時と同じ理由で，学習用にデータのy軸の値を変更する．\n","\n","データのある配列`X`に対して，y軸の値を変更する．"]},{"cell_type":"code","metadata":{"id":"nD3b9caekpMh"},"source":["X_train = np.c_[ X[:,0], ( X[:,1] - 20.0 ) * 266.7 ]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Nq4p5dEkpTB"},"source":["## k値の変更\n","k最近傍法は，識別するサンプルの近くに存在するk個のサンプルがどちらのクラスに属するか判定して識別を行うアルゴリズムである．このプログラムでは，kの値を任意の数に変更し，結果がどう変化するかを調べることができる．\n","\n","この値を変更することで，kNNのkの値を変更することができる．"]},{"cell_type":"code","metadata":{"id":"w-GKjGWCkpZB"},"source":["n_neighbors = 5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4S-FjJ-rmV1V"},"source":["## 学習とテスト\n","識別機を用意して，学習を行う．\n","\n","`classifier = KNeighborsClassifier(algorithm='brute', n_neighbors=n_neighbors)`では，識別器の設定を行う．`KNeighborsClassifier`がkNNの識別器を表す．引数に与えているのはパラメータであり，`algorithm`はkNNのアルゴリズム（ここでは`brute`だが，他に`ball_tree`や`kd_tree`等がある），`n_neighbors`はk値を決める．\n","\n","用意した識別器を用いて学習を行う方法は簡単であり，`[識別器].fit([学習データ])`とするだけである．これはscikit-learn共通の手法であり，他の識別器でも同じ使い方をする．ここでは，`classifier.fit(X[train], y[train])`として，学習用データとその正解ラベルを与えて学習を行う．\n"]},{"cell_type":"code","metadata":{"id":"umJRrXKDmV7T"},"source":["classifier = KNeighborsClassifier(algorithm='brute', n_neighbors=n_neighbors)\n","classifier.fit(X_train, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-GibA4iFmWBc"},"source":["## グラフの描画\n","matplotlibを用いてグラフを描画する．基本的には第2回と同じである．\n","\n","今回は`classifier.predict`を行うことで，格子点すべての点に対して識別を行う．ここでは詳しい説明は割愛するが，識別のやり方を知りたい場合は第5回の解説の「テスト」の項目を先に読むと良い．"]},{"cell_type":"code","metadata":{"id":"yN767LPnmWG5"},"source":["import matplotlib.pyplot as plt\n","\n","fig = plt.figure()\n","subfig = fig.add_subplot(1,1,1)\n","plt.xlim(0, 10000)\n","plt.ylim(20, 50)\n","\n","xx, yy = np.meshgrid(np.linspace(plt.xlim()[0], plt.xlim()[1], 500),\n","                     np.linspace(plt.ylim()[0], plt.ylim()[1], 500))\n","\n","Z = classifier.predict(np.c_[xx.ravel(), ( yy.ravel() - 20.0 ) * 266.7 ])\n","Z = Z.reshape(xx.shape)\n","cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n","\n","subfig.scatter(car[:,0], car[:,1],color='black')\n","subfig.scatter(human[:,0], human[:,1],color='red')\n","\n","subfig.set_title('04 kNN k=' + str(n_neighbors))\n","subfig.set_xlabel('Area')\n","subfig.set_ylabel('complexity')\n","\n","plt.savefig(\"04_graph.png\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XLm-fLNsn-GH"},"source":["#課題\n","\n","1. kの値を変化させて，クラスの境界線の変わり方を確認せよ．\n","2. kNNはすべてのプロトタイプとの距離を計算するため，計算時間がかかる．この対策法として，kd-tree法がある．kd-tree法を調査せよ．\n","\n","#ヒント\n","\n","1. 小さすぎるのも，大きすぎるのも良くない．\n","2. すべてのプロトタイプとの距離を計算する代わりにどうするかに着目しよう．"]}]}