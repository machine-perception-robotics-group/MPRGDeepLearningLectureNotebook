{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_4yIWKExMcV"
      },
      "source": [
        "# ST-GCNによる動作認識\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsAg0haiIC1a"
      },
      "source": [
        "#骨格データからの動作認識\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GslML25UII5Y"
      },
      "source": [
        "骨格データからの動作認識を行います.  \n",
        "動作認識は, その人が何の動作(投げる, 蹴る, ジャンプ...)をしているかを認識するタスクです.  \n",
        "動作認識には,画像などから行う手法もありますが,ここでは骨格データを使います."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42iCYGbOIktX"
      },
      "source": [
        "###骨格データ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B7C36eHRrWb"
      },
      "source": [
        "骨格データとは，フレームごとの関節座標です.  \n",
        "しかしながら,素のデータはただの座標であり, グラフとは言えません.  \n",
        "\n",
        "人間の構造は関節と関節の繋がりで表現できます.つまり人間はグラフとして表現できます.\n",
        "\n",
        "ノードの特徴（座標）とエッジ（関節の繋がり）を使うことで，骨格データをグラフとして表現します.  \n",
        "グラフで表現することで,関節間の関係性を考慮することができます.\n",
        "\n",
        "グラフで表現した骨格データをGraph Convolutional Networks(GCN)に通して, 何の動作をしているかを認識します.\n",
        "\n",
        "下の動画が，グラフ表現をした骨格データの例です.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz6pKa3pt2kE"
      },
      "source": [
        "<!--\n",
        "<img src='https://drive.google.com/uc?id=1WrYd80u9buVcmBnpsgSZnzFlsYMih7Nr' width=30%>\n",
        "-->\n",
        "<!--\n",
        "<img src='https://github.com/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/15_gcn/fig/03_throw_skeleton.gif?raw=true' width=30%>\n",
        "-->\n",
        "<img src='https://github.com/sirakik/MPRGLecture/blob/master/15_gcn/fig/03_throw_skeleton.gif?raw=true' width=30%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty9FHb4yVB94"
      },
      "source": [
        "#Spatial Temporal Graph Convolutional Networks\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnhYROVaVSXp"
      },
      "source": [
        "GCNを使った骨格データからの動作認識の代表的な手法として, Spatial Temporal Graph Convolutional Networks(ST-GCN)[[arXiv](https://arxiv.org/abs/1801.07455)]があります."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwpdRsahuO-S"
      },
      "source": [
        "<!--\n",
        "<img src='https://drive.google.com/uc?id=1ZRf-NF4S0P1VwMxN2DrTFPeO4EJ5if3S' width=100%>\n",
        "-->\n",
        "<!--\n",
        "<img src='https://github.com/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/15_gcn/fig/03_st-gcn.png?raw=true' width=100%>\n",
        "-->\n",
        "<img src='https://github.com/sirakik/MPRGLecture/blob/master/15_gcn/fig/03_st-gcn.png?raw=true' width=100%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U_7UNeBWMN8"
      },
      "source": [
        "ST-GCNの特徴は,骨格データを2つのグラフ構造として表現したことです.\n",
        "- 空間グラフ：同一フレーム内の関節を結ぶグラフ　\n",
        "- 時間グラフ：隣接フレームの同一関節を結ぶグラフ\n",
        "\n",
        "空間グラフと時間グラフをGraph Covnolutionによって特徴を抽出することで，関節間の関係と, 時間的な変化を考慮しています. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAhpuPkHuf2E"
      },
      "source": [
        "<!--\n",
        "<img src='https://drive.google.com/uc?id=1FDOGPZxaIYs-be-6tZzPeBsrtMcBXcuv' width=30%>\n",
        "-->\n",
        "<!--\n",
        "<img src='https://github.com/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/15_gcn/fig/03_st_graph.png?raw=true' width=30%>\n",
        "-->\n",
        "<img src='https://github.com/sirakik/MPRGLecture/blob/master/15_gcn/fig/03_st_graph.png?raw=true' width=30%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9_Um9Dy3ZeI"
      },
      "source": [
        "# 実装前の準備\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa7D5v0f3e_E"
      },
      "source": [
        "必要なモジュールのインポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShT0XzlC4Glz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78xIwKEz4G69"
      },
      "source": [
        "GPU確認　　\n",
        "\n",
        "今回からGPUを使用して学習します. 確認してください."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVXaaWD54IBn",
        "outputId": "e677e041-9d0c-4cd4-df0b-bc8fafe7d462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Use CUDA: True\n"
          ]
        }
      ],
      "source": [
        "print('Use CUDA:', torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5Zpy-Z0YFJS"
      },
      "source": [
        "# データセット\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLl5gIVi4pTn"
      },
      "source": [
        "データセットをダンロードします.今回は，小さな独自のデータセット(注1)を用意したので，それを使用します.  \n",
        "動作クラス数は10クラス（0~9）です.動作は以下の通りです.\n",
        "*   0:　飲む\n",
        "*   1:　投げる\n",
        "*   2:　座る\n",
        "*   3:　立ち上がる\n",
        "*   4:　拍手\n",
        "*   5:　手を振る\n",
        "*   6:　蹴る\n",
        "*   7:　ジャンプ\n",
        "*   8:　敬礼\n",
        "*   9:　転倒\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-9AZzvcT_7o"
      },
      "source": [
        "注1: このデータセットは[NTU-RGB+Dデータセット](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shahroudy_NTU_RGBD_A_CVPR_2016_paper.pdf)を利用して作成しました.  \n",
        "Portions of the research used the NTU RGB+D Action Recognition Dataset made available by the ROSE Lab at the Nanyang Technological University, Singapore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nb7J1jWy1Bpi"
      },
      "outputs": [],
      "source": [
        "!wget -q --load-cookies /tmp/cookies.txt \"https://drive.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://drive.google.com/uc?export=download&id=1zzPvyMLY7jlyJ3phEZRePsNKhhzspD0u' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1zzPvyMLY7jlyJ3phEZRePsNKhhzspD0u\" -O data.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip -q -o data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYixFxzPfgqd"
      },
      "source": [
        "学習データ数が2000(10クラス×200データ), 評価データ数が200(10クラス×20データ)あります.\n",
        "\n",
        "データの構造を確認します.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi7FLBnJ-156",
        "outputId": "ac075d58-d83d-49f1-b9c3-1675b50aa19b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 80, 25)\n"
          ]
        }
      ],
      "source": [
        "test_data = np.load(\"data/test_data.npy\")\n",
        "print(test_data[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftiwD1KhoFVS"
      },
      "source": [
        "(次元数，フレーム数，関節数)の構造です.  \n",
        "1データあたり,3次元座標の25関節が80フレーム分入っています."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGAvBo-MqpJy"
      },
      "outputs": [],
      "source": [
        "# データを読み込むための関数\n",
        "class Feeder(torch.utils.data.Dataset):\n",
        "  def __init__(self, data_path, label_path):\n",
        "      super().__init__()\n",
        "      self.label = np.load(label_path)\n",
        "      self.data = np.load(data_path)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.label)\n",
        "\n",
        "  def __iter__(self):\n",
        "      return self\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      data = np.array(self.data[index])\n",
        "      label = self.label[index]\n",
        "\n",
        "      return data, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEA6NlHGaGSb"
      },
      "source": [
        "### 隣接行列を作成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFYyypxdo3_G"
      },
      "source": [
        "今は座標データ（ノード特徴）しかないため，グラフではありません．  \n",
        "接続関係を定義して，グラフにします．接続関係の表現には隣接行列を使用します.  \n",
        "class化しておきます. モデルの定義する際に呼び出します.\n",
        "\n",
        "隣接行列を手作業で編集するのは大変であるため，接続関係を配列で用意し，それをもとに隣接行列を作ります."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFV6MAuFds7V"
      },
      "outputs": [],
      "source": [
        "class Graph():\n",
        "  def __init__(self, hop_size):\n",
        "    # エッジ配列を宣言します. 集合としては{{始点, 終点}, {始点, 終点}, {始点, 終点}...}のように一つのエッジを要素として宣言します.\n",
        "    self.get_edge()\n",
        "    \n",
        "    # hop: hop数分離れた関節を結びます.\n",
        "    # 例えばhop=2だと, 手首は肘だけではなく肩にも繋がっています.\n",
        "    self.hop_size = hop_size \n",
        "    self.hop_dis = self.get_hop_distance(self.num_node, self.edge, hop_size=hop_size)\n",
        "\n",
        "    # 隣接行列を作ります.ここではhop数ごとに隣接行列を作成します.\n",
        "    # hopが2の時, 0hop, 1hop, 2hopの３つの隣接行列が作成されます.\n",
        "    # 複数の生成方法が論文中に提案されています. 今回はわかりやすいものを使いました.\n",
        "    self.get_adjacency() \n",
        "\n",
        "  def __str__(self):\n",
        "    return self.A\n",
        "\n",
        "  def get_edge(self):\n",
        "    self.num_node = 25\n",
        "    self_link = [(i, i) for i in range(self.num_node)] # ループ\n",
        "    neighbor_base = [(1, 2), (2, 21), (3, 21), (4, 3), (5, 21),\n",
        "                      (6, 5), (7, 6), (8, 7), (9, 21), (10, 9),\n",
        "                      (11, 10), (12, 11), (13, 1), (14, 13), (15, 14),\n",
        "                      (16, 15), (17, 1), (18, 17), (19, 18), (20, 19),\n",
        "                      (22, 23), (23, 8), (24, 25), (25, 12)]\n",
        "    neighbor_link = [(i - 1, j - 1) for (i, j) in neighbor_base]\n",
        "    self.edge = self_link + neighbor_link\n",
        "\n",
        "  def get_adjacency(self):\n",
        "    valid_hop = range(0, self.hop_size + 1, 1)\n",
        "    adjacency = np.zeros((self.num_node, self.num_node))\n",
        "    for hop in valid_hop:\n",
        "        adjacency[self.hop_dis == hop] = 1\n",
        "    normalize_adjacency = self.normalize_digraph(adjacency)\n",
        "    A = np.zeros((len(valid_hop), self.num_node, self.num_node))\n",
        "    for i, hop in enumerate(valid_hop):\n",
        "        A[i][self.hop_dis == hop] = normalize_adjacency[self.hop_dis == hop]\n",
        "    self.A = A\n",
        "\n",
        "  def get_hop_distance(self, num_node, edge, hop_size):\n",
        "    A = np.zeros((num_node, num_node))\n",
        "    for i, j in edge:\n",
        "        A[j, i] = 1\n",
        "        A[i, j] = 1\n",
        "    hop_dis = np.zeros((num_node, num_node)) + np.inf\n",
        "    transfer_mat = [np.linalg.matrix_power(A, d) for d in range(hop_size + 1)]\n",
        "    arrive_mat = (np.stack(transfer_mat) > 0)\n",
        "    for d in range(hop_size, -1, -1):\n",
        "        hop_dis[arrive_mat[d]] = d\n",
        "    return hop_dis\n",
        "\n",
        "  def normalize_digraph(self, A):\n",
        "    Dl = np.sum(A, 0)\n",
        "    num_node = A.shape[0]\n",
        "    Dn = np.zeros((num_node, num_node))\n",
        "    for i in range(num_node):\n",
        "        if Dl[i] > 0:\n",
        "            Dn[i, i] = Dl[i]**(-1)\n",
        "    DAD = np.dot(A, Dn)\n",
        "    return DAD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTCb52YUYHYh"
      },
      "source": [
        "#ST-GCN実装\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H6jSUH9YdCQ"
      },
      "source": [
        "###空間グラフの畳み込み  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvIeqwf7tT1K"
      },
      "source": [
        "まずは，空間グラフ(人間の接続パターン)のグラフ畳み込みを実装します.  \n",
        "式は前回(Graph Convolutional Networksによるノード分類)に示したGraph Convolutionとほぼ同じです.\n",
        "\\begin{equation}\n",
        "{\\bf H}_{out}=\\sum_{j}{\\bf\\tilde D}^{-\\frac{1}{2}}_j{\\bf\\tilde A}_j{\\bf\\tilde D}^{-\\frac{1}{2}}_j{\\bf H}_{in}{\\bf W}_{j}\n",
        "\\end{equation}\n",
        "hop数分の隣接行列($j$:隣接行列の数)があるため，各隣接行列で畳み込んでから，特徴を足し合わせています.  \n",
        "\n",
        "高速化や，今後の拡張性のため，前回のGCとは実装を変えています.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrffcNi0ZCP7"
      },
      "outputs": [],
      "source": [
        "class SpatialGraphConvolution(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, s_kernel_size):\n",
        "    super().__init__()\n",
        "    self.s_kernel_size = s_kernel_size\n",
        "    self.conv = nn.Conv2d(in_channels=in_channels,\n",
        "                          out_channels=out_channels * s_kernel_size,\n",
        "                          kernel_size=1)\n",
        "    \n",
        "  def forward(self, x, A):\n",
        "    x = self.conv(x)\n",
        "    n, kc, t, v = x.size()\n",
        "    x = x.view(n, self.s_kernel_size, kc//self.s_kernel_size, t, v)\n",
        "    # 隣接行列にGCを行い, 特徴を足し合わせています.\n",
        "    x = torch.einsum('nkctv,kvw->nctw', (x, A))\n",
        "    return x.contiguous()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kosLltaeYg55"
      },
      "source": [
        "###時間グラフの畳み込み"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5T1vQz6bvsu"
      },
      "source": [
        "時間グラフは,グラフ畳み込み処理ではなく一般的な2d畳み込み処理で実装できます.  \n",
        "特徴マップは（フレーム数×関節数)の形になっています. 同一関節をフレーム方向に繋いだものが時間グラフです.  \n",
        "フレーム方向に畳み込めばいいため（$T\\times 1$）の2d畳み込みフィルターで実装できます.\n",
        "\n",
        "また, ST-GCNは, 空間グラフと時間グラフの畳み込みを交互に行います.  \n",
        "これを繰り返すので, 空間グラフと時間グラフ，その他(活性化関数やdropout)を備えたクラス(STGC_block)を作っておきます."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPcF2Od9AvmA"
      },
      "outputs": [],
      "source": [
        "class STGC_block(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride, t_kernel_size, A_size, dropout=0.5):\n",
        "    super().__init__()\n",
        "    # 空間グラフの畳み込み\n",
        "    self.sgc = SpatialGraphConvolution(in_channels=in_channels,\n",
        "                                       out_channels=out_channels,\n",
        "                                       s_kernel_size=A_size[0])\n",
        "    \n",
        "    # Learnable weight matrix M エッジに重みを与えます. どのエッジが重要かを学習します.\n",
        "    self.M = nn.Parameter(torch.ones(A_size))\n",
        "\n",
        "    self.tgc = nn.Sequential(nn.BatchNorm2d(out_channels),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Dropout(dropout),\n",
        "                            nn.Conv2d(out_channels,\n",
        "                                      out_channels,\n",
        "                                      (t_kernel_size, 1),\n",
        "                                      (stride, 1),\n",
        "                                      ((t_kernel_size - 1) // 2, 0)),\n",
        "                            nn.BatchNorm2d(out_channels),\n",
        "                            nn.ReLU())\n",
        "\n",
        "  def forward(self, x, A):\n",
        "    x = self.tgc(self.sgc(x, A * self.M))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy0nx9SvYod1"
      },
      "source": [
        "###ネットワークモデル"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzAe1GgpZv7k"
      },
      "outputs": [],
      "source": [
        "class ST_GCN(nn.Module):\n",
        "  def __init__(self, num_classes, in_channels, t_kernel_size, hop_size):\n",
        "    super().__init__()\n",
        "    # グラフ作成\n",
        "    graph = Graph(hop_size)\n",
        "    A = torch.tensor(graph.A, dtype=torch.float32, requires_grad=False)\n",
        "    self.register_buffer('A', A)\n",
        "    A_size = A.size()\n",
        "  \n",
        "    # Batch Normalization\n",
        "    self.bn = nn.BatchNorm1d(in_channels * A_size[1])\n",
        "    \n",
        "    # STGC_blocks\n",
        "    self.stgc1 = STGC_block(in_channels, 32, 1, t_kernel_size, A_size)\n",
        "    self.stgc2 = STGC_block(32, 32, 1, t_kernel_size, A_size)\n",
        "    self.stgc3 = STGC_block(32, 32, 1, t_kernel_size, A_size)\n",
        "    self.stgc4 = STGC_block(32, 64, 2, t_kernel_size, A_size)\n",
        "    self.stgc5 = STGC_block(64, 64, 1, t_kernel_size, A_size)\n",
        "    self.stgc6 = STGC_block(64, 64, 1, t_kernel_size, A_size)\n",
        "\n",
        "    # Prediction\n",
        "    self.fc = nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Batch Normalization\n",
        "    N, C, T, V = x.size() # batch, channel, frame, node\n",
        "    x = x.permute(0, 3, 1, 2).contiguous().view(N, V * C, T)\n",
        "    x = self.bn(x)\n",
        "    x = x.view(N, V, C, T).permute(0, 2, 3, 1).contiguous()\n",
        "\n",
        "    # STGC_blocks\n",
        "    x = self.stgc1(x, self.A)\n",
        "    x = self.stgc2(x, self.A)\n",
        "    x = self.stgc3(x, self.A)\n",
        "    x = self.stgc4(x, self.A)\n",
        "    x = self.stgc5(x, self.A)\n",
        "    x = self.stgc6(x, self.A)\n",
        "\n",
        "    # Prediction\n",
        "    x = F.avg_pool2d(x, x.size()[2:])\n",
        "    x = x.view(N, -1, 1, 1)\n",
        "    x = self.fc(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDCn2cuIYPG1"
      },
      "source": [
        "#モデルの学習\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk-AMCVb5jqM",
        "outputId": "9d3c2de9-eaeb-4f6a-f9ee-bef035a04b38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Epoch: 1 | Loss: 0.0363 | Accuracy: 12.9000\n",
            "# Epoch: 2 | Loss: 0.0350 | Accuracy: 21.1500\n",
            "# Epoch: 3 | Loss: 0.0339 | Accuracy: 23.6500\n",
            "# Epoch: 4 | Loss: 0.0328 | Accuracy: 24.7000\n",
            "# Epoch: 5 | Loss: 0.0318 | Accuracy: 27.8500\n",
            "# Epoch: 6 | Loss: 0.0311 | Accuracy: 28.6500\n",
            "# Epoch: 7 | Loss: 0.0292 | Accuracy: 35.8500\n",
            "# Epoch: 8 | Loss: 0.0279 | Accuracy: 36.6500\n",
            "# Epoch: 9 | Loss: 0.0263 | Accuracy: 42.2500\n",
            "# Epoch: 10 | Loss: 0.0241 | Accuracy: 46.7000\n",
            "# Epoch: 11 | Loss: 0.0206 | Accuracy: 54.0500\n",
            "# Epoch: 12 | Loss: 0.0185 | Accuracy: 57.4500\n",
            "# Epoch: 13 | Loss: 0.0171 | Accuracy: 61.4000\n",
            "# Epoch: 14 | Loss: 0.0172 | Accuracy: 61.3500\n",
            "# Epoch: 15 | Loss: 0.0165 | Accuracy: 62.2000\n",
            "# Epoch: 16 | Loss: 0.0158 | Accuracy: 64.3500\n",
            "# Epoch: 17 | Loss: 0.0153 | Accuracy: 65.7000\n",
            "# Epoch: 18 | Loss: 0.0146 | Accuracy: 68.7500\n",
            "# Epoch: 19 | Loss: 0.0147 | Accuracy: 69.6500\n",
            "# Epoch: 20 | Loss: 0.0133 | Accuracy: 72.8500\n",
            "# Epoch: 21 | Loss: 0.0126 | Accuracy: 73.3000\n",
            "# Epoch: 22 | Loss: 0.0123 | Accuracy: 74.7000\n",
            "# Epoch: 23 | Loss: 0.0118 | Accuracy: 75.5000\n",
            "# Epoch: 24 | Loss: 0.0117 | Accuracy: 76.1500\n",
            "# Epoch: 25 | Loss: 0.0108 | Accuracy: 77.9500\n",
            "# Epoch: 26 | Loss: 0.0105 | Accuracy: 77.7500\n",
            "# Epoch: 27 | Loss: 0.0101 | Accuracy: 78.9500\n",
            "# Epoch: 28 | Loss: 0.0096 | Accuracy: 79.8500\n",
            "# Epoch: 29 | Loss: 0.0097 | Accuracy: 79.6000\n",
            "# Epoch: 30 | Loss: 0.0096 | Accuracy: 78.8000\n",
            "# Epoch: 31 | Loss: 0.0099 | Accuracy: 78.3500\n",
            "# Epoch: 32 | Loss: 0.0084 | Accuracy: 82.1000\n",
            "# Epoch: 33 | Loss: 0.0086 | Accuracy: 82.2500\n",
            "# Epoch: 34 | Loss: 0.0088 | Accuracy: 82.0000\n",
            "# Epoch: 35 | Loss: 0.0079 | Accuracy: 82.6500\n",
            "# Epoch: 36 | Loss: 0.0081 | Accuracy: 82.7000\n",
            "# Epoch: 37 | Loss: 0.0076 | Accuracy: 82.8500\n",
            "# Epoch: 38 | Loss: 0.0074 | Accuracy: 83.8000\n",
            "# Epoch: 39 | Loss: 0.0072 | Accuracy: 84.6500\n",
            "# Epoch: 40 | Loss: 0.0072 | Accuracy: 84.8500\n",
            "# Epoch: 41 | Loss: 0.0075 | Accuracy: 83.5500\n",
            "# Epoch: 42 | Loss: 0.0078 | Accuracy: 83.0000\n",
            "# Epoch: 43 | Loss: 0.0067 | Accuracy: 85.6000\n",
            "# Epoch: 44 | Loss: 0.0065 | Accuracy: 85.4500\n",
            "# Epoch: 45 | Loss: 0.0065 | Accuracy: 85.4500\n",
            "# Epoch: 46 | Loss: 0.0070 | Accuracy: 85.1000\n",
            "# Epoch: 47 | Loss: 0.0068 | Accuracy: 85.1000\n",
            "# Epoch: 48 | Loss: 0.0063 | Accuracy: 86.5500\n",
            "# Epoch: 49 | Loss: 0.0065 | Accuracy: 85.3000\n",
            "# Epoch: 50 | Loss: 0.0060 | Accuracy: 86.6500\n",
            "# Epoch: 51 | Loss: 0.0067 | Accuracy: 85.7000\n",
            "# Epoch: 52 | Loss: 0.0065 | Accuracy: 85.1000\n",
            "# Epoch: 53 | Loss: 0.0059 | Accuracy: 86.8500\n",
            "# Epoch: 54 | Loss: 0.0058 | Accuracy: 87.8000\n",
            "# Epoch: 55 | Loss: 0.0062 | Accuracy: 87.0000\n",
            "# Epoch: 56 | Loss: 0.0063 | Accuracy: 86.2000\n",
            "# Epoch: 57 | Loss: 0.0059 | Accuracy: 86.6500\n",
            "# Epoch: 58 | Loss: 0.0057 | Accuracy: 87.1500\n",
            "# Epoch: 59 | Loss: 0.0060 | Accuracy: 86.7000\n",
            "# Epoch: 60 | Loss: 0.0058 | Accuracy: 87.6500\n",
            "# Epoch: 61 | Loss: 0.0054 | Accuracy: 87.3500\n",
            "# Epoch: 62 | Loss: 0.0055 | Accuracy: 87.6500\n",
            "# Epoch: 63 | Loss: 0.0055 | Accuracy: 88.2000\n",
            "# Epoch: 64 | Loss: 0.0055 | Accuracy: 87.6000\n",
            "# Epoch: 65 | Loss: 0.0052 | Accuracy: 88.2500\n",
            "# Epoch: 66 | Loss: 0.0050 | Accuracy: 88.4000\n",
            "# Epoch: 67 | Loss: 0.0052 | Accuracy: 88.5500\n",
            "# Epoch: 68 | Loss: 0.0047 | Accuracy: 89.8000\n",
            "# Epoch: 69 | Loss: 0.0050 | Accuracy: 88.8500\n",
            "# Epoch: 70 | Loss: 0.0055 | Accuracy: 87.9500\n",
            "# Epoch: 71 | Loss: 0.0047 | Accuracy: 89.2000\n",
            "# Epoch: 72 | Loss: 0.0050 | Accuracy: 88.2500\n",
            "# Epoch: 73 | Loss: 0.0050 | Accuracy: 88.3500\n",
            "# Epoch: 74 | Loss: 0.0050 | Accuracy: 88.0000\n",
            "# Epoch: 75 | Loss: 0.0052 | Accuracy: 88.4000\n",
            "# Epoch: 76 | Loss: 0.0044 | Accuracy: 89.8000\n",
            "# Epoch: 77 | Loss: 0.0049 | Accuracy: 88.8500\n",
            "# Epoch: 78 | Loss: 0.0047 | Accuracy: 89.4500\n",
            "# Epoch: 79 | Loss: 0.0048 | Accuracy: 89.1000\n",
            "# Epoch: 80 | Loss: 0.0050 | Accuracy: 88.6000\n",
            "# Epoch: 81 | Loss: 0.0046 | Accuracy: 90.1000\n",
            "# Epoch: 82 | Loss: 0.0046 | Accuracy: 89.0000\n",
            "# Epoch: 83 | Loss: 0.0044 | Accuracy: 90.0000\n",
            "# Epoch: 84 | Loss: 0.0042 | Accuracy: 89.8500\n",
            "# Epoch: 85 | Loss: 0.0041 | Accuracy: 90.0000\n",
            "# Epoch: 86 | Loss: 0.0041 | Accuracy: 90.5500\n",
            "# Epoch: 87 | Loss: 0.0043 | Accuracy: 91.0000\n",
            "# Epoch: 88 | Loss: 0.0046 | Accuracy: 89.9000\n",
            "# Epoch: 89 | Loss: 0.0042 | Accuracy: 90.1000\n",
            "# Epoch: 90 | Loss: 0.0046 | Accuracy: 89.8000\n",
            "# Epoch: 91 | Loss: 0.0043 | Accuracy: 89.9000\n",
            "# Epoch: 92 | Loss: 0.0039 | Accuracy: 91.2000\n",
            "# Epoch: 93 | Loss: 0.0041 | Accuracy: 90.6000\n",
            "# Epoch: 94 | Loss: 0.0038 | Accuracy: 91.7500\n",
            "# Epoch: 95 | Loss: 0.0039 | Accuracy: 91.6500\n",
            "# Epoch: 96 | Loss: 0.0037 | Accuracy: 91.8000\n",
            "# Epoch: 97 | Loss: 0.0044 | Accuracy: 90.3500\n",
            "# Epoch: 98 | Loss: 0.0037 | Accuracy: 91.0500\n",
            "# Epoch: 99 | Loss: 0.0042 | Accuracy: 90.3500\n",
            "# Epoch: 100 | Loss: 0.0043 | Accuracy: 90.5000\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCH = 100\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# モデルを作成\n",
        "model = ST_GCN(num_classes=10, \n",
        "                  in_channels=3,\n",
        "                  t_kernel_size=9, # 時間グラフ畳み込みのカーネルサイズ (t_kernel_size × 1)\n",
        "                  hop_size=2).cuda()\n",
        "\n",
        "# オプティマイザ\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# 誤差関数\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# データセットの用意\n",
        "data_loader = dict()\n",
        "data_loader['train'] = torch.utils.data.DataLoader(dataset=Feeder(data_path='data/train_data.npy', label_path='data/train_label.npy'), batch_size=BATCH_SIZE, shuffle=True,)\n",
        "data_loader['test'] = torch.utils.data.DataLoader(dataset=Feeder(data_path='data/test_data.npy', label_path='data/test_label.npy'), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# モデルを学習モードに変更\n",
        "model.train()\n",
        "\n",
        "# 学習開始\n",
        "for epoch in range(1, NUM_EPOCH+1):\n",
        "  correct = 0\n",
        "  sum_loss = 0\n",
        "  for batch_idx, (data, label) in enumerate(data_loader['train']):\n",
        "    data = data.cuda()\n",
        "    label = label.cuda()\n",
        "\n",
        "    output = model(data)\n",
        "\n",
        "    loss = criterion(output, label)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    sum_loss += loss.item()\n",
        "    _, predict = torch.max(output.data, 1)\n",
        "    correct += (predict == label).sum().item()\n",
        "\n",
        "  print('# Epoch: {} | Loss: {:.4f} | Accuracy: {:.4f}'.format(epoch, sum_loss/len(data_loader['train'].dataset), (100. * correct / len(data_loader['train'].dataset))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJmolf-w7yDJ"
      },
      "source": [
        "#モデルの評価\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "cHM5wt_670Li",
        "outputId": "f8a53905-c866-4999-848a-7bd92842a764"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Test Accuracy: 81.500[%]\n"
          ]
        }
      ],
      "source": [
        "# モデルを評価モードに変更\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "confusion_matrix = np.zeros((10, 10))\n",
        "with torch.no_grad():\n",
        "  for batch_idx, (data, label) in enumerate(data_loader['test']):\n",
        "    data = data.cuda()\n",
        "    label = label.cuda()\n",
        "\n",
        "    output = model(data)\n",
        "\n",
        "    _, predict = torch.max(output.data, 1)\n",
        "    correct += (predict == label).sum().item()\n",
        "\n",
        "    for l, p in zip(label.view(-1), predict.view(-1)):\n",
        "      confusion_matrix[l.long(), p.long()] += 1\n",
        "\n",
        "len_cm = len(confusion_matrix)\n",
        "for i in range(len_cm):\n",
        "    sum_cm = np.sum(confusion_matrix[i])\n",
        "    for j in range(len_cm):\n",
        "        confusion_matrix[i][j] = 100 * (confusion_matrix[i][j] / sum_cm)\n",
        "\n",
        "classes = ['drink', 'throw', 'sit down', 'stand up', 'clapping', 'hand waving', 'kicking', 'jump up', 'salute', 'falling down']\n",
        "plt.imshow(confusion_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion matrix')\n",
        "plt.tight_layout()\n",
        "tick_marks = np.arange(len(classes))\n",
        "plt.xticks(tick_marks, classes, rotation=45)\n",
        "plt.yticks(tick_marks, classes)\n",
        "plt.ylabel('True')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print('# Test Accuracy: {:.3f}[%]'.format(100. * correct / len(data_loader['test'].dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWjhdqGic0t9"
      },
      "source": [
        "#課題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URmf13hBc2Mw"
      },
      "source": [
        "骨格データに対するグラフ構造を変えて, ネットワークを学習してみましょう.\n",
        "\n",
        "骨格パターンによるグラフ構造は, 論文著者よる設計であり, それに従う必要はありません.  \n",
        "グラフ構造は自由な設計が許されています.  \n",
        "当然， 手と足など離れた関節を結んでも何の問題もありません.\n",
        "\n",
        "そこで, 自由にグラフ構造を設計し認識精度が上がるようなグラフ構造を発見してみましょう.  \n",
        "離れた関節を結んだり, エッジの数を増やすなど様々なグラフ構造が考えられます."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maEWdwZhdqP_"
      },
      "source": [
        "`class Graph`の`neighbor_base = [(1, 2), (2, 21), (3, 21), (4, 3), (5, 21),\n",
        "                      (6, 5), (7, 6), (8, 7), (9, 21), (10, 9),\n",
        "                      (11, 10), (12, 11), (13, 1), (14, 13), (15, 14),\n",
        "                      (16, 15), (17, 1), (18, 17), (19, 18), (20, 19),\n",
        "                      (22, 23), (23, 8), (24, 25), (25, 12)]`を変更することでグラフ構造を変えることができます.\n",
        "\n",
        "関節と関節の番号は以下の通りです."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_rwHqCJfJ7z"
      },
      "source": [
        "<img src='https://github.com/sirakik/MPRGLecture/blob/master/15_gcn/fig/03_skeleton.png?raw=true' width=30%>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "03_action_recognition_ST-GCN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
