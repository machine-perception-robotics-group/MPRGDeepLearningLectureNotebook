{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/15_gcn/03_action_recognition_ST_GCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_4yIWKExMcV"
      },
      "source": [
        "# ST-GCNによる動作認識\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsAg0haiIC1a"
      },
      "source": [
        "#骨格データからの動作認識\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GslML25UII5Y"
      },
      "source": [
        "骨格データからの動作認識を行います.  \n",
        "動作認識は, その人が何の動作(投げる, 蹴る, ジャンプ...)をしているかを認識するタスクです.  \n",
        "動作認識には,画像などから行う手法もありますが,ここでは骨格データを使います."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42iCYGbOIktX"
      },
      "source": [
        "###骨格データ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B7C36eHRrWb"
      },
      "source": [
        "骨格データとは，フレームごとの関節座標です.  \n",
        "しかしながら,素のデータはただの座標であり, グラフとは言えません.  \n",
        "\n",
        "人間の構造は関節と関節の繋がりで表現できます.つまり人間はグラフとして表現できます.\n",
        "\n",
        "ノードの特徴（座標）とエッジ（関節の繋がり）を使うことで，骨格データをグラフとして表現します.  \n",
        "グラフで表現することで,関節間の関係性を考慮することができます.\n",
        "\n",
        "グラフで表現した骨格データをGraph Convolutional Networks(GCN)に通して, 何の動作をしているかを認識します.\n",
        "\n",
        "下の動画が，グラフ表現をした骨格データの例です.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz6pKa3pt2kE"
      },
      "source": [
        "<!--\n",
        "<img src='https://drive.google.com/uc?id=1WrYd80u9buVcmBnpsgSZnzFlsYMih7Nr' width=30%>\n",
        "-->\n",
        "<!--\n",
        "<img src='https://github.com/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/15_gcn/fig/03_throw_skeleton.gif?raw=true' width=30%>\n",
        "-->\n",
        "<img src='https://github.com/sirakik/MPRGLecture/blob/master/15_gcn/fig/03_throw_skeleton.gif?raw=true' width=30%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty9FHb4yVB94"
      },
      "source": [
        "#Spatial Temporal Graph Convolutional Networks\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnhYROVaVSXp"
      },
      "source": [
        "GCNを使った骨格データからの動作認識の代表的な手法として, Spatial Temporal Graph Convolutional Networks(ST-GCN)[[arXiv](https://arxiv.org/abs/1801.07455)]があります."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwpdRsahuO-S"
      },
      "source": [
        "<!--\n",
        "<img src='https://drive.google.com/uc?id=1ZRf-NF4S0P1VwMxN2DrTFPeO4EJ5if3S' width=100%>\n",
        "-->\n",
        "<!--\n",
        "<img src='https://github.com/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/15_gcn/fig/03_st-gcn.png?raw=true' width=100%>\n",
        "-->\n",
        "<img src='https://github.com/sirakik/MPRGLecture/blob/master/15_gcn/fig/03_st-gcn.png?raw=true' width=100%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U_7UNeBWMN8"
      },
      "source": [
        "ST-GCNの特徴は,骨格データを2つのグラフ構造として表現したことです.\n",
        "- 空間グラフ：同一フレーム内の関節を結ぶグラフ　\n",
        "- 時間グラフ：隣接フレームの同一関節を結ぶグラフ\n",
        "\n",
        "空間グラフと時間グラフをGraph Covnolutionによって特徴を抽出することで，関節間の関係と, 時間的な変化を考慮しています. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAhpuPkHuf2E"
      },
      "source": [
        "<!--\n",
        "<img src='https://drive.google.com/uc?id=1FDOGPZxaIYs-be-6tZzPeBsrtMcBXcuv' width=30%>\n",
        "-->\n",
        "<!--\n",
        "<img src='https://github.com/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/15_gcn/fig/03_st_graph.png?raw=true' width=30%>\n",
        "-->\n",
        "<img src='https://github.com/sirakik/MPRGLecture/blob/master/15_gcn/fig/03_st_graph.png?raw=true' width=30%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9_Um9Dy3ZeI"
      },
      "source": [
        "# 実装前の準備\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa7D5v0f3e_E"
      },
      "source": [
        "必要なモジュールのインポート"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "rwUrefn04Vjw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78xIwKEz4G69"
      },
      "source": [
        "GPU確認　　\n",
        "\n",
        "今回からGPUを使用して学習します. 確認してください."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Use CUDA:', torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bx-2j-X4XC4",
        "outputId": "6240905f-6b4c-48c9-b92b-7654884cf389"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use CUDA: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "シード値固定"
      ],
      "metadata": {
        "id": "OkxZo2Id1Bzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 123\n",
        "# Numpy\n",
        "np.random.seed(seed)\n",
        "# Pytorch\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.use_deterministic_algorithms = True"
      ],
      "metadata": {
        "id": "nlsCRsXu4ZAi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5Zpy-Z0YFJS"
      },
      "source": [
        "# データセット\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLl5gIVi4pTn"
      },
      "source": [
        "データセットをダンロードします.今回は，小さな独自のデータセット(注1)を用意したので，それを使用します.  \n",
        "動作クラス数は10クラス（0~9）です.動作は以下の通りです.\n",
        "*   0:　飲む\n",
        "*   1:　投げる\n",
        "*   2:　座る\n",
        "*   3:　立ち上がる\n",
        "*   4:　拍手\n",
        "*   5:　手を振る\n",
        "*   6:　蹴る\n",
        "*   7:　ジャンプ\n",
        "*   8:　敬礼\n",
        "*   9:　転倒\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-9AZzvcT_7o"
      },
      "source": [
        "注1: このデータセットは[NTU-RGB+Dデータセット](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shahroudy_NTU_RGBD_A_CVPR_2016_paper.pdf)を利用して作成しました.  \n",
        "Portions of the research used the NTU RGB+D Action Recognition Dataset made available by the ROSE Lab at the Nanyang Technological University, Singapore."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q http://mprg.cs.chubu.ac.jp/~itaya/share/mprg_colab/NTU-RGBD_data/data.zip\n",
        "!unzip -q -o data.zip"
      ],
      "metadata": {
        "id": "TLBsZgvk4ayd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYixFxzPfgqd"
      },
      "source": [
        "学習データ数が2000(10クラス×200データ), 評価データ数が200(10クラス×20データ)あります.\n",
        "\n",
        "データの構造を確認します.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = np.load(\"data/test_data.npy\")\n",
        "print(test_data[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcAV8Axl4b_1",
        "outputId": "5d4e0219-ab28-4d39-cbcf-1bdba0e5774e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 80, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftiwD1KhoFVS"
      },
      "source": [
        "(次元数，フレーム数，関節数)の構造です.  \n",
        "1データあたり,3次元座標の25関節が80フレーム分入っています."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# データを読み込むための関数\n",
        "class Feeder(torch.utils.data.Dataset):\n",
        "  def __init__(self, data_path, label_path):\n",
        "      super().__init__()\n",
        "      self.label = np.load(label_path)\n",
        "      self.data = np.load(data_path)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.label)\n",
        "\n",
        "  def __iter__(self):\n",
        "      return self\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      data = np.array(self.data[index])\n",
        "      label = self.label[index]\n",
        "\n",
        "      return data, label"
      ],
      "metadata": {
        "id": "Asb7794y4dys"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEA6NlHGaGSb"
      },
      "source": [
        "### 隣接行列を作成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFYyypxdo3_G"
      },
      "source": [
        "今は座標データ（ノード特徴）しかないため，グラフではありません．  \n",
        "接続関係を定義して，グラフにします．接続関係の表現には隣接行列を使用します.  \n",
        "class化しておきます. モデルの定義する際に呼び出します.\n",
        "\n",
        "隣接行列を手作業で編集するのは大変であるため，接続関係を配列で用意し，それをもとに隣接行列を作ります."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IFV6MAuFds7V"
      },
      "outputs": [],
      "source": [
        "class Graph():\n",
        "  def __init__(self, hop_size):\n",
        "    # エッジ配列を宣言します. 集合としては{{始点, 終点}, {始点, 終点}, {始点, 終点}...}のように一つのエッジを要素として宣言します.\n",
        "    self.get_edge()\n",
        "    \n",
        "    # hop: hop数分離れた関節を結びます.\n",
        "    # 例えばhop=2だと, 手首は肘だけではなく肩にも繋がっています.\n",
        "    self.hop_size = hop_size \n",
        "    self.hop_dis = self.get_hop_distance(self.num_node, self.edge, hop_size=hop_size)\n",
        "\n",
        "    # 隣接行列を作ります.ここではhop数ごとに隣接行列を作成します.\n",
        "    # hopが2の時, 0hop, 1hop, 2hopの３つの隣接行列が作成されます.\n",
        "    # 複数の生成方法が論文中に提案されています. 今回はわかりやすいものを使いました.\n",
        "    self.get_adjacency() \n",
        "\n",
        "  def __str__(self):\n",
        "    return self.A\n",
        "\n",
        "  def get_edge(self):\n",
        "    self.num_node = 25\n",
        "    self_link = [(i, i) for i in range(self.num_node)] # ループ\n",
        "    neighbor_base = [(1, 2), (2, 21), (3, 21), (4, 3), (5, 21),\n",
        "                      (6, 5), (7, 6), (8, 7), (9, 21), (10, 9),\n",
        "                      (11, 10), (12, 11), (13, 1), (14, 13), (15, 14),\n",
        "                      (16, 15), (17, 1), (18, 17), (19, 18), (20, 19),\n",
        "                      (22, 23), (23, 8), (24, 25), (25, 12)]\n",
        "    neighbor_link = [(i - 1, j - 1) for (i, j) in neighbor_base]\n",
        "    self.edge = self_link + neighbor_link\n",
        "\n",
        "  def get_adjacency(self):\n",
        "    valid_hop = range(0, self.hop_size + 1, 1)\n",
        "    adjacency = np.zeros((self.num_node, self.num_node))\n",
        "    for hop in valid_hop:\n",
        "        adjacency[self.hop_dis == hop] = 1\n",
        "    normalize_adjacency = self.normalize_digraph(adjacency)\n",
        "    A = np.zeros((len(valid_hop), self.num_node, self.num_node))\n",
        "    for i, hop in enumerate(valid_hop):\n",
        "        A[i][self.hop_dis == hop] = normalize_adjacency[self.hop_dis == hop]\n",
        "    self.A = A\n",
        "\n",
        "  def get_hop_distance(self, num_node, edge, hop_size):\n",
        "    A = np.zeros((num_node, num_node))\n",
        "    for i, j in edge:\n",
        "        A[j, i] = 1\n",
        "        A[i, j] = 1\n",
        "    hop_dis = np.zeros((num_node, num_node)) + np.inf\n",
        "    transfer_mat = [np.linalg.matrix_power(A, d) for d in range(hop_size + 1)]\n",
        "    arrive_mat = (np.stack(transfer_mat) > 0)\n",
        "    for d in range(hop_size, -1, -1):\n",
        "        hop_dis[arrive_mat[d]] = d\n",
        "    return hop_dis\n",
        "\n",
        "  def normalize_digraph(self, A):\n",
        "    Dl = np.sum(A, 0)\n",
        "    num_node = A.shape[0]\n",
        "    Dn = np.zeros((num_node, num_node))\n",
        "    for i in range(num_node):\n",
        "        if Dl[i] > 0:\n",
        "            Dn[i, i] = Dl[i]**(-1)\n",
        "    DAD = np.dot(A, Dn)\n",
        "    return DAD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTCb52YUYHYh"
      },
      "source": [
        "#ST-GCN実装\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H6jSUH9YdCQ"
      },
      "source": [
        "###空間グラフの畳み込み  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvIeqwf7tT1K"
      },
      "source": [
        "まずは，空間グラフ(人間の接続パターン)のグラフ畳み込みを実装します.  \n",
        "式は前回(Graph Convolutional Networksによるノード分類)に示したGraph Convolutionとほぼ同じです.\n",
        "\\begin{equation}\n",
        "{\\bf H}_{out}=\\sum_{j}{\\bf\\tilde D}^{-\\frac{1}{2}}_j{\\bf\\tilde A}_j{\\bf\\tilde D}^{-\\frac{1}{2}}_j{\\bf H}_{in}{\\bf W}_{j}\n",
        "\\end{equation}\n",
        "hop数分の隣接行列($j$:隣接行列の数)があるため，各隣接行列で畳み込んでから，特徴を足し合わせています.  \n",
        "\n",
        "高速化や，今後の拡張性のため，前回のGCとは実装を変えています.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lrffcNi0ZCP7"
      },
      "outputs": [],
      "source": [
        "class SpatialGraphConvolution(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, s_kernel_size):\n",
        "    super().__init__()\n",
        "    self.s_kernel_size = s_kernel_size\n",
        "    self.conv = nn.Conv2d(in_channels=in_channels,\n",
        "                          out_channels=out_channels * s_kernel_size,\n",
        "                          kernel_size=1)\n",
        "    \n",
        "  def forward(self, x, A):\n",
        "    x = self.conv(x)\n",
        "    n, kc, t, v = x.size()\n",
        "    x = x.view(n, self.s_kernel_size, kc//self.s_kernel_size, t, v)\n",
        "    # 隣接行列にGCを行い, 特徴を足し合わせています.\n",
        "    x = torch.einsum('nkctv,kvw->nctw', (x, A))\n",
        "    return x.contiguous()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kosLltaeYg55"
      },
      "source": [
        "###時間グラフの畳み込み"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5T1vQz6bvsu"
      },
      "source": [
        "時間グラフは,グラフ畳み込み処理ではなく一般的な2d畳み込み処理で実装できます.  \n",
        "特徴マップは（フレーム数×関節数)の形になっています. 同一関節をフレーム方向に繋いだものが時間グラフです.  \n",
        "フレーム方向に畳み込めばいいため（$T\\times 1$）の2d畳み込みフィルターで実装できます.\n",
        "\n",
        "また, ST-GCNは, 空間グラフと時間グラフの畳み込みを交互に行います.  \n",
        "これを繰り返すので, 空間グラフと時間グラフ，その他(活性化関数やdropout)を備えたクラス(STGC_block)を作っておきます."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mPcF2Od9AvmA"
      },
      "outputs": [],
      "source": [
        "class STGC_block(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride, t_kernel_size, A_size, dropout=0.5):\n",
        "    super().__init__()\n",
        "    # 空間グラフの畳み込み\n",
        "    self.sgc = SpatialGraphConvolution(in_channels=in_channels,\n",
        "                                       out_channels=out_channels,\n",
        "                                       s_kernel_size=A_size[0])\n",
        "    \n",
        "    # Learnable weight matrix M エッジに重みを与えます. どのエッジが重要かを学習します.\n",
        "    self.M = nn.Parameter(torch.ones(A_size))\n",
        "\n",
        "    self.tgc = nn.Sequential(nn.BatchNorm2d(out_channels),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Dropout(dropout),\n",
        "                            nn.Conv2d(out_channels,\n",
        "                                      out_channels,\n",
        "                                      (t_kernel_size, 1),\n",
        "                                      (stride, 1),\n",
        "                                      ((t_kernel_size - 1) // 2, 0)),\n",
        "                            nn.BatchNorm2d(out_channels),\n",
        "                            nn.ReLU())\n",
        "\n",
        "  def forward(self, x, A):\n",
        "    x = self.tgc(self.sgc(x, A * self.M))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy0nx9SvYod1"
      },
      "source": [
        "###ネットワークモデル"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lzAe1GgpZv7k"
      },
      "outputs": [],
      "source": [
        "class ST_GCN(nn.Module):\n",
        "  def __init__(self, num_classes, in_channels, t_kernel_size, hop_size):\n",
        "    super().__init__()\n",
        "    # グラフ作成\n",
        "    graph = Graph(hop_size)\n",
        "    A = torch.tensor(graph.A, dtype=torch.float32, requires_grad=False)\n",
        "    self.register_buffer('A', A)\n",
        "    A_size = A.size()\n",
        "  \n",
        "    # Batch Normalization\n",
        "    self.bn = nn.BatchNorm1d(in_channels * A_size[1])\n",
        "    \n",
        "    # STGC_blocks\n",
        "    self.stgc1 = STGC_block(in_channels, 32, 1, t_kernel_size, A_size)\n",
        "    self.stgc2 = STGC_block(32, 32, 1, t_kernel_size, A_size)\n",
        "    self.stgc3 = STGC_block(32, 32, 1, t_kernel_size, A_size)\n",
        "    self.stgc4 = STGC_block(32, 64, 2, t_kernel_size, A_size)\n",
        "    self.stgc5 = STGC_block(64, 64, 1, t_kernel_size, A_size)\n",
        "    self.stgc6 = STGC_block(64, 64, 1, t_kernel_size, A_size)\n",
        "\n",
        "    # Prediction\n",
        "    self.fc = nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Batch Normalization\n",
        "    N, C, T, V = x.size() # batch, channel, frame, node\n",
        "    x = x.permute(0, 3, 1, 2).contiguous().view(N, V * C, T)\n",
        "    x = self.bn(x)\n",
        "    x = x.view(N, V, C, T).permute(0, 2, 3, 1).contiguous()\n",
        "\n",
        "    # STGC_blocks\n",
        "    x = self.stgc1(x, self.A)\n",
        "    x = self.stgc2(x, self.A)\n",
        "    x = self.stgc3(x, self.A)\n",
        "    x = self.stgc4(x, self.A)\n",
        "    x = self.stgc5(x, self.A)\n",
        "    x = self.stgc6(x, self.A)\n",
        "\n",
        "    # Prediction\n",
        "    x = F.avg_pool2d(x, x.size()[2:])\n",
        "    x = x.view(N, -1, 1, 1)\n",
        "    x = self.fc(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDCn2cuIYPG1"
      },
      "source": [
        "#モデルの学習\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCH = 100\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# モデルを作成\n",
        "model = ST_GCN(num_classes=10, \n",
        "                  in_channels=3,\n",
        "                  t_kernel_size=9, # 時間グラフ畳み込みのカーネルサイズ (t_kernel_size × 1)\n",
        "                  hop_size=2).cuda()\n",
        "\n",
        "# オプティマイザ\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# 誤差関数\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# データセットの用意\n",
        "data_loader = dict()\n",
        "data_loader['train'] = torch.utils.data.DataLoader(dataset=Feeder(data_path='data/train_data.npy', label_path='data/train_label.npy'), batch_size=BATCH_SIZE, shuffle=True,)\n",
        "data_loader['test'] = torch.utils.data.DataLoader(dataset=Feeder(data_path='data/test_data.npy', label_path='data/test_label.npy'), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# モデルを学習モードに変更\n",
        "model.train()\n",
        "\n",
        "# 学習開始\n",
        "for epoch in range(1, NUM_EPOCH+1):\n",
        "  correct = 0\n",
        "  sum_loss = 0\n",
        "  for batch_idx, (data, label) in enumerate(data_loader['train']):\n",
        "    data = data.cuda()\n",
        "    label = label.cuda()\n",
        "\n",
        "    output = model(data)\n",
        "\n",
        "    loss = criterion(output, label)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    sum_loss += loss.item()\n",
        "    _, predict = torch.max(output.data, 1)\n",
        "    correct += (predict == label).sum().item()\n",
        "\n",
        "  print('# Epoch: {} | Loss: {:.4f} | Accuracy: {:.4f}'.format(epoch, sum_loss/len(data_loader['train'].dataset), (100. * correct / len(data_loader['train'].dataset))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8fuGK9R4jlz",
        "outputId": "290f32e3-59d8-40eb-bbf0-b3dee596fb65"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Epoch: 1 | Loss: 0.0359 | Accuracy: 15.0500\n",
            "# Epoch: 2 | Loss: 0.0345 | Accuracy: 20.4000\n",
            "# Epoch: 3 | Loss: 0.0338 | Accuracy: 23.2000\n",
            "# Epoch: 4 | Loss: 0.0328 | Accuracy: 26.1500\n",
            "# Epoch: 5 | Loss: 0.0313 | Accuracy: 29.5000\n",
            "# Epoch: 6 | Loss: 0.0298 | Accuracy: 32.7000\n",
            "# Epoch: 7 | Loss: 0.0284 | Accuracy: 36.7500\n",
            "# Epoch: 8 | Loss: 0.0269 | Accuracy: 40.7000\n",
            "# Epoch: 9 | Loss: 0.0247 | Accuracy: 46.1500\n",
            "# Epoch: 10 | Loss: 0.0216 | Accuracy: 53.3500\n",
            "# Epoch: 11 | Loss: 0.0195 | Accuracy: 56.3500\n",
            "# Epoch: 12 | Loss: 0.0178 | Accuracy: 59.3500\n",
            "# Epoch: 13 | Loss: 0.0175 | Accuracy: 60.5500\n",
            "# Epoch: 14 | Loss: 0.0177 | Accuracy: 58.2000\n",
            "# Epoch: 15 | Loss: 0.0165 | Accuracy: 61.4000\n",
            "# Epoch: 16 | Loss: 0.0163 | Accuracy: 63.3000\n",
            "# Epoch: 17 | Loss: 0.0155 | Accuracy: 64.7000\n",
            "# Epoch: 18 | Loss: 0.0147 | Accuracy: 66.6000\n",
            "# Epoch: 19 | Loss: 0.0142 | Accuracy: 68.2500\n",
            "# Epoch: 20 | Loss: 0.0140 | Accuracy: 68.9500\n",
            "# Epoch: 21 | Loss: 0.0137 | Accuracy: 70.5000\n",
            "# Epoch: 22 | Loss: 0.0139 | Accuracy: 70.0000\n",
            "# Epoch: 23 | Loss: 0.0130 | Accuracy: 72.3000\n",
            "# Epoch: 24 | Loss: 0.0119 | Accuracy: 75.0500\n",
            "# Epoch: 25 | Loss: 0.0111 | Accuracy: 77.5000\n",
            "# Epoch: 26 | Loss: 0.0113 | Accuracy: 76.1000\n",
            "# Epoch: 27 | Loss: 0.0102 | Accuracy: 77.9500\n",
            "# Epoch: 28 | Loss: 0.0103 | Accuracy: 77.8500\n",
            "# Epoch: 29 | Loss: 0.0100 | Accuracy: 80.0500\n",
            "# Epoch: 30 | Loss: 0.0093 | Accuracy: 80.0500\n",
            "# Epoch: 31 | Loss: 0.0092 | Accuracy: 80.0500\n",
            "# Epoch: 32 | Loss: 0.0087 | Accuracy: 81.6000\n",
            "# Epoch: 33 | Loss: 0.0086 | Accuracy: 81.8000\n",
            "# Epoch: 34 | Loss: 0.0088 | Accuracy: 81.0000\n",
            "# Epoch: 35 | Loss: 0.0082 | Accuracy: 83.9000\n",
            "# Epoch: 36 | Loss: 0.0090 | Accuracy: 80.4500\n",
            "# Epoch: 37 | Loss: 0.0082 | Accuracy: 82.7500\n",
            "# Epoch: 38 | Loss: 0.0077 | Accuracy: 83.5000\n",
            "# Epoch: 39 | Loss: 0.0075 | Accuracy: 83.9000\n",
            "# Epoch: 40 | Loss: 0.0071 | Accuracy: 84.7500\n",
            "# Epoch: 41 | Loss: 0.0071 | Accuracy: 84.4500\n",
            "# Epoch: 42 | Loss: 0.0078 | Accuracy: 84.2500\n",
            "# Epoch: 43 | Loss: 0.0073 | Accuracy: 84.6000\n",
            "# Epoch: 44 | Loss: 0.0075 | Accuracy: 83.1000\n",
            "# Epoch: 45 | Loss: 0.0069 | Accuracy: 84.8000\n",
            "# Epoch: 46 | Loss: 0.0064 | Accuracy: 85.9000\n",
            "# Epoch: 47 | Loss: 0.0060 | Accuracy: 86.6500\n",
            "# Epoch: 48 | Loss: 0.0067 | Accuracy: 85.1500\n",
            "# Epoch: 49 | Loss: 0.0062 | Accuracy: 87.1000\n",
            "# Epoch: 50 | Loss: 0.0063 | Accuracy: 86.2500\n",
            "# Epoch: 51 | Loss: 0.0063 | Accuracy: 85.8500\n",
            "# Epoch: 52 | Loss: 0.0060 | Accuracy: 86.9500\n",
            "# Epoch: 53 | Loss: 0.0060 | Accuracy: 86.6000\n",
            "# Epoch: 54 | Loss: 0.0060 | Accuracy: 86.5000\n",
            "# Epoch: 55 | Loss: 0.0060 | Accuracy: 86.9000\n",
            "# Epoch: 56 | Loss: 0.0059 | Accuracy: 86.7500\n",
            "# Epoch: 57 | Loss: 0.0052 | Accuracy: 88.6000\n",
            "# Epoch: 58 | Loss: 0.0053 | Accuracy: 88.9000\n",
            "# Epoch: 59 | Loss: 0.0054 | Accuracy: 87.4000\n",
            "# Epoch: 60 | Loss: 0.0053 | Accuracy: 87.6500\n",
            "# Epoch: 61 | Loss: 0.0053 | Accuracy: 88.4500\n",
            "# Epoch: 62 | Loss: 0.0053 | Accuracy: 87.6500\n",
            "# Epoch: 63 | Loss: 0.0052 | Accuracy: 88.0000\n",
            "# Epoch: 64 | Loss: 0.0049 | Accuracy: 88.4500\n",
            "# Epoch: 65 | Loss: 0.0053 | Accuracy: 87.9500\n",
            "# Epoch: 66 | Loss: 0.0053 | Accuracy: 88.7000\n",
            "# Epoch: 67 | Loss: 0.0052 | Accuracy: 88.4500\n",
            "# Epoch: 68 | Loss: 0.0054 | Accuracy: 87.2000\n",
            "# Epoch: 69 | Loss: 0.0052 | Accuracy: 88.5000\n",
            "# Epoch: 70 | Loss: 0.0049 | Accuracy: 89.2500\n",
            "# Epoch: 71 | Loss: 0.0052 | Accuracy: 88.4000\n",
            "# Epoch: 72 | Loss: 0.0049 | Accuracy: 89.2500\n",
            "# Epoch: 73 | Loss: 0.0051 | Accuracy: 88.7500\n",
            "# Epoch: 74 | Loss: 0.0048 | Accuracy: 89.2500\n",
            "# Epoch: 75 | Loss: 0.0047 | Accuracy: 88.7500\n",
            "# Epoch: 76 | Loss: 0.0047 | Accuracy: 89.2500\n",
            "# Epoch: 77 | Loss: 0.0045 | Accuracy: 89.2000\n",
            "# Epoch: 78 | Loss: 0.0045 | Accuracy: 89.7500\n",
            "# Epoch: 79 | Loss: 0.0045 | Accuracy: 89.1500\n",
            "# Epoch: 80 | Loss: 0.0042 | Accuracy: 89.5500\n",
            "# Epoch: 81 | Loss: 0.0042 | Accuracy: 90.2000\n",
            "# Epoch: 82 | Loss: 0.0046 | Accuracy: 89.4500\n",
            "# Epoch: 83 | Loss: 0.0044 | Accuracy: 90.1500\n",
            "# Epoch: 84 | Loss: 0.0044 | Accuracy: 90.0500\n",
            "# Epoch: 85 | Loss: 0.0046 | Accuracy: 89.9000\n",
            "# Epoch: 86 | Loss: 0.0045 | Accuracy: 89.8000\n",
            "# Epoch: 87 | Loss: 0.0046 | Accuracy: 89.8000\n",
            "# Epoch: 88 | Loss: 0.0043 | Accuracy: 90.9500\n",
            "# Epoch: 89 | Loss: 0.0038 | Accuracy: 90.9500\n",
            "# Epoch: 90 | Loss: 0.0040 | Accuracy: 91.2500\n",
            "# Epoch: 91 | Loss: 0.0043 | Accuracy: 89.9000\n",
            "# Epoch: 92 | Loss: 0.0041 | Accuracy: 90.8500\n",
            "# Epoch: 93 | Loss: 0.0042 | Accuracy: 90.5500\n",
            "# Epoch: 94 | Loss: 0.0038 | Accuracy: 91.6500\n",
            "# Epoch: 95 | Loss: 0.0038 | Accuracy: 91.4000\n",
            "# Epoch: 96 | Loss: 0.0035 | Accuracy: 91.1000\n",
            "# Epoch: 97 | Loss: 0.0038 | Accuracy: 91.3500\n",
            "# Epoch: 98 | Loss: 0.0042 | Accuracy: 90.8000\n",
            "# Epoch: 99 | Loss: 0.0037 | Accuracy: 91.5000\n",
            "# Epoch: 100 | Loss: 0.0035 | Accuracy: 92.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJmolf-w7yDJ"
      },
      "source": [
        "#モデルの評価\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルを評価モードに変更\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "confusion_matrix = np.zeros((10, 10))\n",
        "with torch.no_grad():\n",
        "  for batch_idx, (data, label) in enumerate(data_loader['test']):\n",
        "    data = data.cuda()\n",
        "    label = label.cuda()\n",
        "\n",
        "    output = model(data)\n",
        "\n",
        "    _, predict = torch.max(output.data, 1)\n",
        "    correct += (predict == label).sum().item()\n",
        "\n",
        "    for l, p in zip(label.view(-1), predict.view(-1)):\n",
        "      confusion_matrix[l.long(), p.long()] += 1\n",
        "\n",
        "len_cm = len(confusion_matrix)\n",
        "for i in range(len_cm):\n",
        "    sum_cm = np.sum(confusion_matrix[i])\n",
        "    for j in range(len_cm):\n",
        "        confusion_matrix[i][j] = 100 * (confusion_matrix[i][j] / sum_cm)\n",
        "\n",
        "classes = ['drink', 'throw', 'sit down', 'stand up', 'clapping', 'hand waving', 'kicking', 'jump up', 'salute', 'falling down']\n",
        "plt.imshow(confusion_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion matrix')\n",
        "plt.tight_layout()\n",
        "tick_marks = np.arange(len(classes))\n",
        "plt.xticks(tick_marks, classes, rotation=45)\n",
        "plt.yticks(tick_marks, classes)\n",
        "plt.ylabel('True')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print('# Test Accuracy: {:.3f}[%]'.format(100. * correct / len(data_loader['test'].dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "_ovKpsCn4l2M",
        "outputId": "d4951193-9408-4554-943a-2ad13ffbf84f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Test Accuracy: 80.000[%]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWjhdqGic0t9"
      },
      "source": [
        "#課題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URmf13hBc2Mw"
      },
      "source": [
        "骨格データに対するグラフ構造を変えて, ネットワークを学習してみましょう.\n",
        "\n",
        "骨格パターンによるグラフ構造は, 論文著者よる設計であり, それに従う必要はありません.  \n",
        "グラフ構造は自由な設計が許されています.  \n",
        "当然， 手と足など離れた関節を結んでも何の問題もありません.\n",
        "\n",
        "そこで, 自由にグラフ構造を設計し認識精度が上がるようなグラフ構造を発見してみましょう.  \n",
        "離れた関節を結んだり, エッジの数を増やすなど様々なグラフ構造が考えられます."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maEWdwZhdqP_"
      },
      "source": [
        "`class Graph`の`neighbor_base = [(1, 2), (2, 21), (3, 21), (4, 3), (5, 21),\n",
        "                      (6, 5), (7, 6), (8, 7), (9, 21), (10, 9),\n",
        "                      (11, 10), (12, 11), (13, 1), (14, 13), (15, 14),\n",
        "                      (16, 15), (17, 1), (18, 17), (19, 18), (20, 19),\n",
        "                      (22, 23), (23, 8), (24, 25), (25, 12)]`を変更することでグラフ構造を変えることができます.\n",
        "\n",
        "関節と関節の番号は以下の通りです."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_rwHqCJfJ7z"
      },
      "source": [
        "<img src='https://github.com/sirakik/MPRGLecture/blob/master/15_gcn/fig/03_skeleton.png?raw=true' width=30%>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}