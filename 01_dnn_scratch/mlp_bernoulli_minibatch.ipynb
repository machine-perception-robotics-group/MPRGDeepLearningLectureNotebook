{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu4LvpLYYfK0"
      },
      "source": [
        "# ミニバッチを用いたMLPの学習\n",
        "\n",
        "\n",
        "---\n",
        "## 目的\n",
        "多層パーセプトロン（Multi Layer Perceptron; MLP）を用いて，乳癌データの2クラス分類を行う．\n",
        "ミニバッチ学習について理解する．\n",
        "\n",
        "\n",
        "## モジュールのインポート\n",
        "プログラムの実行に必要なモジュールをインポートします．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWNhsquGYfK1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfrgHO95YfK5"
      },
      "source": [
        "## データセットの読み込み\n",
        "実験に使用するデータセットを読み込みます．\n",
        "\n",
        "今回は**Breast Cancer Wisconsin Dataset**を用いて2クラス分類を行います．\n",
        "breast cancer datasetは乳癌のデータセットであり，クラス数は悪性腫瘍 (malignant)と良性腫瘍 (benign) の2クラス，データ数は569（悪性腫瘍 (malignant): 220, 良性腫瘍 (benign): 357）のデータセットです．\n",
        "各データは細胞核の半径や面積，テクスチャ情報を表現した30次元のベクトルデータです．\n",
        "\n",
        "[Breast Cancer Wisconsin (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))\n",
        "\n",
        "はじめに，`wget`コマンドを使用して，データセットのファイルをダウンロードします．\n",
        "\n",
        "次に，データと正解ラベルが含まれている`breast_cancer.csv`を読み込みます．\n",
        "読み込んだデータのうち，最初の30列に各データを表現した30次元のベクトルデータが格納されており，最後の1列に正解ラベル`(0, 1)`が格納されています．これらをそれぞれ，`x`と`y`に分割して格納します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHu_uD9jYfK5"
      },
      "outputs": [],
      "source": [
        "# データセットのダウンロード\n",
        "!wget -q http://www.mprg.cs.chubu.ac.jp/~hirakawa/share/tutorial_data/breast_cancer.csv -O breast_cancer.csv\n",
        "\n",
        "# データセットの読み込み\n",
        "breast_cancer_data = np.loadtxt(\"breast_cancer.csv\", dtype=np.float32, delimiter=\",\")\n",
        "x = breast_cancer_data[:, :-1]\n",
        "y = breast_cancer_data[:, -1].astype(np.int32)\n",
        "\n",
        "print(x.shape, x.dtype)\n",
        "print(y.shape, y.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffsUIUvlYfK8"
      },
      "source": [
        "## データの分割と正規化\n",
        "上記で読み込んだデータを学習用データとテストデータに分割し，正規化を行います．\n",
        "\n",
        "データの分割では，`test_sample_ratio`で，テストに用いるサンプルの割合を指定します．\n",
        "その後，データの総数から，学習とテストにするデータの数を算出し，ランダムにサンプルを振り分けます．\n",
        "このとき，`np.random.seed`はデータをランダムに分割する際のseedです．\n",
        "seedを変更，または指定しないことで，無作為にデータを分割することが可能です．\n",
        "\n",
        "次に正規化を行います．\n",
        "データ$x$の最小値を$x_{min}$，最大値を$x_{max}$としたとき，次の式で正規化を行います．\n",
        "$$x_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
        "\n",
        "`np.min`と`np.max`で学習データの最大，最小値を取得し，上記の式に従い0~1の範囲に値を正規化します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gojd7wjYfK8"
      },
      "outputs": [],
      "source": [
        "# データの分割\n",
        "test_sample_ratio = 0.2  # テストデータの割合を指定\n",
        "num_data = x.shape[0]    # データの総数\n",
        "num_test = int(num_data * test_sample_ratio)\n",
        "num_train = num_data - num_test\n",
        "\n",
        "np.random.seed(seed=0)\n",
        "random_index = np.random.permutation(num_data)\n",
        "x_train = x[random_index[0:num_train]]\n",
        "y_train = y[random_index[0:num_train]]\n",
        "x_test = x[random_index[num_train:]]\n",
        "y_test = y[random_index[num_train:]]\n",
        "\n",
        "# データの正規化\n",
        "x_min = np.min(x_train, axis=0)\n",
        "x_max = np.max(x_train, axis=0)\n",
        "\n",
        "x_train = (x_train[:, ] - x_min) / (x_max - x_min)\n",
        "x_test = (x_test[:, ] - x_min) / (x_max - x_min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4X7pcUGYfK_"
      },
      "source": [
        "## ミニバッチ学習を考慮したネットワークモデルの定義\n",
        "次に，ニューラルネットワーク（多層パーセプトロン）を定義します．\n",
        "\n",
        "---\n",
        "### ミニバッチ学習\n",
        "前回の演習プログラムでは，1サンプルごとに学習するオンライン学習とデータセットのサンプル全てを一度に使用して学習するバッチ学習を行いました．\n",
        "\n",
        "それに対して，**ミニバッチ学習**は，事前に決定した数のサンプルを一度に使用してネットワークのパラメータ更新を行う学習方法です．\n",
        "例えば，ミニバッチサイズ（バッチサイズ）を10とした場合，データセットの中からランダムに10サンプルを選択し，それらのサンプルから得られる勾配情報を元にパラメータ更新を行います．\n",
        "\n",
        "---\n",
        "\n",
        "まずはじめに，ネットワークの定義に必要な関数を定義します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSSQUtlaYfK_"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7MMPyQ9YfLC"
      },
      "source": [
        "上で定義した関数を用いてミニバッチ学習に対応するネットワークモデルを作成します．\n",
        "ここでは，入力層，中間層，出力層から構成される多層パーセプトロンを定義します．\n",
        "\n",
        "入力層と中間層のユニット数は引数として与え，それぞれ`input_size`，`hidden_size`とします．\n",
        "出力層サイズについては，今回は2クラス分類問題を扱うため，`0~1`でどちらのクラスに属するかを表現するように，ユニット数は1に固定します．\n",
        "そして，`__init__`関数を用いて，ネットワークのパラメータを初期化します．\n",
        "`w1`および`w2`は各層の重みで，`b1`および`b2`はバイアスを表しています．\n",
        "重みは`randn`関数で，標準正規分布に従った乱数で生成した値を保有する配列を生成します．\n",
        "バイアスは`zeros`関数を用いて，要素が全て0の配列を生成します．\n",
        "\n",
        "そして，`forward`関数で，データを入力して結果を出力するための演算を定義します．\n",
        "ここでの演算は前回の演習プログラムと同様の処理を行います．\n",
        "\n",
        "次に，`backward`関数ではパラメータの更新量を計算します．\n",
        "まず，ネットワークの出力結果と教師ラベルから，誤差`dy`を算出します．\n",
        "その後，連鎖律に基づいて，出力層から順番に勾配を計算していきます．\n",
        "このとき，パラメータの更新量を`self.grads`へ保存しておきます．\n",
        "この時，ミニバッチ内の複数のサンプルの情報から勾配を決定するために，\n",
        "全結合層では`np.dot(self.h2.T, d_h3)`のように，行列を転置させて演算を行なっていることに注意していください．\n",
        "また，バイアスについては，各サンプルから得られた情報を`np.sum()`によって和を取っています．\n",
        "\n",
        "最後に`update_parameters`関数で，更新量をもとにパラメータの更新を行います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZAg5BxCYfLC"
      },
      "outputs": [],
      "source": [
        "class MLPBernoulli:\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, w_std=0.01):\n",
        "        self.w1 = w_std * np.random.randn(input_size, hidden_size)\n",
        "        self.b1 = np.zeros(hidden_size)\n",
        "        self.w2 = w_std * np.random.randn(hidden_size, 1)\n",
        "        self.b2 = np.zeros(1)        \n",
        "        self.grads = {}\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.h1 = np.dot(x, self.w1) + self.b1\n",
        "        self.h2 = sigmoid(self.h1)\n",
        "        self.h3 = np.dot(self.h2, self.w2) + self.b2\n",
        "        self.y = sigmoid(self.h3)\n",
        "        return self.y\n",
        "\n",
        "    def backward(self, x, t): \n",
        "        self.grads = {}\n",
        "\n",
        "        dy = -1 * (t - self.y)\n",
        "        d_h3 = sigmoid_grad(self.h3) * dy\n",
        "        self.grads['w2'] = np.dot(self.h2.T, d_h3)\n",
        "        self.grads['b2'] = np.sum(d_h3, axis=0)\n",
        "\n",
        "        d_h2 = np.dot(d_h3, self.w2.T)\n",
        "        d_h1 = sigmoid_grad(self.h1) * d_h2\n",
        "        self.grads['w1'] = np.dot(x.T, d_h1)\n",
        "        self.grads['b1'] = np.sum(d_h1, axis=0)\n",
        "\n",
        "    def update_parameters(self, lr=0.1):\n",
        "        self.w1 -= lr * self.grads['w1']\n",
        "        self.b1 -= lr * self.grads['b1']\n",
        "        self.w2 -= lr * self.grads['w2']\n",
        "        self.b2 -= lr * self.grads['b2']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFGtCPCLYfLE"
      },
      "source": [
        "## ネットワークの作成と学習の準備\n",
        "上のプログラムで定義したネットワークを作成します．\n",
        "\n",
        "\n",
        "まず，中間層と出力層のユニット数を定義します．\n",
        "ここでは，入力層のユニット数`input_size`を学習データの次元，中間層のユニット数`hidden_size`を128とします．\n",
        "\n",
        "各層のユニット数を`MLPBernoulli`クラスの引数として与え，ネットワークを作成します．\n",
        "\n",
        "### ミニバッチ学習で扱うデータ（配列）の形式\n",
        "作成したネットワークへの入力と出力，教師ラベルのデータ配列のサイズ・形を確認します．\n",
        "ここでは，上で用意したデータセット（`x_train`, `y_train`）のうち，先頭の10サンプルを`input`, `label`として保存します．\n",
        "その後，`input`データを`model.forward()`関数に入力し，分類結果を`y`として受け取ります．\n",
        "\n",
        "**※ ネットワークの学習はまだ行われていないため，分類結果は正しいものとは限らないことに注意してください．**\n",
        "\n",
        "ここで，入力，出力，教師ラベルの配列サイズを確認します．\n",
        "入力データは，`(10, 30)`となっており，1次元目がミニバッチサイズに対応しており，2次元目が各サンプルの特徴次元数となっています．\n",
        "また，出力データのサイズは`(10, 1)`となっており，入力と同様に1次元目がミニバッチサイズ，2次元目は各サンプルの出力次元数となっています．このネットワークでは分類結果を0~1のスコアで出力するため，次元数は1となっています．\n",
        "また，`y_train`から取り出した教師ラベルのサイズは`(10,)`となっており，1次元の配列（ベクトル）となっています．\n",
        "学習時の誤差計算に使用するためには，出力データと配列の形を合わせる必要があります．そのために，`reshape()`という関数を適用し，配列の形状を変更することで，対応します．\n",
        "`reshape(-1, 1)`では，2次元の配列に変更しており，2次元目のサイズを1，1次元目のサイズを-1とすることで任意のサイズに変更します．\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mt23sAwKYfLF"
      },
      "outputs": [],
      "source": [
        "input_size = x.shape[1]\n",
        "hidden_size = 64\n",
        "model = MLPBernoulli(input_size=input_size, hidden_size=hidden_size)\n",
        "\n",
        "# 確認用データの準備\n",
        "input = x_train[0:10]\n",
        "print(\"input data shape:\", input.shape)\n",
        "\n",
        "y = model.forward(input)\n",
        "print(\"output data shape:\", y.shape)\n",
        "\n",
        "label = y_train[0:10]\n",
        "print(\"label data shape:\", label.shape)\n",
        "print(\"label data shape (reshaped):\", label.reshape(-1, 1).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0Jfq04DYfLH"
      },
      "source": [
        "## 学習\n",
        "読み込んだbreast cancerデータセットと作成したネットワークを用いて，学習を行います．\n",
        "\n",
        "1回の誤差を算出するデータ数（ミニバッチサイズ）を10，学習エポック数を100とします．\n",
        "\n",
        "学習データは毎回ランダムに決定するため，numpyの`permutation`という関数を利用します．\n",
        "各更新において，学習用データと教師データをそれぞれ`x_batch`と`y_batch`とします．\n",
        "学習モデルに`x_batch`を与えて，各クラスに対する分類スコア`y_pred`を求めます．\n",
        "取得した`y_pred`は，学習の推移を確認するために，精度および誤差の算出に使用され，値を保存します．\n",
        "そして，誤差を`backward`関数で逆伝播します．この時，教師ラベル`y_batch`の配列サイズを`reshape()`を用いて，`y_pred`と同じ配列サイズに変更して入力します．\n",
        "そして，`update_parameters`でネットワークの更新を行います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDPfJ0RjYfLH"
      },
      "outputs": [],
      "source": [
        "# 学習途中の精度を確認するための関数\n",
        "def binary_classification_accuracy(pred, true):\n",
        "    pred = pred.flatten()\n",
        "    clf_res = np.zeros(pred.shape, dtype=np.int32)\n",
        "    clf_res[pred > 0.5] = 1\n",
        "    return np.sum(clf_res == true).astype(np.float32)\n",
        "\n",
        "num_train_data = x_train.shape[0]\n",
        "num_test_data = x_test.shape[0]\n",
        "epoch_num = 100\n",
        "batch_size= 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "epoch_list = []\n",
        "itr_list = []\n",
        "itr_loss_list = []\n",
        "train_loss_list = []\n",
        "train_accuracy_list = []\n",
        "test_accuracy_list = []\n",
        "\n",
        "iteration = 0\n",
        "for epoch in range(1, epoch_num + 1):\n",
        "    sum_accuracy = 0.0\n",
        "    sum_loss = 0.0\n",
        "\n",
        "    perm = np.random.permutation(num_train_data)\n",
        "    for i in range(0, num_train_data, batch_size):\n",
        "        x_batch = x_train[perm[i:i+batch_size]]\n",
        "        y_batch = y_train[perm[i:i+batch_size]]\n",
        "        \n",
        "        y_pred = model.forward(x_batch)\n",
        "\n",
        "        sum_accuracy += binary_classification_accuracy(y_pred, y_batch)\n",
        "        sum_loss += np.sum(np.power(y_pred - y_batch.reshape(-1, 1), 2))\n",
        "        itr_loss_list.append(np.sum(np.power(y_pred - y_batch.reshape(-1, 1), 2)))\n",
        "        itr_list.append(iteration)\n",
        "        \n",
        "        model.backward(x_batch, y_batch.reshape(-1, 1))\n",
        "        model.update_parameters(lr=learning_rate)\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "    if epoch % 10 == 0:       \n",
        "        print(\"epoch: {} (iteration: {}), mean loss: {}, mean accuracy: {}\".format(epoch, iteration,\n",
        "                                                                   sum_loss / num_train_data,\n",
        "                                                                   sum_accuracy / num_train_data))\n",
        "\n",
        "    test_correct_count = 0\n",
        "    for i in range(num_test_data):\n",
        "        input = x_test[i]\n",
        "        label = y_test[i]\n",
        "        y = model.forward(input)\n",
        "        \n",
        "        if y[0] > 0.5:\n",
        "            pred = 1\n",
        "        else:\n",
        "            pred = 0\n",
        "        if pred == label:\n",
        "            test_correct_count += 1\n",
        "\n",
        "    # 学習途中のlossと精度の保存\n",
        "    epoch_list.append(epoch)\n",
        "    train_loss_list.append(sum_loss / num_train_data)\n",
        "    train_accuracy_list.append(sum_accuracy / num_train_data)\n",
        "    test_accuracy_list.append(test_correct_count / num_test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4_SHgcEYfLJ"
      },
      "source": [
        "## テスト\n",
        "学習したネットワークを用いて，テストデータに対する認識率の確認を行います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HESOU9wYfLK"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "num_test_data = x_test.shape[0]\n",
        "\n",
        "for i in range(num_test_data):\n",
        "    x = np.array([x_test[i]], dtype=np.float32)\n",
        "    t = y_test[i]\n",
        "    y = model.forward(x)\n",
        "    \n",
        "    if y[0, 0] > 0.5:\n",
        "        pred = 1\n",
        "    else:\n",
        "        pred = 0\n",
        "    \n",
        "    if pred == t:\n",
        "        count += 1\n",
        "\n",
        "print(\"test accuracy: {}\".format(count / num_test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfLL2qzQh61D"
      },
      "source": [
        "## 学習推移のグラフ化\n",
        "\n",
        "上の学習プログラムで保存しておいた誤差および精度のデータをグラフ化します．\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0jcmRathLGi"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(epoch_list, train_loss_list, label='loss (train)')\n",
        "plt.xlabel(\"epoch\")     # x軸ラベル\n",
        "plt.ylabel(\"loss\")      # y軸ラベル\n",
        "plt.legend()            # 凡例\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(itr_list, itr_loss_list, label='loss (train)')\n",
        "plt.xlabel(\"iteration\")     # x軸ラベル\n",
        "plt.ylabel(\"loss\")      # y軸ラベル\n",
        "plt.legend()            # 凡例\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epoch_list, train_accuracy_list, label='accuracy (train)')\n",
        "plt.plot(epoch_list, test_accuracy_list, label='accuracy (test)')\n",
        "plt.xlabel(\"epoch\")     # x軸ラベル\n",
        "plt.ylabel(\"accuracy\")  # y軸ラベル\n",
        "plt.legend()            # 凡例\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVXdjR-9VPLH"
      },
      "source": [
        "## 課題\n",
        "1. ミニバッチのサイズを大きく変更し学習の推移を確認しよう．ミニバッチのサイズを変えた際のイタレーションとエポックの関係を理解しよう\n",
        "2. 02の結果を合わせて，オンライン学習，ミニバッチ学習，バッチ学習を比較し，それぞれの特性をまとめよう"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "03_mlp_bernoulli_minibatch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
