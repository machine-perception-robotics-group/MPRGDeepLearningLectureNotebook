{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2seq",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN8E7PngfgBCR2SiMZcBdQj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/13_rnn/04_Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2BhrFGqFx_M"
      },
      "source": [
        "#エンコーダ・デコーダ\n",
        "\n",
        "リカレントニューラルネットワークは，系列データ内の関連性を内部状態として保持することができます．\n",
        "この内部状態を利用して，新たな出力ができるようにした構造としてエンコーダ・デコーダがあります．\n",
        "エンコーダ側に系列データを入力して，中間層では系列データ内の関連性を内部状態を形成します．\n",
        "デコーダ側には内部状態を与えることで，内部状態を反映した何かしらの結果を出力します．\n",
        "この応用が，google 翻訳などの機械翻訳です．\n",
        "\n",
        "<img src=\"https://github.com/himidev/Lecture/blob/main/13_rnn/04_Seq2seq/Seq2Seq.png?raw=true\" width = 100%>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCmM6-6zF2hG"
      },
      "source": [
        "##計算機の実装\n",
        "ここでは，エンコーダ・デコーダ構造で計算機（足し算）を作ってみます．\n",
        "このエンコーダ・デコーダ構造のことをSeq2seqと呼びます．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvQCLiV4F1T6"
      },
      "source": [
        "###データローダの作成\n",
        "まず，データローダを用意します．データは0から9までの数字と加算記号，開始，終了のフラグです．また，３桁の数字の足し算を行うため，各桁の値を１つずつランダムに生成して連結しています．\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wUc1QN0Fygb"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "word2id = {str(i): i for i in range(10)}\n",
        "word2id.update({\"<pad>\": 10, \"+\": 11, \"<eos>\": 12})\n",
        "id2word = {v: k for k, v in word2id.items()}\n",
        "\n",
        "class CalcDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def transform(self, string, seq_len=7):\n",
        "        tmp = []\n",
        "        for i, c in enumerate(string):\n",
        "            try:\n",
        "                tmp.append(word2id[c])\n",
        "            except:\n",
        "                tmp += [word2id[\"<pad>\"]] * (seq_len - i)\n",
        "                break\n",
        "        return tmp\n",
        "\n",
        "    def __init__(self, data_num, train=True):\n",
        "        super().__init__()\n",
        "        self.data_num = data_num\n",
        "        self.train = train\n",
        "        self.data = []\n",
        "        self.label = []\n",
        "\n",
        "        for _ in range(data_num):\n",
        "            x = int(\"\".join([random.choice(list(\"0123456789\")) for _ in range(random.randint(1, 3))] ))\n",
        "            y = int(\"\".join([random.choice(list(\"0123456789\")) for _ in range(random.randint(1, 3))] ))\n",
        "            left = (\"{:*<7s}\".format(str(x) + \"+\" + str(y))).replace(\"*\", \"<pad>\")\n",
        "            self.data.append(self.transform(left))\n",
        "\n",
        "            z = x + y\n",
        "            right = (\"{:*<6s}\".format(str(z))).replace(\"*\", \"<pad>\")\n",
        "            right = self.transform(right, seq_len=5)\n",
        "            right = [12] + right\n",
        "            right[right.index(10)] = 12\n",
        "            self.label.append(right)\n",
        "        \n",
        "\n",
        "\n",
        "        self.data = np.asarray(self.data)\n",
        "        self.label = np.asarray(self.label)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        d = self.data[item]\n",
        "        l = self.label[item]\n",
        "        return d, l\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTHnQ96hF7cd"
      },
      "source": [
        "###エンコーダ・デコーダの作成\n",
        "エンコーダとデコーダを用意します．エンコーダは，ワードエンベディングという特徴表現に変換する層とGRU層から構成されています．デコーダも同様の構造です．エンコーダ側の中間層の値がstateとして出力され，デコーダ側の中間層に入力されます．\n",
        "エンコーダとデコーダは別々のネットワークとして用意し，それぞれの最適化にはAdamを利用します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0IE4XYHF690"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "embedding_dim = 16\n",
        "hidden_dim = 128\n",
        "vocab_size = len(word2id)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size=100):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=word2id[\"<pad>\"])\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, indices):\n",
        "        embedding = self.word_embeddings(indices)\n",
        "        if embedding.dim() == 2:\n",
        "            embedding = torch.unsqueeze(embedding, 1)\n",
        "        _, state = self.gru(embedding, torch.zeros(1, self.batch_size, self.hidden_dim, device=device))\n",
        "        \n",
        "        return state\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size=100):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=word2id[\"<pad>\"])\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, index, state):\n",
        "        embedding = self.word_embeddings(index)\n",
        "        if embedding.dim() == 2:\n",
        "            embedding = torch.unsqueeze(embedding, 1)\n",
        "        gruout, state = self.gru(embedding, state)\n",
        "        output = self.output(gruout)\n",
        "        return output, state\n",
        "\n",
        "\n",
        "encoder = Encoder(vocab_size, embedding_dim, hidden_dim, batch_size=100).to(device)\n",
        "decoder = Decoder(vocab_size, embedding_dim, hidden_dim, batch_size=100).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=word2id[\"<pad>\"])\n",
        "\n",
        "# Initialize opotimizers\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jPYDEx8F-d6"
      },
      "source": [
        "###学習\n",
        "学習を行います．学習データを2万サンプル生成して，データローダに与えます．\n",
        "学習は200エポック行います．エンコーダの入力は数字または開始・終了・加算記号です．\n",
        "デコーダの入力は計算結果です．\n",
        "具体的には，54+37 を行う時，\n",
        "エンコーダには，まず開始記号を最初に入力し，次に，5, 4, +, 3, 7 を入力します．そして，最後に終了記号を入力します．その時の中間層の情報をhidden_stateとしてエンコーダから受け取ります．\n",
        "デコーダは，開始記号と中間情報(hidden_state)を最初に入力します，そして，計算結果の9, 1 を入力し，最後に終了記号を入力します．\n",
        "この時，デコーダは各数字（または記号）の確率をdecoder_outputとして出力します．\n",
        "decoder_outputは，[バッチサイズ, 1, 各クラス確率]の３次元なので，squeezeによって，[バッチサイズ,  各クラス確率] に次元削減します．\n",
        "そして，クロスエントロピー誤差関数によって，ロスを求めます．\n",
        "これを正解の長さ(=5)分繰り返し行い，ロスを累積します．\n",
        "その後，誤差逆伝播，デコーダ，エンコーダの更新を行います．\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6oQRPeTF-Bj"
      },
      "source": [
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "# GPUの確認\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('Use CUDA:', use_cuda)\n",
        "\n",
        "batch_size=100\n",
        "epoch_num = 10\n",
        "\n",
        "train_data = CalcDataset(data_num = 20000)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "start = time()\n",
        "for epoch in range(1, epoch_num+1):\n",
        "    for data, label in train_loader:\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        if use_cuda:\n",
        "            data = data.cuda()\n",
        "            label = label.cuda()\n",
        "\n",
        "        encoder_hidden = encoder(data)\n",
        "        source = label[:, :-1]\n",
        "        target = label[:, 1:]\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        loss = 0\n",
        "        for i in range(source.size(1)):\n",
        "            decoder_output, decoder_hidden = decoder(source[:, i], decoder_hidden)\n",
        "            decoder_output = torch.squeeze(decoder_output)\n",
        "            loss += criterion(decoder_output, target[:, i])\n",
        "\n",
        "        # Perform backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # Adjust model weights\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "  \n",
        "    elapsed_time = time() - start\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"epoch: {}, mean loss: {}, elapsed_time: {}\".format(epoch, loss.item(), elapsed_time))\n",
        "        \n",
        "model_name = \"seq2seq_calculator_v{}.pt\".format(epoch)\n",
        "torch.save({\n",
        "    'encoder_model': encoder.state_dict(),\n",
        "    'decoder_model': decoder.state_dict(),\n",
        "}, model_name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVUbOtyfGLwA"
      },
      "source": [
        "###評価\n",
        "次に，学習したモデルを評価をします．テストデータを2000サンプル生成して，データローダに与えます．\n",
        "ここで，学習時はエンコーダとデコーダのバッチサイズを100としていました．\n",
        "テスト時は１つずつ行いたいので，エンコーダとデコーダを新たに生成し，学習したパラメータをロードします．\n",
        "エンコーダ側に計算したい数字（または記号）を入力して中間情報stateを得ます．\n",
        "デコーダ側に，中間情報stateと開始記号<eos>を入力します．\n",
        "デコーダ側の出力は数字または記号(token)と中間情報です．\n",
        "これらを繰り返しデコーダに入力します．<eos>が出力されたら繰り返しは終了です．\n",
        "出力されたtokenを追加したリストrightを計算結果とします．\n",
        "計算する式(left)を作成した後，evalでその計算結果が正しいかどうかを判定します．\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGdS8DWwGC_D"
      },
      "source": [
        "batch_size = 1\n",
        "test_data = CalcDataset(data_num = 2000)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "encoder = Encoder(vocab_size, embedding_dim, hidden_dim, batch_size=1).to(device)\n",
        "decoder = Decoder(vocab_size, embedding_dim, hidden_dim, batch_size=1).to(device)\n",
        "\n",
        "model_name = \"seq2seq_calculator_v{}.pt\".format(epoch)\n",
        "checkpoint = torch.load(model_name)\n",
        "encoder.load_state_dict(checkpoint[\"encoder_model\"])\n",
        "decoder.load_state_dict(checkpoint[\"decoder_model\"])\n",
        "\n",
        "accuracy = 0\n",
        "        \n",
        "# 評価の実行   \n",
        "with torch.no_grad():\n",
        "    for data, label in test_loader:\n",
        "        if use_cuda:\n",
        "            data = data.cuda()\n",
        "\n",
        "        state = encoder(data)\n",
        "\n",
        "        right = []\n",
        "        token = \"<eos>\"\n",
        "        for _ in range(7):\n",
        "            index = word2id[token]\n",
        "            input_tensor = torch.tensor([index], device=device)\n",
        "            output, state = decoder(input_tensor, state)\n",
        "            prob = F.softmax(torch.squeeze(output))\n",
        "            index = torch.argmax(prob.cpu().detach()).item()\n",
        "            token = id2word[index]\n",
        "            if token == \"<eos>\":\n",
        "                break\n",
        "            right.append(token)\n",
        "        right = \"\".join(right)\n",
        "\n",
        "        if \"+\" in right or \"<pad>\" in right:\n",
        "          accuracy += 0\n",
        "          continue\n",
        "        \n",
        "        x = list(data[0].to('cpu').detach().numpy() )\n",
        "        try:\n",
        "            padded_idx_x = x.index(word2id[\"<pad>\"])\n",
        "        except ValueError:\n",
        "            padded_idx_x = len(x)\n",
        "        left = \"\".join(map(lambda c: str(id2word[c]), x[:padded_idx_x]))\n",
        "\n",
        "\n",
        "\n",
        "        flag = [\"F\", \"T\"][eval(left) == int(right)]\n",
        "        print(\"{:>7s} = {:>4s} :{}\".format(left, right, flag))\n",
        "        if flag == \"T\":\n",
        "            accuracy += 1\n",
        "print(\"Accuracy: {:.2f}\".format(accuracy / len(test_loader)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVqKQdWxGOg5"
      },
      "source": [
        "#課題\n",
        "* 足し算だけでなく，色々な四則演算を試そう\n",
        "* 他のリカレントニューラルネットワークを使って精度比較をしてみよう"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMni_cMPGgJj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}