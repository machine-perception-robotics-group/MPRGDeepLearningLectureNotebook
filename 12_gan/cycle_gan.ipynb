{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "05_CycleGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/12_gan/cycle_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r9qjTJxb6st"
      },
      "source": [
        "# CycleGAN\n",
        "## 目的\n",
        "Cycle GANのネットワークを構築して動作を理解する．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPGo3zbkSCyN"
      },
      "source": [
        "# データセット\n",
        "りんごとオレンジが含まれるデータセットのzipファイルをダウンロードして，解凍します．\n",
        "もし違うデータを使いたい場合は，以下のURLから選択してURL先のデータをwgetで取得してください．<br>\n",
        "https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MliglJq9Qg4k"
      },
      "source": [
        "!wget -q https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/apple2orange.zip -O apple2orange.zip\n",
        "!unzip -q -o apple2orange.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGjewjTHTbch"
      },
      "source": [
        "# 各種モジュールのインポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht4YxManb6sx"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import itertools\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzVLYWOqTfki"
      },
      "source": [
        "# ネットワークの構築\n",
        "CycleGANは，「画像を入力→異なるスタイルへ変換→変換したものを元のスタイルへ戻す」という処理によって，対になる画像がない場合であってもスタイルを変換できる代物です．<br>\n",
        "以下に示す図は，CycleGANによる変換及び再構成を表しています．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGtp4p_FRVKL"
      },
      "source": [
        "<img src=\"https://dl.dropboxusercontent.com/s/7339ohcadohs6o7/Cyclegan.png\" width=40%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDHDDjFLRY_g"
      },
      "source": [
        "\n",
        "\n",
        "CycleGANが提案される前は，pix2pixと呼ばれるペア画像を必要とするスタイル変換が主流でした．\n",
        "ペア画像とは，例えばRGB画像とその画像に対するセグメンテーション画像のことを指します．<br>\n",
        "\n",
        "CycleGANのGeneratorはResidual networkをベースに設計します．\n",
        "Residual network (ResNet)は，残差機構を用いることで多層になった場合でも特徴を残すことが可能なネットワークです．\n",
        "ResBlockは，カーネルサイズ3×3，ストライド1，パディング1の2層の畳み込み層で構築します．\n",
        "以下にResBlockの図を示します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1M94GqluOIP"
      },
      "source": [
        "<img src=\"https://dl.dropboxusercontent.com/s/pow1wsxhc37gmts/ResBlock.png\" width=50%>\n",
        "\n",
        "ここで$f(x)$は，前層が出力した特徴マップを表しています．\n",
        "また$g(x)$は，残差を加算した特徴マップです．\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_q0pI8tzb6sz"
      },
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, features):\n",
        "        super(ResBlock, self).__init__()\n",
        "        block_list = []\n",
        "        self.block = self.make_block(block_list, features)\n",
        "        \n",
        "    def make_block(self, modules_list, features):\n",
        "        modules_list.append(nn.ReflectionPad2d(1))\n",
        "        modules_list.append(nn.Conv2d(features, features, kernel_size=3, stride=1, bias=True))\n",
        "        modules_list.append(self.select_normalization(norm='instance', features=features))\n",
        "        modules_list.append(nn.ReLU(inplace=True))\n",
        "        modules_list.append(nn.ReflectionPad2d(1))\n",
        "        modules_list.append(nn.Conv2d(features, features, kernel_size=3, stride=1, bias=True))\n",
        "        modules_list.append(self.select_normalization(norm='instance', features=features))\n",
        "        modules = nn.Sequential(*modules_list)\n",
        "        return modules\n",
        "        \n",
        "    def select_normalization(self, norm, features):\n",
        "        if norm == 'batch':\n",
        "            return nn.BatchNorm2d(features)\n",
        "        elif norm == 'instance':\n",
        "            return nn.InstanceNorm2d(features)\n",
        "        else:\n",
        "            assert 0, '%s is not supported.' % norm\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.block(x)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwdCasq1N7cb"
      },
      "source": [
        "CycleGANのGeneratorは，入力に潜在変数ではなく変換元となる画像を入力して，画像を出力します．そのため，GeneratorはEncoder-Decoder構造をしています．Encoderは，入力画像を表現するために有益な情報を残した特徴抽出をします．一方で，DecoderはEncoderがエンコードした特徴を用いて画像の変換をします．このときにEncoderとDecoderの間には，任意の数積み上げたResBlockを使用することで，さらに特徴を凝縮します．\n",
        "ここで，ResBlockは，残差を用いてネットワークを構築するResidual Networksで使用されているものです．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ9N0ENevqmS"
      },
      "source": [
        "<img src=\"https://dl.dropboxusercontent.com/s/yha9im0bbb6p0t1/CycleGAN_G.png\" width=50%>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myUSGWPzb6s1"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, n_down, n_up, n_res, in_features):\n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        out_features = 64\n",
        "        first_conv = [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(in_features, out_features, kernel_size=7, stride=1, padding=0, bias=True),\n",
        "            self.select_normalization(norm='instance', features=out_features),\n",
        "            nn.ReLU(inplace=True)]\n",
        "        \n",
        "        down_block = []\n",
        "        for _ in range(n_down):\n",
        "            in_features = out_features\n",
        "            out_features = in_features * 2\n",
        "            down_block += [\n",
        "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1, bias=True),\n",
        "                self.select_normalization(norm='instance', features=out_features),\n",
        "                nn.ReLU(inplace=True)]\n",
        "            \n",
        "        res_block = []\n",
        "        res_features = out_features\n",
        "        for _ in range(n_res):\n",
        "            res_block.append(ResBlock(res_features))\n",
        "            \n",
        "        up_block = []\n",
        "        in_features = res_features\n",
        "        out_features = in_features // 2\n",
        "        for _ in range(n_up):\n",
        "            up_block += [\n",
        "                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1, bias=True),\n",
        "                self.select_normalization(norm='instance', features=out_features),\n",
        "                nn.ReLU(inplace=True)]\n",
        "            in_features = out_features\n",
        "            out_features = in_features // 2\n",
        "        \n",
        "        last_conv = [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(in_features, 3, kernel_size=7, stride=1, padding=0, bias=True),\n",
        "            nn.Tanh()]\n",
        "        \n",
        "        self.first_conv = nn.Sequential(*first_conv)\n",
        "        self.down_block = nn.Sequential(*down_block)\n",
        "        self.res_block = nn.Sequential(*res_block)\n",
        "        self.up_block = nn.Sequential(*up_block)\n",
        "        self.last_conv = nn.Sequential(*last_conv)\n",
        "        self.init_weights(self.first_conv)\n",
        "        self.init_weights(self.down_block)\n",
        "        self.init_weights(self.res_block)\n",
        "        self.init_weights(self.up_block)\n",
        "        self.init_weights(self.last_conv)\n",
        "\n",
        "    def init_weights(self, net):\n",
        "        classname = net.__class__.__name__\n",
        "        if classname.find('Conv') != -1:\n",
        "            torch.nn.init.normal_(net.weight.data, 0.0, 0.02)\n",
        "            if hasattr(net, 'bias') and net.bias is not None:\n",
        "                torch.nn.init.constant_(net.bias.data, 0.0)\n",
        "    \n",
        "    def select_normalization(self, norm, features):\n",
        "        if norm == 'batch':\n",
        "            return nn.BatchNorm2d(features)\n",
        "        elif norm == 'instance':\n",
        "            return nn.InstanceNorm2d(features)\n",
        "        else:\n",
        "            assert 0, '%s is not supported.' % norm\n",
        "            \n",
        "    def forward(self, x):\n",
        "        h = self.first_conv(x)\n",
        "        h = self.down_block(h)\n",
        "        h = self.res_block(h)\n",
        "        h = self.up_block(h)\n",
        "        out = self.last_conv(h)   \n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qta1yAAWCRoc"
      },
      "source": [
        "Discriminatorは，実画像であるかGeneratorによって変換された画像であるかの分類をするため，通常のGANと同じ振る舞いです．\n",
        "DC-GANは，画像1枚を入力して実画像であるか生成画像であるかを判定しています．\n",
        "つまり，Dicriminatorは画像全体を見て判断していると言っても良いでしょう．<br>\n",
        "ところが，CycleGANはPatchGANをベースにDiscriminatorを構築しているので，少し変わった方法で実画像か生成画像かの判定をしています．\n",
        "PatchGANは，画像全体ではなく画像をいくつかのパッチに区切って入力することで，局所領域ごとに実画像なのか生成画像なのかを判定します．\n",
        "これによって，画像全体で判定するよりも性能がよくなることが知られています．\n",
        "以下に通常のDiscriminatorとCycleGANのDsicriminatorを示します．\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQE5nsqA13fI"
      },
      "source": [
        "<img src=\"https://dl.dropboxusercontent.com/s/yrud8444g26g0yz/CycleGAN_D.png\" width=50%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgE-BTCT2CnG"
      },
      "source": [
        "図にも示したように，厳密には任意の数のPatchに分割することは手間なので，Discriminator内部の畳み込み処理のカーネルサイズによって受容野の広さを制御して，出力値をスカラー値ではなく特徴マップとすることによってPatchに分割したときと同様の処理を実現しています．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M0h0cZQb6s3"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, n_layers=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "        out_features = 64\n",
        "        modules = [nn.Conv2d(3, out_features, kernel_size=4, stride=2, padding=1, bias=True),\n",
        "                   nn.LeakyReLU(negative_slope=0.2, inplace=True)]\n",
        "\n",
        "        for i in range(n_layers):\n",
        "            in_features = out_features\n",
        "            out_features = in_features * 2\n",
        "            if i == n_layers - 1:    stride=1\n",
        "            else:    stride=2\n",
        "            modules += [nn.Conv2d(in_features, out_features, kernel_size=4, stride=stride, padding=1, bias=True),\n",
        "                        self.select_normalization(norm='instance', features=out_features),\n",
        "                        nn.LeakyReLU(negative_slope=0.2, inplace=True)]\n",
        "        \n",
        "        modules += [nn.Conv2d(out_features, 1, kernel_size=4, stride=1, padding=1, bias=True)]\n",
        "        self.layers = nn.Sequential(*modules)\n",
        "        self.init_weights(self.layers)\n",
        "\n",
        "    def init_weights(self, net):\n",
        "        classname = net.__class__.__name__\n",
        "        if classname.find('Conv') != -1:\n",
        "            torch.nn.init.normal_(net.weight.data, 0.0, 0.02)\n",
        "            if hasattr(net, 'bias') and net.bias is not None:\n",
        "                torch.nn.init.constant_(net.bias.data, 0.0)\n",
        "    \n",
        "    def select_normalization(self, norm, features):\n",
        "        if norm == 'batch':\n",
        "            return nn.BatchNorm2d(features)\n",
        "        elif norm == 'instance':\n",
        "            return nn.InstanceNorm2d(features)\n",
        "        else:\n",
        "            assert 0, '%s is not supported.' % norm\n",
        "            \n",
        "    def forward(self, x):\n",
        "        out = self.layers(x)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EZI5T5j-9Mn"
      },
      "source": [
        "# DataLoaderの定義\n",
        "pytorchに含まれているデータ以外のデータセットを用いて学習するときや特殊なデータ構造を利用して学習をする時は，DataLoaderを自分で定義する必要があります．今回使用するデータセットは，zipファイルを解凍すると，内部にtrainA，trainB，testA，testBという名前のディレクトリが含まれています．\n",
        "また，それぞれのディレクトリの中に画像が入っています．\n",
        "\n",
        "これらを考慮して以下に示すようにDataLoaderを定義します．\n",
        "\n",
        "datapath：zipファイルを解凍したときにできるディレクトリのパス\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42LbNtoTb6s5"
      },
      "source": [
        "class CycleGAN_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, datapath, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.A_path = os.path.join(datapath, 'trainA')\n",
        "        self.B_path = os.path.join(datapath, 'trainB')\n",
        "        dataA_list = os.listdir(self.A_path)\n",
        "        dataB_list = os.listdir(self.B_path)\n",
        "        random.shuffle(dataA_list)\n",
        "        random.shuffle(dataB_list)\n",
        "        self.datalength = min(len(dataA_list), len(dataB_list))\n",
        "        self.dataA = dataA_list[:self.datalength]\n",
        "        self.dataB = dataB_list[:self.datalength]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.datalength\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        imgA = Image.open(os.path.join(self.A_path, self.dataA[i]))\n",
        "        imgB = Image.open(os.path.join(self.B_path, self.dataB[i]))\n",
        "        \n",
        "        if self.transforms:\n",
        "            imgA = self.transforms(imgA)\n",
        "            imgB = self.transforms(imgB)\n",
        "        \n",
        "        return imgA, imgB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqq6NIWqBHsX"
      },
      "source": [
        "# Image History Buffer\n",
        "CycleGANでは，任意のサイズのBufferを定義します．\n",
        "これにより，過去にDiscriminatorをうまく騙せた画像に依存して変換のバリエーションが少なくなる問題を回避することができます．\n",
        "\n",
        "Image Bufferは，Generatorが変換した画像を溜め込みますが，任意のiteration数で中身の画像を最新の画像にアップデートします．このときに，全ての画像を破棄して新しいものへアップデートするのではなく，一部のみをアップデートします．つまり，異なるiteration数で変換した画像が混在したBufferとなっています．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPOcEIJLb6s7"
      },
      "source": [
        "class Image_History_Buffer:\n",
        "    def __init__(self, pool_size=50):\n",
        "        self.pool_size = pool_size\n",
        "        self.buffer = []\n",
        "    \n",
        "    def get_images(self,pre_images):\n",
        "        return_imgs = []\n",
        "        for img in pre_images:\n",
        "            img = torch.unsqueeze(img,0)\n",
        "            if len(self.buffer) < self.pool_size:\n",
        "                self.buffer.append(img)\n",
        "                return_imgs.append(img)\n",
        "            else:\n",
        "                if random.randint(0,1)>0.5:\n",
        "                    i = random.randint(0,self.pool_size-1)\n",
        "                    tmp = self.buffer[i].clone()\n",
        "                    self.buffer[i]=img\n",
        "                    return_imgs.append(tmp)\n",
        "                else:\n",
        "                    return_imgs.append(img)\n",
        "        return torch.cat(return_imgs,dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laJRoFEWb6s9"
      },
      "source": [
        "class loss_scheduler():\n",
        "    def __init__(self, epoch_decay):\n",
        "        self.epoch_decay = epoch_decay\n",
        "\n",
        "    def f(self, epoch):\n",
        "        if epoch<=self.epoch_decay:\n",
        "            return 1\n",
        "        else:\n",
        "            scaling = 1 - (epoch-self.epoch_decay)/float(self.epoch_decay)\n",
        "            return scaling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Div2ag6vPtnr"
      },
      "source": [
        "# ネットワークの作成，学習に必要なパラメータの定義\n",
        "CycleGANは，ドメインAからBに変換するGeneratorとその逆の処理をするGeneratorの2つ作成する必要があります．また，Discriminatorも同様で，ドメインAのためのDiscriminator，ドメインBのためのDicriminatorも作成します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mpFvV3_b6s_"
      },
      "source": [
        "lr = 0.0002\n",
        "img_size = 256\n",
        "betas = (0.5, 0.999)\n",
        "batchsize = 1\n",
        "imgsize = 256\n",
        "n_epochs = 200\n",
        "decay_epoch = 100\n",
        "lambda_val = 10\n",
        "lambda_id_val = 0\n",
        "datapath = 'apple2orange'\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Make training dataset\n",
        "mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
        "transform = transforms.Compose([transforms.Resize(img_size, Image.BICUBIC),\n",
        "                                   transforms.RandomCrop(imgsize),\n",
        "                                   transforms.RandomHorizontalFlip(),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize(mean, std)])\n",
        "train_data = CycleGAN_Dataset(datapath=datapath, transforms=transform)\n",
        "training_dataset = DataLoader(train_data, batch_size=batchsize, shuffle=True)\n",
        "\n",
        "# Define networks\n",
        "G_A2B = Generator(n_down=2, n_up=2, n_res=9, in_features=3).to(device)\n",
        "G_B2A = Generator(n_down=2, n_up=2, n_res=9, in_features=3).to(device)\n",
        "D_A = Discriminator(n_layers=3).to(device)\n",
        "D_B = Discriminator(n_layers=3).to(device)\n",
        "\n",
        "g_opt = optim.Adam(itertools.chain(G_A2B.parameters(), G_B2A.parameters()), lr=lr, betas=betas)\n",
        "d_A_opt = optim.Adam(D_A.parameters(), lr=lr, betas=betas)\n",
        "d_B_opt = optim.Adam(D_B.parameters(), lr=lr, betas=betas)\n",
        "\n",
        "g_lr_scheduler = torch.optim.lr_scheduler.LambdaLR(g_opt, lr_lambda=loss_scheduler(decay_epoch).f)\n",
        "d_a_lr_scheduler = torch.optim.lr_scheduler.LambdaLR(d_A_opt, lr_lambda=loss_scheduler(decay_epoch).f)\n",
        "d_b_lr_scheduler = torch.optim.lr_scheduler.LambdaLR(d_B_opt, lr_lambda=loss_scheduler(decay_epoch).f)\n",
        "\n",
        "adv_loss = nn.MSELoss()\n",
        "l1_norm = nn.L1Loss()\n",
        "criterion_idn = nn.L1Loss()\n",
        "\n",
        "buffer_for_fakeA = Image_History_Buffer()\n",
        "buffer_for_fakeB = Image_History_Buffer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5fMDxHRfY_R"
      },
      "source": [
        "# CycleGANの学習\n",
        "これまでに定義したネットワークを用いて学習します．\n",
        "\n",
        "CycleGANの誤差関数は，オリジナルのGANとは異なるものが使用されています．\n",
        "* Real/Fakeの誤差関数：GANを学習するために必要な誤差関数です．LS-GANで提案されているカイ2乗誤差を誤差関数とする方法を利用しています．\n",
        "$$\\mathcal{L}_{adv}^{A} = \\mathbb{E}\\left[\\left(D_{A}(x_{A}) - 1\\right)^{2}\\right] + \\mathbb{E}\\left[D_{A}\\left(G_{B\\rightarrow A}(x_{B})\\right)^{2}\\right]$$\n",
        "$$\\mathcal{L}_{adv}^{B} = \\mathbb{E}\\left[\\left(D_{B}(x_{B}) - 1\\right)^{2}\\right] + \\mathbb{E}\\left[D_{B}\\left(G_{A\\rightarrow B}(x_{A})\\right)^{2}\\right]$$\n",
        "* Cycle consistency loss：オリジナルの画像と再構成した画像の一貫性を保つために必要な誤差関数です．\n",
        "$$\n",
        "\\mathcal{L}_{cycle}^{A} = \\|x_{A} - G_{B\\rightarrow A}\\left(G_{A\\rightarrow B}\\left(x_{A}\\right)\\right)\\|_{1}\n",
        "$$\n",
        "$$\n",
        "\\mathcal{L}_{cycle}^{B} = \\|x_{B} - G_{A\\rightarrow B}\\left(G_{B\\rightarrow A}\\left(x_{B}\\right)\\right)\\|_{1}\n",
        "$$\n",
        "* Identity loss：別のスタイルへ変換したときにオリジナル画像を大幅に変更してしまうことを抑制するために必要な誤差です．実際，この誤差を抜いても学習は可能で，スタイルの変換もすることはできます．この誤差関数が最大の効果を発揮する場面は，ある風景画を絵画風に変換するときです．\n",
        "$$\n",
        "\\mathcal{L}_{identity}^{A} = \\|x_{A} - G_{A\\rightarrow B}\\left(x_{A}\\right)\\|_{1}\n",
        "$$\n",
        "$$\n",
        "\\mathcal{L}_{identity}^{B} = \\|x_{B} - G_{B\\rightarrow A}\\left(x_{B}\\right)\\|_{1}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t2r4i8xb6tE"
      },
      "source": [
        "for epoch in range(1, n_epochs+1):\n",
        "    G_B2A.train()\n",
        "    G_A2B.train()\n",
        "    D_A.train()\n",
        "    D_B.train()\n",
        "    for idx, (imgA, imgB) in enumerate(training_dataset):\n",
        "        imgA = Variable(imgA.to(device), requires_grad=True)\n",
        "        imgB = Variable(imgB.to(device), requires_grad=True)\n",
        "        imgA_fake, imgB_fake = G_B2A(imgB), G_A2B(imgA)\n",
        "        imgA_rec, imgB_rec = G_B2A(imgB_fake), G_A2B(imgA_fake)\n",
        "        if lambda_id_val > 0:\n",
        "            iden_imgA, iden_imgB = G_B2A(imgA), G_A2B(imgB)\n",
        "        \n",
        "        # Update the discriminator (D_A, D_B)\n",
        "        d_A_opt.zero_grad()\n",
        "        disA_out_real = D_A(imgA)\n",
        "        imgA_fake_ = buffer_for_fakeA.get_images(imgA_fake)\n",
        "        disA_out_fake = D_A(imgA_fake_.detach())\n",
        "        d_lossA_real = adv_loss(disA_out_real, torch.tensor(1.0).expand_as(disA_out_real).to(device))\n",
        "        d_lossA_fake = adv_loss(disA_out_fake, torch.tensor(0.0).expand_as(disA_out_fake).to(device))\n",
        "        disA_loss = (d_lossA_real + d_lossA_fake) * 0.5\n",
        "        disA_loss.backward()\n",
        "        d_A_opt.step()\n",
        "        \n",
        "        d_B_opt.zero_grad()\n",
        "        disB_out_real = D_B(imgB)\n",
        "        imgB_fake_ = buffer_for_fakeB.get_images(imgB_fake)\n",
        "        disB_out_fake = D_B(imgB_fake_.detach())\n",
        "        d_lossB_real = adv_loss(disB_out_real, torch.tensor(1.0).expand_as(disB_out_real).to(device))\n",
        "        d_lossB_fake = adv_loss(disB_out_fake, torch.tensor(0.0).expand_as(disA_out_fake).to(device))\n",
        "        disB_loss = (d_lossB_real + d_lossB_fake) * 0.5\n",
        "        disB_loss.backward()\n",
        "        d_B_opt.step()\n",
        "        \n",
        "        # Update the generator (G)\n",
        "        g_opt.zero_grad()\n",
        "        disB_out_fake = D_B(imgB_fake)\n",
        "        disA_out_fake = D_A(imgA_fake)\n",
        "        g_lossA = adv_loss(disA_out_fake, torch.tensor(1.0).expand_as(disA_out_fake).to(device))\n",
        "        g_lossB = adv_loss(disB_out_fake, torch.tensor(1.0).expand_as(disB_out_fake).to(device))\n",
        "        gen_adv_loss = g_lossA + g_lossB\n",
        "        \n",
        "        cycle_consistency_loss = l1_norm(imgA_rec, imgA) + l1_norm(imgB_rec, imgB)\n",
        "        if lambda_id_val > 0:\n",
        "            identity_loss = criterion_idn(iden_imgA, imgA) + criterion_idn(iden_imgB, imgB)\n",
        "            gen_loss = gen_adv_loss + lambda_val * cycle_consistency_loss + lambda_id_val * identity_loss\n",
        "        else:\n",
        "            gen_loss = gen_adv_loss + lambda_val * cycle_consistency_loss\n",
        "        gen_loss.backward()\n",
        "        g_opt.step()\n",
        "        \n",
        "        if idx % 100 == 0:\n",
        "            print('Training epoch: {} [{}/{} ({:.0f}%)] | D loss (A): {:.6f} | D loss (B): {:.6f} | G loss: {:.6f} | Consistency: {:.6f} | Identity: {:.6f} |'\\\n",
        "                  .format(epoch, idx * len(imgA), len(training_dataset.dataset),\n",
        "                  100. * idx / len(training_dataset), disA_loss.item(), disB_loss.item(), gen_loss.item(), cycle_consistency_loss.item(), identity_loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUycupIgLODf"
      },
      "source": [
        "# 学習済みモデルのロード\n",
        "CycleGANはネットワークが深いため，綺麗な変換をするためには長時間の学習を必要とします．\n",
        "Colabratory上で学習できなくはないですが，手早くCycleGANが出力する結果を見たい方は，以下を動作させて学習済みモデルをダウンロードしてください．\n",
        "\n",
        "ディレクトリ内には，appleとorangeの相互変換をするモデルとhorseとzebraの相互変換するモデルが含まれています．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzAKjd3rLWdl"
      },
      "source": [
        "!wget -q https://www.mprg.cs.chubu.ac.jp/~hirakawa/share/tutorial_data/CycleGAN_pretrained_model.zip --no-check-certificate\n",
        "!unzip -q -o CycleGAN_pretrained_model.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7ik4gPALZpQ"
      },
      "source": [
        "data_name = 'apple2orange'\n",
        "G_A2B_path = os.path.join('./CycleGAN_pretrained_model', data_name, 'G_A2B')\n",
        "G_B2A_path = os.path.join('./CycleGAN_pretrained_model', data_name, 'G_B2A')\n",
        "D_A_path = os.path.join('./CycleGAN_pretrained_model', data_name, 'D_A')\n",
        "D_B_path = os.path.join('./CycleGAN_pretrained_model', data_name, 'D_B')\n",
        "G_A2B.load_state_dict(torch.load(G_A2B_path))\n",
        "G_B2A.load_state_dict(torch.load(G_B2A_path))\n",
        "D_A.load_state_dict(torch.load(D_A_path))\n",
        "D_B.load_state_dict(torch.load(D_B_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3i4F1urb6tG"
      },
      "source": [
        "mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
        "normalize = transforms.Normalize(mean=mean, std=std)\n",
        "to_tensor = transforms.ToTensor()\n",
        "transform = transforms.Compose([to_tensor, normalize])\n",
        "\n",
        "domainA_path = os.path.join(data_name, 'testA')\n",
        "domainB_path = os.path.join(data_name, 'testB')\n",
        "imgA_list, imgB_list = os.listdir(domainA_path), os.listdir(domainB_path)\n",
        "indexA, indexB = np.random.randint(len(imgA_list)), np.random.randint(len(imgB_list))\n",
        "imgA = Image.open(os.path.join(domainA_path, imgA_list[indexA]))\n",
        "imgB = Image.open(os.path.join(domainB_path, imgB_list[indexB]))\n",
        "imgA_tensor = transform(imgA).to(device)[None,:,:,:]\n",
        "imgB_tensor = transform(imgB).to(device)[None,:,:,:]\n",
        "G_A2B.eval()\n",
        "G_B2A.eval()\n",
        "with torch.no_grad():\n",
        "    fake_B = G_A2B(imgA_tensor)\n",
        "    fake_A = G_B2A(imgB_tensor)\n",
        "    rec_B = G_A2B(fake_A)\n",
        "    rec_A = G_B2A(fake_B)\n",
        "    mean = torch.tensor(mean, dtype=torch.float32)[None,:,None,None].to(device)\n",
        "    std = torch.tensor(std, dtype=torch.float32)[None,:,None,None].to(device)\n",
        "    fake_B = (fake_B * std) + mean\n",
        "    fake_A = (fake_A * std) + mean\n",
        "\n",
        "fake_imgA = Image.fromarray((fake_A * 256.).clamp(min=0, max=255).data.cpu().squeeze().permute(1,2,0).numpy().astype(np.uint8))\n",
        "fake_imgB = Image.fromarray((fake_B * 256.).clamp(min=0, max=255).data.cpu().squeeze().permute(1,2,0).numpy().astype(np.uint8))\n",
        "plt_items = [imgA, fake_imgB, imgB, fake_imgA]\n",
        "title_list = ['Real_A', 'Fake_B', 'Real_B', 'Fake_A']\n",
        "rows = 2\n",
        "cols = 2\n",
        "axes=[]\n",
        "fig=plt.figure(figsize=(16, 9))\n",
        "\n",
        "for i in range(rows*cols):\n",
        "    item = plt_items[i]\n",
        "    axes.append( fig.add_subplot(rows, cols, i+1) )\n",
        "    axes[-1].set_title(title_list[i])\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.imshow(item)\n",
        "fig.tight_layout()    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws3szHBNoR0V"
      },
      "source": [
        "rec_A = (rec_A * std) + mean\n",
        "Image.fromarray((rec_A * 256.).clamp(min=0, max=255).data.cpu().squeeze().permute(1,2,0).numpy().astype(np.uint8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtI7H0yR0ZOO"
      },
      "source": [
        "# 課題\n",
        "\n",
        "\n",
        "*   データセットを変更して学習してみましょう．\n",
        "*   変換するデータと学習済みモデルが一致していない場合の画像変換をしてみましょう\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvWxlMQ2qwzW"
      },
      "source": [
        "# 参考文献\n",
        "[1] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou and Alexei A. Efros, Image-to-Image Translation with Conditional Adversarial Nets, CVPR, 2017.\\\n",
        "[2] Jun-Yan Zhu, Taesung Park, Phillip Isola and Alexei A. Efros, Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, ICCV, 2017."
      ]
    }
  ]
}