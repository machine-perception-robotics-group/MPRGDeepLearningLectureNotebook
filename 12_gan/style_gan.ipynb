{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "06_StyleGAN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/12_gan/06_StyleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TEDRHqjBMP8"
      },
      "source": [
        "# StyleGAN\n",
        "GANで高解像な画像生成をするためには，人の目で見ても区別が付かない程，鮮明な画像を生成する必要があります．\n",
        "つまり，鮮明でない画像は，Discriminatorで容易に本物か偽物かを区別できてしまうため，モード崩壊が起こります．<br>\n",
        "この問題を回避する方法として，徐々に層を追加するプログレッシブな学習プロセスを使用しています．プログレッシブな学習は，Progressive Growing GAN (PG-GAN)で提案された方法です．\n",
        "\n",
        "PG-GANと異なる点は，潜在変数を直接画像生成に使用するのではなく，Adaptive Instance Normalization (AdaIN)のパラメータに利用しています．\n",
        "ここで，AdaINは，しばしばスタイル変換に利用されるNormalization方法です．\n",
        "潜在変数をAdaINのパラメータに利用することでネットワークの浅い層と深い層で変化するスタイルに違いがあることを発見しました．\n",
        "\n",
        "Generatorの入力層へ与える変数は，ランダムサンプリングしたベクトルではなく，学習初期に決定した任意のサイズの定数（4x4のmap）を入力として学習をしています．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIM8-nUWBMP9"
      },
      "source": [
        "## 必要モジュールのインポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deM07eaPBMP9"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from math import sqrt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQjhl95oBMQA"
      },
      "source": [
        "## ネットワーク構築のために必要なレイヤの定義\n",
        "### Equalized Convolution，Equalized Linear\n",
        "一般的にDeep Learningで識別，認識，分類をするときにネットワークを初期化する方法として，Heの初期化やXavierなど正規分布に基づく初期化が多用されています．\n",
        "しかしながら，これらの初期化方法とAdamやRMSPropなどの最適化関数を組み合わせて学習をすることは，画像生成には適さないことがPG-GANの論文中で示されました．\n",
        "StyleGANでも同様のレイヤが用いられており，以下のように動的に初期値を決定しています．\n",
        "$$\n",
        "w^{\\prime}\\leftarrow w\\sqrt{\\frac{2}{Ch * K}}\n",
        "$$\n",
        "ここで，$w$は正規分布からサンプリングした重みフィルタ，$Ch$はチャネル数，$K$はカーネルサイズ（全結合の場合は1）です．<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W6Vc6mDBMQA"
      },
      "source": [
        "class Equalized_LR:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        \n",
        "    def compute_weight(self, module):\n",
        "        weight = getattr(module, self.name+'_orig')\n",
        "        fan_in = weight.data.size(1) * weight.data[0][0].numel()        \n",
        "        return weight * sqrt(2 / fan_in)\n",
        "    \n",
        "    @staticmethod\n",
        "    def apply(module, name):\n",
        "        fn = Equalized_LR(name)\n",
        "\n",
        "        weight = getattr(module, name)\n",
        "        del module._parameters[name]\n",
        "        module.register_parameter(name + '_orig', nn.Parameter(weight.data))\n",
        "        module.register_forward_pre_hook(fn)\n",
        "\n",
        "        return fn\n",
        "    \n",
        "    def __call__(self, module, input):\n",
        "        weight = self.compute_weight(module)\n",
        "        setattr(module, self.name, weight)\n",
        "\n",
        "def equal_lr(module, name='weight'):\n",
        "    Equalized_LR.apply(module, name)\n",
        "    return module\n",
        "\n",
        "class EqualConv2d(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        conv = nn.Conv2d(*args, **kwargs)\n",
        "        conv.weight.data.normal_()\n",
        "        conv.bias.data.zero_()\n",
        "        self.conv = equal_lr(conv)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.conv(input)\n",
        "\n",
        "class EqualLinear(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, mapping_linear=False, \n",
        "                 gain=None, lrmul=None, bias=True, use_wscale=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mapping_linear = mapping_linear\n",
        "        linear = nn.Linear(in_dim, out_dim)\n",
        "        linear.weight.data.normal_()\n",
        "        linear.bias.data.zero_()\n",
        "\n",
        "        if mapping_linear:\n",
        "            he_std = gain * in_dim ** (-0.5)\n",
        "            if use_wscale:\n",
        "                init_std = 1.0 / lrmul\n",
        "                self.w_mul = he_std * lrmul\n",
        "            else:\n",
        "                init_std = he_std / lrmul\n",
        "                self.w_mul = lrmul\n",
        "            self.weight = torch.nn.Parameter(torch.randn(out_dim, in_dim) * init_std)\n",
        "            if bias:\n",
        "                self.bias = torch.nn.Parameter(torch.zeros(out_dim))\n",
        "                self.b_mul = lrmul\n",
        "            else:\n",
        "                self.bias = None\n",
        "        else:\n",
        "            self.linear = equal_lr(linear)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.mapping_linear:\n",
        "            bias = self.bias\n",
        "            if bias is not None:\n",
        "                bias = bias * self.b_mul\n",
        "            return F.linear(input, self.weight * self.w_mul, bias)\n",
        "        else:\n",
        "            return self.linear(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYAAV0kFBMQC"
      },
      "source": [
        "### FusedUpsampling，FusedDownsampling\n",
        "DC-GANをはじめとする多くのGANで問題視されている1つが，Generatorのアップサンプル時にartifactが載りやすいことです．\n",
        "アップサンプリングでは，bilinear補間やdeconvolutionを用いて引き延ばすため，artifactが載りやすくなります．\n",
        "この問題を解消するために，アップサンプル時のConvolutionで用いる重みフィルタにローパスフィルタ（ガウスフィルタ）を用いています．\n",
        "これによって，鮮明な画像生成の手助けをしています．\n",
        "\n",
        "この問題はダウンサンプル時にも同様のことが言えて，ダウンサンプル前にガウスフィルタを用います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC0nlDi0BMQC"
      },
      "source": [
        "class FusedUpsample(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size, padding=0):\n",
        "        super().__init__()\n",
        "\n",
        "        weight = torch.randn(in_channel, out_channel, kernel_size, kernel_size)\n",
        "        bias = torch.zeros(out_channel)\n",
        "\n",
        "        fan_in = in_channel * kernel_size * kernel_size\n",
        "        self.multiplier = sqrt(2 / fan_in)\n",
        "\n",
        "        self.weight = nn.Parameter(weight)\n",
        "        self.bias = nn.Parameter(bias)\n",
        "\n",
        "        self.pad = padding\n",
        "\n",
        "    def forward(self, input):\n",
        "        weight = F.pad(self.weight * self.multiplier, [1, 1, 1, 1])\n",
        "        weight = (\n",
        "            weight[:, :, 1:, 1:]\n",
        "            + weight[:, :, :-1, 1:]\n",
        "            + weight[:, :, 1:, :-1]\n",
        "            + weight[:, :, :-1, :-1]\n",
        "        ) / 4\n",
        "\n",
        "        out = F.conv_transpose2d(input, weight, self.bias, stride=2, padding=self.pad)\n",
        "\n",
        "        return out\n",
        "    \n",
        "class FusedDownsample(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size, padding=0):\n",
        "        super().__init__()\n",
        "\n",
        "        weight = torch.randn(out_channel, in_channel, kernel_size, kernel_size)\n",
        "        bias = torch.zeros(out_channel)\n",
        "\n",
        "        fan_in = in_channel * kernel_size * kernel_size\n",
        "        self.multiplier = sqrt(2 / fan_in)\n",
        "\n",
        "        self.weight = nn.Parameter(weight)\n",
        "        self.bias = nn.Parameter(bias)\n",
        "\n",
        "        self.pad = padding\n",
        "\n",
        "    def forward(self, input):\n",
        "        weight = F.pad(self.weight * self.multiplier, [1, 1, 1, 1])\n",
        "        weight = (\n",
        "            weight[:, :, 1:, 1:]\n",
        "            + weight[:, :, :-1, 1:]\n",
        "            + weight[:, :, 1:, :-1]\n",
        "            + weight[:, :, :-1, :-1]\n",
        "        ) / 4\n",
        "\n",
        "        out = F.conv2d(input, weight, self.bias, stride=2, padding=self.pad)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5nvUGYGBMQF"
      },
      "source": [
        "### Pixel-wise normalization，Adaptive instance normalization\n",
        "DC-GANでは，Batch Normalizationをネットワーク全体に使用することで高解像な画像生成をしていました．\n",
        "Batch Normalizationは高精度な認識を得るためのネットワークに使用することは効果的ですが，単純にミニバッチで正規化をする処理は画像生成には適さないことが判明しました．\n",
        "Batch Normalizationは，学習可能なパラメータを含むので，画像生成には不要かつモード崩壊を起こす1つの原因となるため，排除してPixel-wise normalizationと呼ばれる正規化が利用されています．\n",
        "これはPG-GANでGeneratorの全層に用いられている，各画像で正規化をする方法です．\n",
        "StyleGANでは，Mapping networkのみに利用されています．Pixel-wise normalizationは以下の式のとおりです．\n",
        "$$\n",
        "b_{x, y} = \\frac{a_{x, y}}{\\sqrt{\\frac{1}{N}\\sum_{j=0}^{N-1}(a_{x,y}^{j})^{2}+\\epsilon}}\n",
        "$$\n",
        "ここで，$a_{x,y}, b_{x,y}$は，それぞれ正規化前の特徴マップと正規化後の特徴マップです．また，Nは特徴マップの総数で，$\\epsilon=10^{-8}$です．\n",
        "\n",
        "Adaptive instance normalizationは，pytorch内部で実装がされていますが，内部処理を少々変更する必要があるので以下に定義してあります．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf2Ssp1VBMQF"
      },
      "source": [
        "class PixelNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Pixel-wise normalization\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input / torch.sqrt(torch.mean(input ** 2, dim=1, keepdim=True) + 1e-8)\n",
        "\n",
        "class AdaptiveInstanceNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Adaptive instance normalization\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channel, style_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.norm = nn.InstanceNorm2d(in_channel)\n",
        "        self.style = EqualLinear(style_dim, in_channel * 2)\n",
        "\n",
        "        self.style.linear.bias.data[:in_channel] = 1\n",
        "        self.style.linear.bias.data[in_channel:] = 0\n",
        "\n",
        "    def forward(self, input, style):\n",
        "        style = self.style(style).unsqueeze(2).unsqueeze(3)\n",
        "        gamma, beta = style.chunk(2, 1)\n",
        "\n",
        "        out = self.norm(input)\n",
        "        out = gamma * out + beta\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUHj3KMrBMQH"
      },
      "source": [
        "### その他のレイヤ\n",
        "* ConstantInput：Generatorに入力する任意のサイズの定数マップを作成するためのクラス．\n",
        "* NoiseInjection：GeneratorのAdaINの前に与えるノイズを作成するために必要なクラス．\n",
        "* Blur：ダウンサンプル，アップサンプル前後でローパスフィルタを施すために必要となるクラス．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWpgXFJ_BMQH"
      },
      "source": [
        "class ConstantInput(nn.Module):\n",
        "    def __init__(self, channel, size=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input = nn.Parameter(torch.randn(1, channel, size, size))\n",
        "\n",
        "    def forward(self, batch):\n",
        "        out = self.input.repeat(batch, 1, 1, 1)\n",
        "        return out\n",
        "    \n",
        "class NoiseInjection(nn.Module):\n",
        "    def __init__(self, channel):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.zeros(1, channel, 1, 1))\n",
        "\n",
        "    def forward(self, image, noise):\n",
        "        return image + self.weight * noise\n",
        "    \n",
        "class Blur(nn.Module):\n",
        "    def __init__(self, kernel=None, normalize=None, flip=None, stride=1):\n",
        "        super(Blur, self).__init__()\n",
        "        self.normalize = normalize\n",
        "        self.flip = flip\n",
        "        if kernel is None:\n",
        "            kernel = [1, 2, 1]\n",
        "        kernel = self.create_kernel(kernel)\n",
        "\n",
        "        self.register_buffer('kernel', kernel)\n",
        "        self.stride = stride\n",
        "        \n",
        "        \n",
        "    def create_kernel(self, kernel):\n",
        "        kernel = torch.tensor(kernel).float().view(1, len(kernel))\n",
        "        kernel = kernel.t() * kernel\n",
        "        kernel = kernel.unsqueeze(0).unsqueeze(0)\n",
        "        \n",
        "        if self.normalize:\n",
        "            kernel /= kernel.sum()\n",
        "        if self.flip:\n",
        "            kernel = torch.flip(kernel, [2, 3])\n",
        "        return kernel\n",
        "    \n",
        "    def forward(self, input):\n",
        "        kernel = self.kernel.expand(input.size(1), -1, -1, -1)\n",
        "        padding_size = int((self.kernel.size(2) - 1) / 2)\n",
        "        return F.conv2d(input, kernel, stride=self.stride, \n",
        "                        padding=padding_size, groups=input.size(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuoGefe5BMQJ"
      },
      "source": [
        "## Generatorの定義\n",
        "Generatorはいくつかのブロックを用いて構成します．\n",
        "各ブロックは，2層の畳み込み処理を含んでいます．\n",
        "この2層の内，１つ目の畳み込み処理の前に特徴マップをアップサンプルします．\n",
        "全てのブロックの後に使用する活性化関数は，LeakyReLUです．\n",
        "活性化関数の前にノイズを加算する処理であるNoise injectionを導入します．\n",
        "活性化関数を通した後は，mapping networkで写像した空間$\\mathcal{W}$からサンプリングした潜在変数と特徴マップを用いてAdaptive instance normalizationで正規化します．Generatorは，mapping networkを必要とするため，全結合層8層とLeakyReLUで非線形変換するネットワークを構築しています．\n",
        "\n",
        "Generatorのネットワーク図を以下に示します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT-iYuccXcjD"
      },
      "source": [
        "<img src=\"https://dl.dropboxusercontent.com/s/myeopyl0g9n4iks/StyleGAN_G.png\" width=40%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_EwXYoIXbaa"
      },
      "source": [
        "また，StyleGANはプログレッシブな学習をするため，高解像度の画像を生成するために層を追加します．\n",
        "しかしながら，数epoch学習した層と次の解像度を生成する初期値の畳み込み処理は，ギャップが大きいため徐々に新たに追加した層を反映するように線形変化する重み$\\alpha$を使用します．<br>\n",
        "以下に16×16から32×32への移り変わりを示す．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFhrgQzbcLQf"
      },
      "source": [
        "<img src=\"https://dl.dropboxusercontent.com/s/c21cxt4bxdntlsn/G_transition.png\" width=40%>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPH482jgBMQK"
      },
      "source": [
        "class GBlock(nn.Module):\n",
        "    def __init__(self, in_features, out_features, kernel_size, stride, padding,\n",
        "                 up=False, fused=False, n_style=512, initial=False):\n",
        "        super(GBlock, self).__init__()\n",
        "        if not initial:\n",
        "            if up:\n",
        "                if fused:\n",
        "                    self.c1 = nn.Sequential(\n",
        "                        FusedUpsample(in_features, out_features, kernel_size=kernel_size, padding=padding),\n",
        "                        Blur([1, 2, 1], True, True, 1))\n",
        "                else:\n",
        "                    self.c1 = nn.Sequential(\n",
        "                        nn.Upsample(scale_factor=2, mode='bilinear'),\n",
        "                        Blur([1, 2, 1], True, True, 1),\n",
        "                        EqualConv2d(in_features, out_features, kernel_size=kernel_size, stride=stride,\n",
        "                                    padding=padding, bias=True)\n",
        "                    )\n",
        "            else:\n",
        "                self.c1 = EqualConv2d(in_features, out_features, kernel_size=kernel_size, stride=stride,\n",
        "                                          padding=padding, bias=True)\n",
        "                \n",
        "        self.initial = initial\n",
        "        self.make_noise1 = NoiseInjection(out_features)\n",
        "        self.adain1 = AdaptiveInstanceNorm(out_features, n_style)\n",
        "        self.lrelu1 = nn.LeakyReLU(negative_slope=0.2)\n",
        "        \n",
        "        self.c2 = nn.Sequential(\n",
        "            EqualConv2d(out_features, out_features, kernel_size=kernel_size, stride=stride,\n",
        "                        padding=padding, bias=True))\n",
        "        self.make_noise2 = NoiseInjection(out_features)\n",
        "        self.adain2 = AdaptiveInstanceNorm(out_features, n_style)\n",
        "        self.lrelu2 = nn.LeakyReLU(negative_slope=0.2)\n",
        "        \n",
        "    def forward(self, x, style_code, noise):\n",
        "        out = x\n",
        "        if not self.initial:\n",
        "            out = self.c1(out)\n",
        "        out = self.make_noise1(out, noise)\n",
        "        out = self.lrelu1(out)\n",
        "        out = self.adain1(out, style_code)\n",
        "\n",
        "        out = self.c2(out)\n",
        "        out = self.make_noise2(out, noise)\n",
        "        out = self.lrelu2(out)\n",
        "        out = self.adain2(out, style_code)\n",
        "        return out\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, switch_ch=4, img_size=1024, n_ch=512, n_style=512, n_mapping_layers=8):\n",
        "        super(Generator, self).__init__()\n",
        "        n_block = int(np.log2(img_size))\n",
        "        \n",
        "        module_list = []\n",
        "        conv_module_list = []\n",
        "        fused = False\n",
        "        in_ch = n_ch\n",
        "        out_ch = n_ch\n",
        "        for block_idx in range(1, n_block+1):\n",
        "            if block_idx == 1:initial = True\n",
        "            else:initial = False\n",
        "            module_list.append(GBlock(in_ch, out_ch, kernel_size=3, stride=1, padding=1, up=True, fused=fused, initial=initial))\n",
        "            conv_module_list.append(ConvBlock(out_ch, 3, kernel_size=1, stride=1, padding=0))\n",
        "            if block_idx >= switch_ch:\n",
        "                fused = True\n",
        "                in_ch = out_ch\n",
        "                out_ch = in_ch // 2\n",
        "        \n",
        "        mapping_network = [PixelNorm()]\n",
        "        for i in range(n_mapping_layers):\n",
        "            mapping_network.append(EqualLinear(n_style, n_style, mapping_linear=True, \n",
        "                                   gain=2 ** 0.5, lrmul=0.01, bias=True, use_wscale=True))\n",
        "            mapping_network.append(nn.LeakyReLU(negative_slope=0.2))\n",
        "        \n",
        "        self.const = ConstantInput(n_ch)\n",
        "        self.main_net = nn.ModuleList(module_list)\n",
        "        self.to_rgb = nn.ModuleList(conv_module_list)\n",
        "        self.mapping_network = nn.Sequential(*mapping_network)\n",
        "        \n",
        "    def forward(self, style, noise, depth=0, alpha=-1, true_size=1024):\n",
        "        const = self.const(style.size(0))\n",
        "        style_code = self.mapping_network(style)\n",
        "        \n",
        "        if depth > 0 and alpha < 1.0:\n",
        "            h = const\n",
        "            presize_noise = torch.randn(h.size(0), 1, h.size(2), h.size(3)).type_as(style)\n",
        "            for i in range(depth - 1):\n",
        "                size = true_size // (2 ** (depth - i))\n",
        "                noise_roop = torch.randn(h.size(0), 1, size, size).type_as(style)\n",
        "                h = self.main_net[i](h, style_code, noise_roop)\n",
        "                presize_noise = torch.randn(h.size(0), 1, h.size(2)*2, h.size(3)*2).type_as(style)\n",
        "            \n",
        "            h1 = self.main_net[depth - 1](h, style_code, presize_noise)\n",
        "            h2 = F.upsample(h1, scale_factor=2, mode='bilinear')\n",
        "            h3 = self.to_rgb[depth - 1](h2)\n",
        "            h4 = self.main_net[depth](h1, style_code, noise)\n",
        "            h4 = self.to_rgb[depth](h4)\n",
        "            \n",
        "            h = h3 - alpha * (h3 - h4)\n",
        "        else:\n",
        "            h = const\n",
        "            for i in range(depth):\n",
        "                size = true_size // (2 ** (depth - i))\n",
        "                noise_roop = torch.randn(h.size(0), 1, size, size).type_as(style)\n",
        "                h = self.main_net[i](h, style_code, noise_roop)\n",
        "                \n",
        "            h = self.main_net[depth](h, style_code, noise)\n",
        "            h = self.to_rgb[depth](h)\n",
        "        \n",
        "        return h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4h_7FRpBMQM"
      },
      "source": [
        "## Discriminatorの定義\n",
        "DiscriminatorもGeneratorと同様でいくつかのブロックが積み重なったネットワークです．DiscriminatorへはBatchNormalizationなどの正規化を一切使用しません．\n",
        "DiscriminatorもGeneratorと同様で，スムーズに解像度の移り変わりができるように線形増加する$\\alpha$によって徐々に次の改造へ移り変わるネットワーク構造を使用します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N563Dm2ofQ9B"
      },
      "source": [
        "<img src=\"https://dl.dropboxusercontent.com/s/zpfdnq4gcilochs/StyleGAN_D_transition.png\" width=40%>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pYFfpH5BMQM"
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_features, out_features, kernel_size, stride,\n",
        "                 padding, down=False, fused=False, d_last=False):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        \n",
        "        conv_layer1 = [\n",
        "            EqualConv2d(in_features, out_features, kernel_size=kernel_size, stride=stride,\n",
        "                        padding=padding, bias=True),\n",
        "            nn.LeakyReLU(negative_slope=0.2)\n",
        "        ]\n",
        "        \n",
        "        if d_last:\n",
        "            kernel_size = 4\n",
        "            stride = 1\n",
        "            padding = 0\n",
        "            \n",
        "        if down:\n",
        "            if fused:\n",
        "                conv_layer2 = [\n",
        "                    Blur([1, 2, 1], True, True, 1),\n",
        "                    FusedDownsample(out_features, out_features, kernel_size=kernel_size, padding=padding),\n",
        "                    nn.LeakyReLU(negative_slope=0.2)\n",
        "                ]\n",
        "            else:\n",
        "                conv_layer2 = [\n",
        "                    Blur([1, 2, 1], True, True, 1),\n",
        "                    EqualConv2d(out_features, out_features, kernel_size=kernel_size, stride=stride,\n",
        "                        padding=padding, bias=True),\n",
        "                    nn.AvgPool2d(2, 2),\n",
        "                    nn.LeakyReLU(negative_slope=0.2)\n",
        "                ]\n",
        "        else:\n",
        "            conv_layer2 = [\n",
        "                EqualConv2d(out_features, out_features, kernel_size=kernel_size, stride=stride,\n",
        "                            padding=padding, bias=True),\n",
        "                nn.LeakyReLU(negative_slope=0.2)\n",
        "            ]\n",
        "        \n",
        "        self.c1 = nn.Sequential(*conv_layer1)\n",
        "        self.c2 = nn.Sequential(*conv_layer2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.c1(x)\n",
        "        out = self.c2(h)\n",
        "        return out\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, switch_ch=4, img_size=1024, n_ch=16):\n",
        "        super(Discriminator, self).__init__()\n",
        "        n_block = int(np.log2(img_size))\n",
        "        self.total_depth = n_block - 1\n",
        "        \n",
        "        module_list = []\n",
        "        conv_module_list = []\n",
        "        down = True\n",
        "        fused = False\n",
        "        d_last = False\n",
        "        in_ch = n_ch\n",
        "        out_ch = n_ch\n",
        "        for block_idx in range(1, n_block+1):\n",
        "            if block_idx == n_block:\n",
        "                down = False\n",
        "                d_last = True\n",
        "            conv_module_list.append(nn.Sequential(EqualConv2d(3, in_ch, kernel_size=1, stride=1, padding=0),\n",
        "                                    nn.LeakyReLU(negative_slope=0.2)))\n",
        "            module_list.append(ConvBlock(in_ch, out_ch, kernel_size=3, stride=1, padding=1, down=down, fused=fused, d_last=d_last))\n",
        "            if block_idx <= switch_ch + 1:\n",
        "                fused = True\n",
        "                in_ch = out_ch\n",
        "                out_ch = in_ch * 2\n",
        "            else:\n",
        "                in_ch = out_ch\n",
        "            \n",
        "        self.main_net = nn.Sequential(*module_list)\n",
        "        self.from_rgb = nn.Sequential(*conv_module_list)\n",
        "        self.linear = EqualLinear(out_ch, 1)\n",
        "        \n",
        "    def forward(self, x, depth=0, alpha=0):\n",
        "        if depth > 0 and alpha < 1.0:\n",
        "            h1 = self.from_rgb[self.total_depth - depth](x)\n",
        "            h1 = self.main_net[self.total_depth - depth](h1)\n",
        "            x2 = F.avg_pool2d(x, 2, 2)\n",
        "            h2 = F.leaky_relu(self.from_rgb[self.total_depth - depth+1](x2))\n",
        "            h = h2 - alpha * (h2 - h1)\n",
        "            \n",
        "        else:\n",
        "            h = self.from_rgb[self.total_depth - depth](x)\n",
        "            h = self.main_net[self.total_depth - depth](h)\n",
        "            \n",
        "        for i in range(depth):\n",
        "            h = self.main_net[self.total_depth - depth+1+i](h)\n",
        "            \n",
        "        out = self.linear(h.view(h.size(0), -1))\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5fCHZypgU-4"
      },
      "source": [
        "## StyleGANの学習コード\n",
        "StyleGANは，非常に多くの時間と計算リソースを必要とする手法です．\n",
        "そのため，Colabratory上では到底学習が終わりません．\n",
        "余力と計算リソースがある人は，以下のコードを利用して好きなデータセットで学習をしてみてください．\n",
        "\n",
        "このStyleGANは，CelebA-HQを利用して画像生成をしました．\n",
        "CelebA-HQのダウンロード方法は，[著者の実装](https://github.com/tkarras/progressive_growing_of_gans)を参考にしてください．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KltGlWEY5bLZ"
      },
      "source": [
        "<img src=\"https://dl.dropboxusercontent.com/s/tp2l6mz6x7ky90f/CelebA-hq.png\" width=50%>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtjiapmbjc9S"
      },
      "source": [
        "!wget -q https://www.dropbox.com/s/v8vdmzk4l50qtw7/params.yml?dl=1 -O params.yml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAA6yc5Xkk1_"
      },
      "source": [
        "def update_EMA(src, tgt, strength=0.999):\n",
        "    with torch.no_grad():\n",
        "        paramnames = dict(src.module.named_parameters())\n",
        "        for k, v in tgt.named_parameters():\n",
        "            param = paramnames[k].detach().cpu()\n",
        "            v.copy_(strength * v + (1 - strength) * param)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3mgbesvklro"
      },
      "source": [
        "def gradient_penalty(real_img, fake_img, alpha, d):           \n",
        "    eps = torch.rand(real_img.size(0), 1, 1, 1).cuda().expand_as(real_img)\n",
        "    interpolated = torch.autograd.Variable(eps * real_img.data + (1 - eps) * fake_img.data, \n",
        "                                           requires_grad=True)\n",
        "    out = D(interpolated, d, alpha=alpha)\n",
        "\n",
        "    grad = torch.autograd.grad(outputs=out,\n",
        "                               inputs=interpolated,\n",
        "                               grad_outputs=torch.ones(out.size()).to(device),\n",
        "                               retain_graph=True,\n",
        "                               create_graph=True,\n",
        "                               only_inputs=True)[0]\n",
        "\n",
        "    grad = grad.view(grad.size(0), -1)\n",
        "    grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n",
        "    d_loss_gp = torch.mean((grad_l2norm - 1) ** 2)\n",
        "    \n",
        "    return d_loss_gp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_Fb0G-Vg3ln"
      },
      "source": [
        "yaml_path = \"params.yml\"\n",
        "with open(yaml_path, 'r+') as f:\n",
        "    training_params = yaml.load(f)\n",
        "\n",
        "depth = 9\n",
        "alpha = 0\n",
        "n_style = 512\n",
        "lambda_r1 = 10\n",
        "lambda_drift = 1e-3\n",
        "lambda_gp = 10\n",
        "loss_func = 'hinge'\n",
        "\n",
        "for d in range(depth):\n",
        "    print('='*50, 'Now network depth is %d' % d, '='*50)\n",
        "    size = 4 * 2 ** d\n",
        "    alpha = alpha\n",
        "    iteration = 0\n",
        "    delta = deltas['%dx%d' % (size, size)]\n",
        "    epoch = epochs['%dx%d' % (size, size)]\n",
        "    batch = batch_size['%dx%d' % (size, size)]\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.Resize((size, size)),\n",
        "        transforms.ToTensor()])\n",
        "    training_data = DatasetMaker(size, conf.h5py_path, conf.data_path, conf.txt_path, transform=transform_train)\n",
        "    training_dataset = DataLoader(dataset=training_data, batch_size=batch,\n",
        "                                  drop_last=True, shuffle=True, num_workers=conf.workers)\n",
        "    \n",
        "    print('Delta: %f' % delta)\n",
        "    print('Epoch: %d' % epoch)\n",
        "    print('Mini batch size: %d' % batch_size['%dx%d' % (size, size)])\n",
        "    print('Learning rate: %f' % lr['%dx%d' % (size, size)])\n",
        "    print('='*50, 'Currently, image size is %d x %d' % (size, size), '='*50)\n",
        "    \n",
        "    for g_params in G_opt.param_groups:\n",
        "        g_params['lr'] == lr['%dx%d' % (size, size)]\n",
        "    for d_params in D_opt.param_groups:\n",
        "        d_params['lr'] == lr['%dx%d' % (size, size)]\n",
        "    G.train()\n",
        "    D.train()\n",
        "    #for epoch in range(1, epochs['%dx%d' % (size, size)]+1):\n",
        "    for epoch in range(1, epoch+1):\n",
        "        Tensor = torch.cuda.FloatTensor\n",
        "        for idx, img in enumerate(training_dataset):\n",
        "            flag_real = Tensor(img.size(0)).fill_(1.0)\n",
        "            flag_fake = Tensor(img.size(0)).fill_(0.0)\n",
        "            real_img = img.to(device)\n",
        "            noise = torch.randn(real_img.size(0), 1, size, size).to(device)\n",
        "            style_noise = torch.randn(real_img.size(0), n_style).to(device)\n",
        "        \n",
        "            # ====================== Update Discriminator ======================\n",
        "            D.zero_grad()\n",
        "            dis_real_out = D(real_img, d, alpha=alpha)\n",
        "            fake_img = G(style_noise, noise, d, alpha=alpha, true_size=size)\n",
        "            dis_fake_out = D(fake_img, d, alpha=alpha)\n",
        "            \n",
        "            if loss_func == 'wgan-gp':\n",
        "                dis_real = - torch.mean(dis_real_out)\n",
        "                dis_fake = torch.mean(dis_fake_out)\n",
        "            elif loss_func == 'hinge':\n",
        "                dis_real = nn.ReLU()(1.0 - dis_real_out).mean()\n",
        "                dis_fake = nn.ReLU()(1.0 + dis_fake_out).mean()\n",
        "            elif loss_func == 'relativistic_hinge':\n",
        "                r_diff = dis_fake_out - torch.mean(dis_real_out)\n",
        "                f_diff = dis_real_out - torch.mean(dis_fake_out)\n",
        "                dis_real = nn.ReLU()(1.0 + r_diff).mean()\n",
        "                dis_fake = nn.ReLU()(1.0 - f_diff).mean()\n",
        "            elif loss_func == 'R1':\n",
        "                dis_real = criterion(dis_real_out.view(-1), flag_real)\n",
        "                dis_fake = criterion(dis_fake_out.view(-1), flag_fake)\n",
        "            \n",
        "            dis_loss = dis_real + dis_fake\n",
        "            \n",
        "            if loss_func == 'R1':\n",
        "                real_img_detach = torch.autograd.Variable(real_img, requires_grad=True)\n",
        "                dis_real_out = D(real_img_detach, d, alpha=alpha).view(real_img.size(0), -1)\n",
        "                dis_grad = torch.autograd.grad(outputs=dis_real_out,\n",
        "                               inputs=real_img,\n",
        "                               grad_outputs=torch.ones(dis_real_out.size()).to(device),\n",
        "                               retain_graph=True,\n",
        "                               create_graph=True)[0].view(real_img.size(0), -1)\n",
        "                r1_penalty = torch.sum(torch.mul(dis_grad, dis_grad))\n",
        "                r1_loss = lambda_r1 / 2 + r1_penalty\n",
        "                dis_loss += r1_loss\n",
        "            elif loss_func == 'wgan-gp':\n",
        "                loss_gp = gradient_penalty(real_img, fake_img, alpha, d)\n",
        "                d_drift = lambda_drift * torch.mean(dis_real ** 2)\n",
        "                dis_loss = dis_loss + lambda_gp * loss_gp + d_drift\n",
        "            \n",
        "            dis_loss.backward()\n",
        "            D_opt.step()\n",
        "        \n",
        "            # ====================== Update Generator ======================\n",
        "            G.zero_grad()\n",
        "            style_noise = torch.randn(real_img.size(0), conf.n_style)\n",
        "            fake_img = G(style_noise, noise, d, alpha=alpha, true_size=size)\n",
        "            gen_out = D(fake_img, d, alpha=alpha)\n",
        "            if loss_func == 'wgan-gp' or 'hinge':\n",
        "                gen_loss = - gen_out.mean()\n",
        "            elif loss_func == 'relativistic_hinge':\n",
        "                r_diff = gen_out - torch.mean(dis_real_out)\n",
        "                f_diff = dis_real_out - torch.mean(gen_out)\n",
        "                gen_real_loss = nn.ReLU()(1.0 - r_diff).mean()\n",
        "                gen_fake_loss = nn.ReLU()(1.0 + f_diff).mean()\n",
        "                gen_loss = gen_real_loss + gen_fake_loss\n",
        "            elif loss_func == 'R1':\n",
        "                gen_loss = criterion(gen_out.view(-1), flag_real)\n",
        "                \n",
        "            gen_loss.backward()\n",
        "            G_opt.step()\n",
        "            update_EMA(G, G_ema, strength=0.999)\n",
        "            \n",
        "            alpha = min(1, alpha + delta)\n",
        "        \n",
        "            iteration += 1\n",
        "            if idx %  100 == 0:\n",
        "                print('Training epoch: {} [{}/{} ({:.0f}%)] | D loss : {:.6f} | G loss: {:.6f} | Alpha: {:.6f}|'\\\n",
        "                      .format(epoch, idx * len(img), len(training_dataset.dataset),\n",
        "                      100. * idx / len(training_dataset), dis_loss.item(), gen_loss.item(), alpha))\n",
        "                \n",
        "                tb.add_scalars('prediction loss depth%d' % d,\n",
        "                               {'D': dis_loss.item(),\n",
        "                                'G': gen_loss.item()},\n",
        "                                iteration)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9GvfyZzE-DN"
      },
      "source": [
        "# 学習済みモデルのダウンロード\n",
        "長時間を要するStyleGANを事前に学習しておいたモデルをダウンロードして，どのような画像が生成されるのか確認してください．\n",
        "\n",
        "学習済みモデルは，各解像度で用意しました．以下にダウンロードURLを載せておきます．\n",
        "* 解像度4x4：https://www.dropbox.com/s/ybqnx1xgz6y0fuc/StyleGAN_4x4_model.zip?dl=0\n",
        "* 解像度8x8：https://www.dropbox.com/s/ir0txeef6sogj2t/StyleGAN_8x8_model.zip?dl=0\n",
        "* 解像度16x16：https://www.dropbox.com/s/l6rb4r552ffu3kw/StyleGAN_16x16_model.zip?dl=0\n",
        "* 解像度32x32：https://www.dropbox.com/s/4rzn4g5d80s7rrw/StyleGAN_32x32_model.zip?dl=0\n",
        "* 解像度64x64：https://www.dropbox.com/s/2t70nux0mftnrtl/StyleGAN_64x64_model.zip?dl=0\n",
        "* 解像度128x128：https://www.dropbox.com/s/cth6bonvc6f4hnd/StyleGAN_128x128_model.zip?dl=0\n",
        "* 解像度256x256：https://www.dropbox.com/s/ap8glamsjhzxp9u/StyleGAN_256x256_model.zip?dl=0\n",
        "* 解像度512x512：https://www.dropbox.com/s/543rh2xsnt2mz5t/StyleGAN_512x512_model.zip?dl=0\n",
        "\n",
        "必要に応じて以下のコードのURLを変更してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpUSBqpNEjub"
      },
      "source": [
        "!wget -q https://www.dropbox.com/s/543rh2xsnt2mz5t/StyleGAN_512x512_model.zip?dl=0 -O StyleGAN_model_512x512.zip\n",
        "!unzip -q -o StyleGAN_model_512x512.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbCEzKKTZTOg"
      },
      "source": [
        "img_size = 1024\n",
        "n_ch = 512\n",
        "n_style = 512\n",
        "n_mapping = 8\n",
        "model_path = './StyleGAN_512x512_model/gen_ema_512x512'\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "G = Generator(switch_ch=4, img_size=img_size, n_ch=n_ch, n_style=n_style, n_mapping_layers=n_mapping).to(device)\n",
        "G.load_state_dict(torch.load(model_path))\n",
        "\n",
        "d = 7\n",
        "alpha = 1\n",
        "size = 4 * 2 ** d\n",
        "n_img = 1\n",
        "style_noise = torch.randn(n_img, n_style).to(device)\n",
        "noise = torch.randn(1, 1, size, size).cuda()\n",
        "img = G(style_noise, noise, d, alpha=alpha, true_size=size)\n",
        "Image.fromarray((img * 256.).clamp(min=0, max=255).data.cpu().squeeze().permute(1,2,0).numpy().astype(np.uint8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo_POTpwBMQR"
      },
      "source": [
        "img_size = 1024\n",
        "n_ch = 512\n",
        "n_style = 512\n",
        "n_mapping = 8\n",
        "model_path = './StyleGAN_512x512_model/gen_ema_512x512'\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "G = Generator(switch_ch=4, img_size=img_size, n_ch=n_ch, n_style=n_style, n_mapping_layers=n_mapping).to(device)\n",
        "G.load_state_dict(torch.load(model_path))\n",
        "\n",
        "d = 7\n",
        "alpha = 1\n",
        "size = 4 * 2 ** d\n",
        "n_interp = 10\n",
        "style_noise1 = torch.randn(1, n_style).to(device)\n",
        "style_noise2 = torch.randn(1, n_style).to(device)\n",
        "noise = torch.randn(1, 1, size, size).to(device)\n",
        "Ims = []\n",
        "for i in range(n_interp):\n",
        "    weight = i / (n_interp+1)\n",
        "    style_noise = (1-weight)*style_noise1 + weight*style_noise2\n",
        "    fake_img = G(style_noise, noise, d, alpha=alpha, true_size=size)\n",
        "    Ims.append((fake_img * 256).clamp(min=0., max=255.).data.cpu().squeeze().permute(1,2,0).numpy().astype(np.uint8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHGm0QuGBMQS"
      },
      "source": [
        "%matplotlib inline\n",
        "#%matplotlib nbagg\n",
        "from IPython.display import HTML\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "fig = plt.figure()\n",
        "plt.axis('off')\n",
        "ims = [[plt.imshow(im, animated=True)] for im in Ims]\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K5lXmiwBMQY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGpxdkUk22WQ"
      },
      "source": [
        "## 課題\n",
        "* StyleCodeを動かすと，どのようになるでしょうか？また，写像した空間からサンプリングした潜在変数を各解像度で補間したときにどのような変化が現れるか確認してください．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHXi4U39wF5_"
      },
      "source": [
        "# 参考文献\n",
        "[1] Tero Karras, Samuli Laine and Timo Aila, A Style-Based Generator Architecture for Generative Adversarial Networks, CVPR, 2019.\\\n",
        "[2] Tero Karras, Timo Aila, Samuli Laine and Jaakko Lehtinen, Progressive Growing of GANs for Improved Quality, Stability, and Variation, ICLR, 2018.\\\n",
        "[3] Xun Huanga and Serge Belongie, Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization, ICCV, 2017."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg3P5ceJBMQa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}