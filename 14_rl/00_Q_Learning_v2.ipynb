{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/14_rl/00_Q_Learning_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUK6DbbAq0Al"
      },
      "source": [
        "# 強化学習（Q学習とQ Network）によるCart Pole制御\n",
        "\n",
        "---\n",
        "\n",
        "## 目的\n",
        "強化学習を用いてCart Pole制御を学習する．\n",
        "Cart Pole制御とは，Cartに乗っているPoleが倒れないように台車を左右に動かすことである．\n",
        "\n",
        "Q-Tableを用いた従来のQ学習による方法と，Q-Networkを用いた方法の２種類を行う．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Log6bIaiDnkV"
      },
      "source": [
        "## 準備\n",
        "\n",
        "### Google Colaboratoryの設定確認・変更\n",
        "本チュートリアルではPyTorchを利用してニューラルネットワークの実装を確認，学習および評価を行います．\n",
        "**GPUを用いて処理を行うために，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．**\n",
        "\n",
        "### モジュールの追加インストール\n",
        "下記のプログラムを実行して，実験結果の表示に必要な追加ライブラリやモジュールをインストールする．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piC8yNcqDmyC"
      },
      "outputs": [],
      "source": [
        "!apt-get -qq -y install libcusparse9.1 libnvrtc9.1 libnvtoolsext1 > /dev/null\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.9.1 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "\n",
        "!pip -q install gym==0.17.3\n",
        "!pip -q install pyglet==1.5.0\n",
        "!pip -q install pyopengl==3.1.6\n",
        "!pip -q install pyvirtualdisplay==3.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aKH_oZ9Lb6i"
      },
      "source": [
        "## モジュールのインポート\n",
        "はじめに必要なモジュールをインポートする．\n",
        "\n",
        "今回はPyTorchに加えて，Cart Poleを実行するためのシミュレータであるOpenAI Gym（gym）をインポートする．"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import gym.spaces\n",
        "\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# GPUの確認\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('Use CUDA:', use_cuda)"
      ],
      "metadata": {
        "id": "IIv_AyxF9pwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0V-KflRLV-6"
      },
      "source": [
        "# Q学習 (Q-table)\n",
        "Q学習とは，TD法を用いた代表的な手法の一つです．\n",
        "最適な行動価値（Q値）を推定するように学習を行い，Q値を行動の指針として用いることで最適な行動を行います．\\\n",
        "Q学習では，全ての状態$s$と行動$a$に対する価値$Q(s, a)$を記録するテーブル(Q-table) を作成します．\n",
        "しかし，学習初期の段階では，各状態と行動に対する正確な行動価値が分からないため，Q-tableを任意の値に初期化します．\n",
        "その後，様々な状態の下で実際に行動を選択し，推定した行動価値を用いてQ-tableを修正していきます．\n",
        "以下に，簡略化したQ学習の流れを記載します．\n",
        "1. ある環境における全ての状態と行動に対する価値（Q値）を記録するためのQ-tableを作成\n",
        "2. Q-tableに記録されたQ値を任意の値で初期化\n",
        "3. $\\epsilon$-greedy法などを用いて環境に対する行動を選択\n",
        "4. 選択した行動に対する報酬値とQ-tableに記録されたQ値をもとに，Q-tableを更新\n",
        "5. 最適なQ-tableが完成するまで3,4を繰り返す"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mdO4rIl6IUQ"
      },
      "source": [
        "## 環境の作成\n",
        "今回の実験で使用する環境の作成を行います． [OpenAI Gym](https://www.gymlibrary.ml/) は，様々な種類の環境を提供しているモジュールです．\n",
        " \n",
        "今回の実験では，gymで利用できるCartPoleを使用します．(今回はCartPole-v0)\\\n",
        "まず，gym.make関数で使用したい環境を指定します．\n",
        "その後，reset関数を用いて環境を初期化し，環境に対する情報を確認します．\n",
        "\n",
        "CartPoleは，Cartに乗ったPoleを倒さないようにCartを制御するという環境です．環境における行動は，Cartを右か左に動かすという離散的な行動をもち，状態は連続的な値で表されたCart位置，Cart速度，Poleの角度，Poleの角速度からなる状態を持っています．"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "obs = env.reset()\n",
        "print('observation space:',env.observation_space)\n",
        "print('action space:',env.action_space)\n",
        "print('initial observation',obs)"
      ],
      "metadata": {
        "id": "Rf7M1xmD9tHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnmhZNvQLw4u"
      },
      "source": [
        "## Q-tableの作成，離散化処理\n",
        "Q-tableの作成とCartPole環境における状態の離散化処理を行います．\n",
        "\n",
        "Q-Learningは，Q-tableを用いた学習を行います．しかし，今回使用する環境であるCartPoleは，Cart位置，Cart速度，Poleの角度，Poleの角速度からなる4次元の状態をもっており，それぞれ以下の範囲で連続的な値をとります． \n",
        "\n",
        "- Cart位置：(-2.4～2.4) \n",
        "- Cart速度：(-3.0～3.0) \n",
        "- Poleの角度：(-0.5, 0.5) \n",
        "- Poleの角速度：(-2.0, 2.0) \n",
        "\n",
        "Q-Learningでは，任意の大きさのQ-tableを作成しなければいけないため，連続的な数値ではQ-tableを作成することができません．そのため，状態を分割し，離散的な値に変換することでQ-tableを作成可能とします． \n",
        "\n",
        "今回は，numpyのdigitize関数とlinspace関数を組み合わせて，離散化処理を行います． \\\n",
        "まず，linspace関数を用いて，分割数に応じて状態の範囲を区切ります．そして，dizitize関数で，ある値が区切られた範囲でどこの区画に属するのかを返します．これにより，連続的な数値であっても，その値がどの区画なのかという数値に変換されるため，離散化された値とすることができます．\n",
        "\n",
        "状態の分割により，環境における状態数が決定されるため，決定した状態数の大きさに合わせたQ-tableを作成します．\\\n",
        "はじめに，決定した状態の分割数$x$により，離散化する際の値が変化し，Q-tableの大きさが変化します．CartPoleは，4次元の状態を持っているため，Q-tableの大きさは$x^4(状態数)×2(actionの数)$となります．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XI-PzPcLUQh"
      },
      "outputs": [],
      "source": [
        "num_dizitized = 10  #状態の分割数\n",
        "\n",
        "def bins(clip_min, clip_max, num):\n",
        "    return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n",
        "\n",
        "# 各値を離散値に変換\n",
        "def digitize_state(observation):\n",
        "    cart_pos, cart_v, pole_angle, pole_v = observation\n",
        "    digitized = [\n",
        "        np.digitize(cart_pos, bins=bins(-2.4, 2.4, num_dizitized)),\n",
        "        np.digitize(cart_v, bins=bins(-3.0, 3.0, num_dizitized)),\n",
        "        np.digitize(pole_angle, bins=bins(-0.5, 0.5, num_dizitized)),\n",
        "        np.digitize(pole_v, bins=bins(-2.0, 2.0, num_dizitized))\n",
        "    ]\n",
        "    return sum([x * (num_dizitized**i) for i, x in enumerate(digitized)])\n",
        "\n",
        "q_table = np.random.uniform(low=-1, high=1, size=(num_dizitized**4, env.action_space.n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk0eS4l0L9Ym"
      },
      "source": [
        "## アクションの選択 ($\\epsilon$-greedy法)\n",
        "アクションの選択方法として，$\\epsilon$-greedy法を用いた行動選択の定義を行います．\n",
        "\n",
        "Q学習では，Q値をもとに行動選択を行いますが，Q値が大きい行動のみをとり続けることは，局所解へ陥ってしまう問題につながります．\n",
        "そのため，環境の適度な探索と，学習によって得られた知見を活用した行動のバランスが重要となります．\n",
        "この問題を，探索と利用のトレードオフと呼び，強化学習での大きな課題の一つとなっています．\n",
        "\n",
        "$\\epsilon$-greedy法は，適度な探索を行う手法であり，確率$\\epsilon$でQ値が最も大きい行動を選択し，確率$1-\\epsilon$で，ランダムな行動を選択するといった手法です．\n",
        "確率$\\epsilon$の値は，学習が進むごとに大きくなっていき，学習初期は探索を行い，徐々に最適な行動のみを選択するようになります．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZNVYe4RL_Po"
      },
      "outputs": [],
      "source": [
        "def get_action_q(next_state, episode):\n",
        "    #徐々に最適行動のみをとる、ε-greedy法\n",
        "    epsilon = 0.5 * (1 / (episode + 1))\n",
        "    if epsilon <= np.random.uniform(0, 1):\n",
        "        next_action = np.argmax(q_table[next_state])\n",
        "    else:\n",
        "        next_action = np.random.choice([0, 1])\n",
        "    return next_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3i-zLzYNgNr"
      },
      "source": [
        "## Qテーブルの更新\n",
        "Q-tableの更新関数を定義します．\\\n",
        "Q学習は，報酬と行動価値をもとに最適なQ-tableを推定していく手法であり，Q-tableの更新にはTD法を用いた更新を行います．\n",
        "ある状態におけるある行動の価値を，次状態における最良の行動価値に近似するように更新を行っていきます．\n",
        "Q値の更新式としては以下の式となります．\n",
        "\n",
        "$$\n",
        "Q(s,a)←Q(s,a)+\\alpha(r+\\gamma \\max_{a'}Q(s',a')-Q(s,a))\n",
        "$$\n",
        "\n",
        "この更新式を用いて，Q-tableの更新を行っていきます．あらゆる状態を経験することにより，最適なQ-tableが作成されます．\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYrNsbg1NjMM"
      },
      "outputs": [],
      "source": [
        "def update_Qtable(q_table, state, action, reward, next_state):\n",
        "    gamma = 0.99\n",
        "    alpha = 0.5\n",
        "    next_Max_Q=max(q_table[next_state][0],q_table[next_state][1] )\n",
        "    q_table[state, action] = (1 - alpha) * q_table[state, action] + alpha * (reward + gamma * next_Max_Q)\n",
        "\n",
        "    return q_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdnnYxzvtmi2"
      },
      "source": [
        "## 学習に必要な各種パラメータ設定\n",
        "今回の実験に使用する各種パラメータの設定を行います．ここでは，学習における総試行回数などの学習で必要なパラメータの設定を行っています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACap98mhtwgl"
      },
      "outputs": [],
      "source": [
        "max_number_of_steps = 200  #1試行のstep数\n",
        "num_consecutive_iterations = 100  #学習完了評価に使用する平均試行回数\n",
        "num_episodes = 1000  #総試行回数\n",
        "total_reward_vec = np.zeros(num_consecutive_iterations)  #各試行の報酬を格納"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqz_xb-PN15S"
      },
      "source": [
        "## Q学習メインプログラム\n",
        "Q学習のメイン関数です．\n",
        "ここまでの設定に従い，Q学習を用いて学習を行います．\n",
        "以下のような流れのプログラムを作成します．\n",
        "1. 環境の初期化・状態の離散化を行い，初期状態を獲得\n",
        "2. 獲得した初期状態から行動を選択（$\\epsilon$-greedy法を用いた行動選択）\n",
        "3. 環境に対してエージェントが行動（遷移情報の獲得）\n",
        "4. 得られた経験を用いてQ-tableを更新\n",
        "5. 指定したステップ数，2～4を繰り返す"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(num_episodes):  # 試行数分繰り返す\n",
        "    # 環境の初期化\n",
        "    observation = env.reset()\n",
        "    state = digitize_state(observation)\n",
        "    action = np.argmax(q_table[state])\n",
        "    episode_reward = 0\n",
        "\n",
        "    for t in range(max_number_of_steps):  # 1試行のループ\n",
        "\n",
        "        # 行動a_tの実行により、s_{t+1}, r_{t}などを計算する\n",
        "        observation, reward, done, info = env.step(action)\n",
        "\n",
        "        # 報酬設計\n",
        "        if done:\n",
        "            if t < 195:\n",
        "                reward = -200  # 倒れたら罰則\n",
        "            else:\n",
        "                reward = 1  # 立ったまま終了時は罰則はなし\n",
        "        else:\n",
        "            reward = 1  # 各ステップで立ってたら報酬追加\n",
        "\n",
        "        episode_reward += reward  # 報酬を追加\n",
        "\n",
        "        # 離散状態s_{t+1}を求め、Q関数を更新する\n",
        "        next_state = digitize_state(observation)  # t+1での観測状態を、離散値に変換\n",
        "        q_table = update_Qtable(q_table, state, action, reward, next_state)\n",
        "\n",
        "        #  次の行動a_{t+1}を求める \n",
        "        action = get_action_q(next_state, episode)    # a_{t+1} \n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        # 終了時の処理\n",
        "        if done:\n",
        "            if (episode+1)%10==0:\n",
        "              print('{} Episode finished after {} steps / mean reward {}' .format(episode+1, t+1, total_reward_vec.mean()))\n",
        "            total_reward_vec = np.hstack((total_reward_vec[1:], episode_reward))  # 報酬を記録\n",
        "            break"
      ],
      "metadata": {
        "id": "ZVRem88P9xPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX-V2yIoF1zG"
      },
      "source": [
        "## 学習結果を保存\n",
        "Q学習を用いて学習したエージェントを確認してみます．\\\n",
        "ここでは，学習時と同様の処理を行い，framesに描画したフレームを順次格納します．\n",
        "学習時と異なるのは，$\\epsilon$-greedy法は用いず常にQ値が最も高い行動を選択します．\n",
        "指定したステップ数，フレームを獲得したら終了します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wCnPwgoc8QR"
      },
      "outputs": [],
      "source": [
        "# 結果を描画するための設定\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "import os\n",
        "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display._obj._screen)\n",
        "\n",
        "\n",
        "frames = []\n",
        "for i in range(3):\n",
        "    done = False\n",
        "    t = 0\n",
        "    observation = env.reset() \n",
        "    state = digitize_state(observation)\n",
        "    action = np.argmax(q_table[state])\n",
        "    episode_reward = 0  \n",
        "    while not done and t < 200:\n",
        "        frames.append(env.render(mode='rgb_array'))\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        state = digitize_state(observation)\n",
        "        action = np.argmax(q_table[state])\n",
        "        t += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps1P-_Y4F6MA"
      },
      "source": [
        "## 学習結果を描画\n",
        "maptlotlibを用いて，保存した動画フレームをアニメーションとして作成し，表示しています．"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 実行結果の表示\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "animate = lambda i: patch.set_data(frames[i])\n",
        "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
        "HTML(ani.to_jshtml())"
      ],
      "metadata": {
        "id": "2sU5u0uN91P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mqb7BDsXP1bR"
      },
      "source": [
        "# Q-Network\n",
        "Q-Networkは，Q学習にニューラルネットワークの技術を適用した手法です．\\\n",
        "Q学習では，状態・行動数に合わせたQ-tableを作成し，TD法を用いたQ値の更新を行うことで，最適なQ-tableを作成する手法でした．\n",
        "しかし，CartPoleのような状態数が連続的な値の場合，Q-tableが作成可能となるように，状態数の離散化を行わなければいけないという問題がありました．\n",
        "そこで，Q-Networkではニューラルネットワークを用いた写像関数によって，Q-tableを表現することで，連続的な状態でも直接入力として使用可能としています．\n",
        "ネットワークの出力を各行動に対応するのQ値を出力するように設計し，損失関数を用いた学習により最適なQ関数を近似します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MpSMZnZPOHL"
      },
      "source": [
        "## 環境の作成\n",
        "Q学習と同様に環境の作成を行います．（OpenAI gymのCartPole）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUkpNZYuPM6r"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSbxNbZLP87L"
      },
      "source": [
        "## ネットワークモデル\n",
        "ネットワークの設計を行います．\n",
        "今回は，全結合層が2層で構成されるネットワークとします．\\\n",
        "入力には，CartPoleにおけるCart位置，Cart速度，Poleの角度，Poleの角速度の4次元の情報を直接入力とします．\n",
        "出力には，各行動ごとのQ値を出力するため，出力層はactionの数として定義します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zXvS2kHHSe2"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(QNetwork, self).__init__()\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(4, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekd_xLCIQVY8"
      },
      "source": [
        "## アクション選択\n",
        "アクションの選択方法として$\\epsilon$-greedy法を用いた行動選択の定義を行います．\\\n",
        "Q学習と同様に適度な探索を行うため，$\\epsilon$-greedy法を用いて行動を選択します．\n",
        "確率$\\epsilon$で，ネットワークが出力したQ値が最も大きい行動を選択し，確率$1-\\epsilon$でランダムな行動を選択します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wx1PsYe1QVA1"
      },
      "outputs": [],
      "source": [
        "def get_action_qn(next_state, net, episode):\n",
        "    epsilon = 0.5 * (1 / (episode + 1))\n",
        "    if epsilon <= np.random.uniform(0, 1):\n",
        "        state_a = np.array([next_state], copy=False)\n",
        "        state_v = torch.tensor(state_a).float()\n",
        "        if use_cuda:\n",
        "          state_v = state_v.cuda()\n",
        "        q_vals_v = net(state_v)\n",
        "        _, act_v = torch.max(q_vals_v, dim=1)\n",
        "        next_action = int(act_v.item())\n",
        "    else:\n",
        "        next_action = np.random.choice([0, 1])\n",
        "\n",
        "    return next_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWZEuBXNuSXK"
      },
      "source": [
        "## 損失関数の計算\n",
        "ネットワークを更新するための損失関数の定義を行います．\\\n",
        "Q-Networkでは，ニューラルネットワークで最適なQ関数を近似します．\n",
        "ニューラルネットワークを用いた学習のため損失関数を設計し，逆伝搬によりネットワークのパラメータを更新していきます．\n",
        "損失関数は，Q学習の更新式をもとに設計されていて，以下のような二乗誤差になっています．\n",
        "\n",
        "$$\n",
        "L_{\\theta}=(r+\\gamma \\max_{a'}Q_{\\theta_{i}}(s',a')-Q_{\\theta_{i}}(s,a))^{2}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9kh56HGuR4y"
      },
      "outputs": [],
      "source": [
        "def calc_loss(batch, net):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "    states_v = torch.tensor(states).float()\n",
        "    next_states_v = torch.tensor(next_states).float()\n",
        "    actions_v = torch.tensor(actions)\n",
        "    rewards_v = torch.tensor(rewards)\n",
        "    done_mask = torch.ByteTensor(dones)\n",
        "\n",
        "    if use_cuda:\n",
        "      states_v = states_v.cuda()\n",
        "      next_states_v = next_states_v.cuda()\n",
        "      actions_v = actions_v.cuda()\n",
        "      rewards_v = rewards_v.cuda()\n",
        "      done_mask = done_mask.cuda()\n",
        "\n",
        "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "    next_state_values = net(next_states_v).max(1)[0]\n",
        "    next_state_values[done_mask] = 0.0\n",
        "    next_state_values = next_state_values.detach()\n",
        "\n",
        "    expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY5714Ivsr1_"
      },
      "source": [
        "## バッチ処理のためのbuffer\n",
        "バッチ学習のために経験を収納，取り出す関数を定義します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8svUw3jsoS-"
      },
      "outputs": [],
      "source": [
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "class Buffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in range(batch_size)])\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.uint8), np.array(next_states)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvbY013-wMbx"
      },
      "source": [
        "## 学習に必要な各種パラメータ設定\n",
        "今回の実験に使用する各種パラメータの設定を行います．\n",
        "ここでは，総試行回数などの学習に必要なパラメータの設定を行っています．\n",
        "また，ネットワークの定義や最適化手法を指定しています．\n",
        "今回の実験では，最適化手法はAdamとしています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzXWZYdawLJD"
      },
      "outputs": [],
      "source": [
        "max_number_of_steps = 200  # 1試行のstep数\n",
        "num_consecutive_iterations = 100  # 学習完了評価に使用する平均試行回数\n",
        "num_episodes = 2000  # 総試行回数\n",
        "\n",
        "total_reward_vec = np.zeros(num_consecutive_iterations)  # 各試行の報酬を格納\n",
        "\n",
        "LEARNING_RATE = 1e-4 # 学習率\n",
        "GAMMA = 0.99 # 割引率\n",
        "\n",
        "batch_size = 32 # バッチサイズ\n",
        "train_num = 0\n",
        "\n",
        "net = QNetwork(env.observation_space.shape, env.action_space.n)\n",
        "if use_cuda:\n",
        "  net = net.cuda()\n",
        "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "buffer = Buffer(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfrffImFQAKv"
      },
      "source": [
        "## 学習のメイン関数\n",
        "Q学習のメイン関数です．\n",
        "ここまでの設定に従い，Q-Newtworkの学習を行います．\n",
        "以下のような流れのプログラムを作成します．\n",
        "1. 環境を初期化し，初期状態を獲得\n",
        "2. 獲得した初期状態から行動を選択（$\\epsilon$-greedy法を用いた行動選択）\n",
        "3. 環境に対してエージェントが行動（遷移情報の獲得）\n",
        "4. バッチ数，経験を獲得したら損失関数の計算\n",
        "5. 誤差逆伝播法を用いてネットワークの更新\n",
        "6. 指定したステップ数，2～5を繰り返す"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(num_episodes):  # 試行数分繰り返す\n",
        "    # 環境の初期化\n",
        "    observation = env.reset()\n",
        "    state = observation\n",
        "    episode_reward = 0\n",
        "\n",
        "    for t in range(max_number_of_steps):  # 1試行のループ\n",
        "        action = get_action_qn(observation, net, episode)\n",
        "        # 行動a_tの実行により、s_{t+1}, r_{t}などを計算する\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        next_state = observation\n",
        "\n",
        "        # 報酬設計\n",
        "        if done:\n",
        "            if t < 195:\n",
        "                reward = -200  # 倒れたら罰則\n",
        "            else:\n",
        "                reward = 1  # 立ったまま終了時は罰則はなし\n",
        "        else:\n",
        "            reward = 1  # 各ステップで立ってたら報酬追加\n",
        "\n",
        "        episode_reward += reward  # 報酬を追加\n",
        "        \n",
        "        exp = Experience(state, action, reward, done, next_state)\n",
        "        buffer.append(exp)\n",
        "        state = next_state\n",
        "\n",
        "        if train_num > batch_size:\n",
        "            optimizer.zero_grad()\n",
        "            batch = buffer.sample(batch_size)\n",
        "            loss_t = calc_loss(batch, net)\n",
        "            loss_t.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        train_num += 1\n",
        "\n",
        "        # 終了時の処理\n",
        "        if done:\n",
        "            if (episode+1)%10==0:\n",
        "              print('{} Episode finished after {} steps / mean reward {}' .format(episode+1, t+1, total_reward_vec.mean()))\n",
        "            total_reward_vec = np.hstack((total_reward_vec[1:], episode_reward))  # 報酬を記録\n",
        "            break"
      ],
      "metadata": {
        "id": "fC7q5vMJ98Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3iQMt8MonQI"
      },
      "source": [
        "## 学習結果を保存\n",
        "学習したネットワーク（エージェント）を確認してみます．\\\n",
        "ここでは，学習時と同様の処理を行い，framesに描画したフレームを順次格納します．\n",
        "学習時と異なるのは，$\\epsilon$-greedy法は用いず，常にQ値が最も高い行動を選択します．\n",
        "指定したステップ数，フレームを獲得したら終了します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj4FhGY_omOQ"
      },
      "outputs": [],
      "source": [
        "# 結果を描画するための設定\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "import os\n",
        "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display._obj._screen)\n",
        "\n",
        "\n",
        "frames = []\n",
        "for i in range(3):\n",
        "    done = False\n",
        "    t = 0\n",
        "    state = env.reset() \n",
        "    episode_reward = 0  \n",
        "    while not done and t < 200:\n",
        "        frames.append(env.render(mode='rgb_array'))\n",
        "        state_a = np.array([state], copy=False)\n",
        "        state_v = torch.tensor(state_a).float()\n",
        "        if use_cuda:\n",
        "          state_v = state_v.cuda()\n",
        "        q_vals_v = net(state_v)\n",
        "        _, act_v = torch.max(q_vals_v, dim=1)\n",
        "        action = int(act_v.item())\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        state = new_state\n",
        "        t += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5tJoQekxstF"
      },
      "source": [
        "## 描画\n",
        "\n",
        "maptlotlibを用いて，保存した動画フレームをアニメーションとして作成し，表示しています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFDCxq1GeXl4"
      },
      "outputs": [],
      "source": [
        "# 実行結果の表示\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "animate = lambda i: patch.set_data(frames[i])\n",
        "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
        "HTML(ani.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttgAMbF5odix"
      },
      "source": [
        "# 課題\n",
        "\n",
        "1. Q-Tableを用いた学習方法において，テーブルを作成する際の「状態の分割数」を変えて実験してみましょう．\n",
        "\n",
        "2. 学習のパラメータを変えて実験してみましょう．\n",
        "  * 通常の深層学習のパラメータに加えて，強化学習特有のパラメータとして割引率 `gamma` などがあります．\n",
        "\n",
        "3. 報酬の値を変更して実験してみましょう．\\\n",
        "※ Q-Tableを用いる学習の場合は「Q学習メインプログラム」，Q-Networkの場合は「学習のメイン関数」にある報酬の値を変更します．"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "00.Q-Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}